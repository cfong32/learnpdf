{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import urllib\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "nof_pdf_per_query = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading arxiv categories ...\n",
      "153 categories read.\n",
      "sample: cs.MS , q-fin.PR , q-bio.TO\n"
     ]
    }
   ],
   "source": [
    "print ('reading arxiv categories ...')\n",
    "\n",
    "catlist = []\n",
    "with io.open('ArxivSubjectCategory.csv', newline='\\n') as csvfile:\n",
    "    catreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    catlist = [row[0] for row in catreader]\n",
    "\n",
    "print (len(catlist), 'categories read.')\n",
    "print ('sample:', np.random.choice(catlist), ',', np.random.choice(catlist), ',', np.random.choice(catlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating queries ...\n",
      "18 years,  153 categories,  total 2754 queries.\n",
      "sample: http://export.arxiv.org/api/query?search_query=cat:physics.space-ph&all:2017&start=0&max_results=3\n"
     ]
    }
   ],
   "source": [
    "print ('generating queries ...')\n",
    "q = 'http://export.arxiv.org/api/query?search_query=cat:{cat}&all:{yr}&start=0&max_results={n}'\n",
    "yrlist = list(range(2001, 2019)) # from 2001 to 2018\n",
    "qrylist = []\n",
    "\n",
    "for yr in yrlist:\n",
    "    for cat in catlist:\n",
    "        qry = q.format(cat=cat, yr=yr, n=nof_pdf_per_query)\n",
    "        qrylist.append(qry)\n",
    "        \n",
    "print (len(yrlist), 'years, ', len(catlist), 'categories, ', 'total', len(qrylist), 'queries.')\n",
    "print ('sample:', np.random.choice(qrylist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reporting times\n",
    "def t_print(t, msg):\n",
    "    te = time.time()-t\n",
    "    print (msg, 'in %.2fs' % te)\n",
    "    return time.time()\n",
    "\n",
    "# make 1 query, return an empty {} if error\n",
    "def make_query(query, data={}):\n",
    "    try:\n",
    "        data[query] = urllib.request.urlopen(query).read()\n",
    "    except:\n",
    "        print (err)\n",
    "    return data\n",
    "\n",
    "# read data{} from pickle, otherwise create a new dict\n",
    "def load_pickle(path):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    except:\n",
    "        print (err)\n",
    "        data = {}\n",
    "    return data\n",
    "\n",
    "# dump data{} into pickle\n",
    "def dump_pickle(data, path):\n",
    "    pickle.dump(data, open(path, 'wb'))\n",
    "\n",
    "# load or make 1 query, then save to pickle\n",
    "def query_and_store(query, pickle_fpath='data.pickle', force=False):\n",
    "    data = load_pickle(pickle_fpath)\n",
    "\n",
    "    if query not in data:\n",
    "        data[query] = make_query(query)\n",
    "        dump_pickle(data, pickle_fpath)\n",
    "    \n",
    "    return data[query]\n",
    "\n",
    "# load or make a list of queries, then save to pickle\n",
    "def qrylist_and_store(qrylist, pickle_fpath='data.pickle', force=False, nof_thread=10):\n",
    "    \n",
    "    #init a timer\n",
    "    t = time.time()\n",
    "    \n",
    "    # read data(dict) from pickle, otherwise create a new dict\n",
    "    data = load_pickle(pickle_fpath)\n",
    "    t = t_print(t, 'finish load_pickle')\n",
    "    \n",
    "    # find query which is not in data\n",
    "    todo_qrylist = list(set(qrylist) - set(data)) if not force  else qrylist.copy()\n",
    "    t = t_print(t, 'finish todo_qrylist')\n",
    "    \n",
    "    # make query for those which are not found\n",
    "    if todo_qrylist:\n",
    "        \n",
    "        # init multiple threads\n",
    "        pool = ThreadPool(nof_thread)\n",
    "        \n",
    "        # make queries asynchronously\n",
    "        results = pool.imap(make_query, todo_qrylist)\n",
    "        t = t_print(t, 'finish imap')\n",
    "        \n",
    "        # wait and report progress\n",
    "        for i, result in enumerate(results):\n",
    "            q = todo_qrylist[i]\n",
    "            data[q] = result[q]\n",
    "            sys.stdout.write('\\rquery progress: {0}/{1}'.format(i, len(todo_qrylist)))\n",
    "        print ('')\n",
    "        t = t_print(t, 'finish results')\n",
    "        \n",
    "        # close the pool and wait for the work to finish \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        t = t_print(t, 'finish join')\n",
    "        \n",
    "        # save data to pickle\n",
    "        dump_pickle(data, pickle_fpath)\n",
    "        t = t_print(t, 'finish dump')\n",
    "    \n",
    "    else:\n",
    "        print ('data found in \"%s\", no query is needed.' % pickle_fpath)\n",
    "        \n",
    "    return {qry : data[qry] for qry in qrylist}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish load_pickle in 0.03s\n",
      "finish todo_qrylist in 0.00s\n",
      "data found in \"data.pickle\", no query is needed.\n"
     ]
    }
   ],
   "source": [
    "data = qrylist_and_store(qrylist, pickle_fpath='data.pickle', nof_thread=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':'asdf', 'z':'zxcv', 's':'sdfg'}\n",
    "b = {'a':'asdf', 'z':'zxcv', 'q':'qwer'}\n",
    "c = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q'}\n",
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "print (set(b)-set(a))\n",
    "if a: print ('a')\n",
    "if b: print ('b')\n",
    "if c: print ('c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['a', 'z', 's'])\n"
     ]
    }
   ],
   "source": [
    "print (a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {'a':'a'}\n",
    "type(None) is not dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
