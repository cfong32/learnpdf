{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a dataset of pdf research papers\n",
    "### Part 1. Collecting urls of pdf files from Arvix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import urllib\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "nof_pdf_per_query = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading arxiv categories ...\n",
      "153 categories read.\n",
      "sample: hep-lat , cs.LG , cs.SC\n"
     ]
    }
   ],
   "source": [
    "print ('reading arxiv categories ...')\n",
    "\n",
    "catlist = []\n",
    "with io.open('ArxivSubjectCategory.csv', newline='\\n') as csvfile:\n",
    "    catreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    catlist = [row[0] for row in catreader]\n",
    "\n",
    "print (len(catlist), 'categories read.')\n",
    "print ('sample:', np.random.choice(catlist), ',', np.random.choice(catlist), ',', np.random.choice(catlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating queries ...\n",
      "9 years,  153 categories,  total 1377 queries.\n",
      "sample: http://export.arxiv.org/api/query?search_query=cat:cs.NA&all:2010&start=0&max_results=30\n"
     ]
    }
   ],
   "source": [
    "print ('generating queries ...')\n",
    "q = 'http://export.arxiv.org/api/query?search_query=cat:{cat}&all:{yr}&start=0&max_results={n}'\n",
    "yrlist = list(range(2010, 2019)) # from 2010 to 2018\n",
    "qrylist = []\n",
    "\n",
    "for yr in yrlist:\n",
    "    for cat in catlist:\n",
    "        qry = q.format(cat=cat, yr=yr, n=nof_pdf_per_query)\n",
    "        qrylist.append(qry)\n",
    "        \n",
    "print (len(yrlist), 'years, ', len(catlist), 'categories, ', 'total', len(qrylist), 'queries.')\n",
    "print ('sample:', np.random.choice(qrylist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     1,
     7,
     15,
     25,
     29,
     34,
     44,
     59
    ]
   },
   "outputs": [],
   "source": [
    "# for reporting times\n",
    "def t_print(t, msg):\n",
    "    te = time.time()-t\n",
    "    print (msg, 'in %.2fs' % te)\n",
    "    return time.time()\n",
    "\n",
    "# make 1 query, return an empty {} if error\n",
    "def make_query(query, data={}):\n",
    "    try:\n",
    "        data[query] = urllib.request.urlopen(query).read()\n",
    "    except:\n",
    "        print ('urlopen error, returning empty{}')\n",
    "    return data\n",
    "\n",
    "# read data{} from pickle, otherwise create a new dict\n",
    "def load_pickle(path):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print ('file not found, returning empty{}')\n",
    "        data = {}\n",
    "    return data\n",
    "\n",
    "# dump data{} into pickle\n",
    "def dump_pickle(data, path):\n",
    "    pickle.dump(data, open(path, 'wb'))\n",
    "\n",
    "# dump data{} into pickle if the file is not there\n",
    "def check_dump_pickle(data, path, force=False):\n",
    "    if not os.path.isfile(path) or force:\n",
    "        pickle.dump(data, open(path, 'wb'))\n",
    "\n",
    "# load or make 1 query, then save to pickle\n",
    "def query_and_store(query, pickle_fpath='data.pickle', force=False):\n",
    "    data = load_pickle(pickle_fpath)\n",
    "\n",
    "    if query not in data:\n",
    "        data[query] = make_query(query)\n",
    "        dump_pickle(data, pickle_fpath)\n",
    "    \n",
    "    return data[query]\n",
    "\n",
    "# run a function in multi-threads, reporting progress\n",
    "def run_multi_thread(function, iterables, nof_thread=10):\n",
    "    pool = ThreadPool(nof_thread)\n",
    "    results = pool.imap(function, iterables)\n",
    "    \n",
    "    output = []\n",
    "    for i, result in enumerate(results):\n",
    "        output.append(result)\n",
    "        sys.stdout.write('\\rprogress: {0}/{1}'.format(i, len(iterables)))\n",
    "    print ('')\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return output\n",
    "    \n",
    "# load or make a list of queries, then save to pickle\n",
    "def qrylist_and_store(qrylist, pickle_fpath='data.pickle', force=False, nof_thread=10):\n",
    "    \n",
    "    #init a timer\n",
    "    t = time.time()\n",
    "    \n",
    "    # read data(dict) from pickle, otherwise create a new dict\n",
    "    data = load_pickle(pickle_fpath)\n",
    "    t = t_print(t, 'finish load_pickle')\n",
    "    \n",
    "    # find query which is not in data\n",
    "    todo_qrylist = list(set(qrylist) - set(data)) if not force  else qrylist.copy()\n",
    "    \n",
    "    # make query for those which are not found\n",
    "    if todo_qrylist:\n",
    "        \n",
    "        # make query in multi-threads\n",
    "        results = run_multi_thread(make_query, todo_qrylist, nof_thread=nof_thread)\n",
    "        t = t_print(t, 'finish http requests')\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            q = todo_qrylist[i]\n",
    "            data[q] = result[q]\n",
    "        if len(results) == 0: print ('ERROR: len(results) == 0')\n",
    "        \n",
    "        # save data to pickle\n",
    "        dump_pickle(data, pickle_fpath)\n",
    "        t = t_print(t, 'finish dump_pickle')\n",
    "        \n",
    "        print ('%d query(s) finished.' % len(todo_qrylist))\n",
    "        \n",
    "    else:\n",
    "        print ('data found in \"%s\", no query is needed.' % pickle_fpath)\n",
    "        \n",
    "    return {qry : data[qry] for qry in qrylist}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making queries thru arvix api ...\n",
      "finish load_pickle in 1.44s\n",
      "data found in \"xmls.pickle\", no query is needed.\n",
      "sample: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Acs.FL%26id_list%3D%26start%3D0%26max_results%3D30\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=cat:cs.FL&amp;id_list=&amp;start=0&amp;max_results=30</title>\\n  <id>http://arxiv.org/api/kR51HcppIHuzzWwG+SVLlcxFQBM</id>\\n  <updated>2018-10-29T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">2462</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">30</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/0903.2554v2</id>\\n    <updated>2013-07-31T16:23:22Z</updated>\\n    <published>2009-03-14T16:14:18Z</published>\\n    <title>Bottom-up rewriting for words and terms</title>\\n    <sum' ...\n"
     ]
    }
   ],
   "source": [
    "print ('making queries thru arvix api ...')\n",
    "\n",
    "data = qrylist_and_store(qrylist, pickle_fpath='xmls.pickle', nof_thread=100)\n",
    "xmllist = list(data.values())\n",
    "\n",
    "print ('sample:', np.random.choice(xmllist)[:1000], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Retrieving pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "required_field = ['id', 'title', 'authors', 'link', 'updated'] # these fields will be keys of the papers[i]{}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing xmls (queried results) ...\n",
      "sample: {'links': [{'href': 'http://arxiv.org/api/query?search_query%3Dcat%3Acs.DB%26id_list%3D%26start%3D0%26max_results%3D30', 'rel': 'self', 'type': 'application/atom+xml'}], 'title': 'ArXiv Query: search_query=cat:cs.DB&id_list=&start=0&max_results=30', 'title_detail': {'type': 'text/html', 'language': None, 'base': '', 'value': 'ArXiv Query: search_query=cat:cs.DB&id_list=&start=0&max_results=30'}, 'id': 'http://arxiv.org/api/63/IcBsQBzDvn01GqFFrQrNQed0', 'guidislink': True, 'link': 'http://arxiv.org/api/63/IcBsQBzDvn01GqFFrQrNQed0', 'updated': '2018-10-29T00:00:00-04:00', 'updated_parsed': time.struct_time(tm_year=2018, tm_mon=10, tm_mday=29, tm_hour=4, tm_min=0, tm_sec=0, tm_wday=0, tm_yday=302, tm_isdst=0), 'opensearch_totalresults': '3917', 'opensearch_startindex': '0', 'opensearch_itemsperpage': '30'} ...\n"
     ]
    }
   ],
   "source": [
    "print ('parsing xmls (queried results) ...')\n",
    "\n",
    "force = False\n",
    "feeds = load_pickle('feeds.pickle')\n",
    "if not(feeds) or force:\n",
    "    feeds = run_multi_thread(feedparser.parse, xmllist, nof_thread=100)\n",
    "    dump_pickle(feeds, 'feeds.pickle')\n",
    "\n",
    "print ('sample:', np.random.choice(feeds).feed, '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     2,
     5
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting feeds into papers(a list of dict containing required fields) ...\n",
      "4500 papers read.\n",
      "sample: {'id': 'http://arxiv.org/abs/hep-ph/9203217v1', 'link': 'http://arxiv.org/abs/hep-ph/9203217v1', 'updated': '1992-03-18T20:13:19Z', 'title': 'Multiple photon effects in fermion-(anti)fermion scattering at SSC\\n  energies', 'authors': [{'name': 'D. B. DeLaney'}, {'name': 'S. Jadach'}, {'name': 'Ch. Shio'}, {'name': 'G. Siopsis'}, {'name': 'B. F. L. Ward'}], 'filename': 'dataset/hep-ph_9203217v1.pdf', 'xml': 'dataset/hep-ph_9203217v1.xml', 'info': 'dataset/hep-ph_9203217v1.info.pickle', 'pdfurl': 'http://arxiv.org/pdf/hep-ph/9203217v1.pdf'}\n",
      "pdf url:  http://arxiv.org/pdf/hep-ph/9203217v1.pdf\n",
      "local filename:  dataset/hep-ph_9203217v1.pdf\n",
      "local xml:  dataset/hep-ph_9203217v1.xml\n",
      "local info.pickle: dataset/hep-ph_9203217v1.info.pickle\n"
     ]
    }
   ],
   "source": [
    "print ('converting feeds into papers(a list of dict containing required fields) ...')\n",
    "\n",
    "def id_to_pdfurl(arxiv_id):\n",
    "    return arxiv_id.replace('abs', 'pdf') + '.pdf'\n",
    "\n",
    "def id_to_filename(arxiv_id, ext='pdf'):\n",
    "    parts = arxiv_id.split('/')\n",
    "    dstdir = 'dataset/'\n",
    "    if ext[0] != '.':\n",
    "        ext = '.' + ext\n",
    "    return dstdir + '_'.join(parts[parts.index('abs')+1:]) + ext\n",
    "\n",
    "papers = {}\n",
    "for feed in feeds:\n",
    "    for entry in feed.entries:\n",
    "        paper = {}\n",
    "        for i, k in enumerate(entry):\n",
    "            if k in required_field:\n",
    "                paper[k] = entry[k]\n",
    "                \n",
    "        paper['filename'] = id_to_filename(paper['id'])\n",
    "        paper['xml'] = id_to_filename(paper['id'], 'xml')\n",
    "        paper['info'] = id_to_filename(paper['id'], 'info.pickle')\n",
    "        paper['pdfurl'] = id_to_pdfurl(paper['id'])\n",
    "        \n",
    "        if paper['id'] not in papers:\n",
    "            papers[paper['id']] = paper\n",
    "            \n",
    "papers = list(papers.values())\n",
    "\n",
    "print (len(papers), 'papers read.')\n",
    "i = np.random.randint(1, len(papers)) - 1\n",
    "print ('sample:', papers[i])\n",
    "print ('pdf url: ', papers[i]['pdfurl'])\n",
    "print ('local filename: ', papers[i]['filename'])\n",
    "print ('local xml: ', papers[i]['xml'])\n",
    "print ('local info.pickle:', papers[i]['info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def download_file(url_localpath_tuple, nof_trial=5):\n",
    "    url, local_fpath = url_localpath_tuple\n",
    "\n",
    "    for i in range(nof_trial):\n",
    "        try:\n",
    "            saved_fpath, header = urllib.request.urlretrieve (url, local_fpath)\n",
    "            \n",
    "            if 'application/pdf' in header['content-type']:\n",
    "                print ('saved %s to %s.' % (url, local_fpath))\n",
    "                return True\n",
    "            \n",
    "            elif 'text/html' in header['content-type']:\n",
    "                with open(saved_fpath, 'r') as f:\n",
    "                    fcontent = f.read()\n",
    "                if 'No PDF' in fcontent:\n",
    "                    copyfile(local_fpath, local_fpath+'.no')\n",
    "                    print ('failed downloading %s to %s.' % (url, local_fpath))\n",
    "                    return False\n",
    "                elif 'reload this URL' in fcontent:\n",
    "#                     print ('Reload needed: re-trying %s %d time%s ... ' % (url, i+1, '' if i == 0 else 's'))\n",
    "                    pass\n",
    "                else:\n",
    "#                     print ('Unknown HTML: re-trying %s %d time%s ... ' % (url, i+1, '' if i == 0 else 's'))\n",
    "                    pass\n",
    "            else:\n",
    "#                 print ('Unknown content: re-trying %s %d time%s ... ' % (url, i+1, '' if i == 0 else 's'))\n",
    "                pass    \n",
    "        except:\n",
    "#             print ('Connection error: re-trying %s %d time%s ... ' % (url, i+1, '' if i == 0 else 's'))\n",
    "            pass\n",
    "    \n",
    "    print ('failed downloading %s to %s.' % (url, local_fpath))\n",
    "    return False\n",
    "#     print ('Failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading pdfs ...\n",
      "files to be downloaded:  0\n",
      "\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "print ('downloading pdfs ...')\n",
    "\n",
    "url_localpath_tuples = [(p['pdfurl'], p['filename']) for p in papers if not os.path.isfile(p['filename'])]\n",
    "print ('files to be downloaded: ', len(url_localpath_tuples))\n",
    "results = run_multi_thread(download_file, url_localpath_tuples, nof_thread=20)\n",
    "print ('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dataset(directory = 'dataset/', ext = '.pdf'):\n",
    "    if ext[0] != '.':\n",
    "        ext = '.' + ext\n",
    "    return [f for f in os.listdir(directory) if os.path.isfile(directory) and f.endswith(ext)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "print ('writing info files ...')\n",
    "\n",
    "pdfs = list_dataset('dataset/', '.pdf')\n",
    "papersset = {id_to_filename(p['id'], ext='pdf') for p in papers}\n",
    "for pdf in pdfs:\n",
    "    check_dump_pickle(data, path, force=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
