{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a dataset of pdf research papers\n",
    "### Part 1. Collecting urls of pdf files from Arvix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import urllib\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "nof_pdf_per_query = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading arxiv categories ...\n",
      "153 categories read.\n",
      "sample: math.LO , astro-ph , cs.DS\n"
     ]
    }
   ],
   "source": [
    "print ('reading arxiv categories ...')\n",
    "\n",
    "catlist = []\n",
    "with io.open('ArxivSubjectCategory.csv', newline='\\n') as csvfile:\n",
    "    catreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    catlist = [row[0] for row in catreader]\n",
    "\n",
    "print (len(catlist), 'categories read.')\n",
    "print ('sample:', np.random.choice(catlist), ',', np.random.choice(catlist), ',', np.random.choice(catlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating queries ...\n",
      "18 years,  153 categories,  total 2754 queries.\n",
      "sample: http://export.arxiv.org/api/query?search_query=cat:hep-th&all:2009&start=0&max_results=3\n"
     ]
    }
   ],
   "source": [
    "print ('generating queries ...')\n",
    "q = 'http://export.arxiv.org/api/query?search_query=cat:{cat}&all:{yr}&start=0&max_results={n}'\n",
    "yrlist = list(range(2001, 2019)) # from 2001 to 2018\n",
    "qrylist = []\n",
    "\n",
    "for yr in yrlist:\n",
    "    for cat in catlist:\n",
    "        qry = q.format(cat=cat, yr=yr, n=nof_pdf_per_query)\n",
    "        qrylist.append(qry)\n",
    "        \n",
    "print (len(yrlist), 'years, ', len(catlist), 'categories, ', 'total', len(qrylist), 'queries.')\n",
    "print ('sample:', np.random.choice(qrylist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reporting times\n",
    "def t_print(t, msg):\n",
    "    te = time.time()-t\n",
    "    print (msg, 'in %.2fs' % te)\n",
    "    return time.time()\n",
    "\n",
    "# make 1 query, return an empty {} if error\n",
    "def make_query(query, data={}):\n",
    "    try:\n",
    "        data[query] = urllib.request.urlopen(query).read()\n",
    "    except:\n",
    "        print ('urlopen error, returning empty{}')\n",
    "    return data\n",
    "\n",
    "# read data{} from pickle, otherwise create a new dict\n",
    "def load_pickle(path):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print ('file not found, returning empty{}')\n",
    "        data = {}\n",
    "    return data\n",
    "\n",
    "# dump data{} into pickle\n",
    "def dump_pickle(data, path):\n",
    "    pickle.dump(data, open(path, 'wb'))\n",
    "\n",
    "# load or make 1 query, then save to pickle\n",
    "def query_and_store(query, pickle_fpath='data.pickle', force=False):\n",
    "    data = load_pickle(pickle_fpath)\n",
    "\n",
    "    if query not in data:\n",
    "        data[query] = make_query(query)\n",
    "        dump_pickle(data, pickle_fpath)\n",
    "    \n",
    "    return data[query]\n",
    "\n",
    "# run a function in multi-threads, reporting progress\n",
    "def run_multi_thread(function, iterables, nof_thread=10):\n",
    "    pool = ThreadPool(nof_thread)\n",
    "    results = pool.imap(function, iterables)\n",
    "    \n",
    "    output = []\n",
    "    for i, result in enumerate(results):\n",
    "        output.append(result)\n",
    "        sys.stdout.write('\\rprogress: {0}/{1}'.format(i, len(iterables)))\n",
    "    print ('')\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return output\n",
    "    \n",
    "# load or make a list of queries, then save to pickle\n",
    "def qrylist_and_store(qrylist, pickle_fpath='data.pickle', force=False, nof_thread=10):\n",
    "    \n",
    "    #init a timer\n",
    "    t = time.time()\n",
    "    \n",
    "    # read data(dict) from pickle, otherwise create a new dict\n",
    "    data = load_pickle(pickle_fpath)\n",
    "    t = t_print(t, 'finish load_pickle')\n",
    "    \n",
    "    # find query which is not in data\n",
    "    todo_qrylist = list(set(qrylist) - set(data)) if not force  else qrylist.copy()\n",
    "    \n",
    "    # make query for those which are not found\n",
    "    if todo_qrylist:\n",
    "        \n",
    "        # make query in multi-threads\n",
    "        results = run_multi_thread(make_query, todo_qrylist, nof_thread=nof_thread)\n",
    "        t = t_print(t, 'finish http requests')\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            q = todo_qrylist[i]\n",
    "            data[q] = result[q]\n",
    "        if len(results) == 0: print ('ERROR: len(results) == 0')\n",
    "        \n",
    "        # save data to pickle\n",
    "        dump_pickle(data, pickle_fpath)\n",
    "        t = t_print(t, 'finish dump_pickle')\n",
    "        \n",
    "        print ('%d query(s) finished.' % len(todo_qrylist))\n",
    "        \n",
    "    else:\n",
    "        print ('data found in \"%s\", no query is needed.' % pickle_fpath)\n",
    "        \n",
    "    return {qry : data[qry] for qry in qrylist}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making queries thru arvix api ...\n",
      "finish load_pickle in 0.58s\n",
      "data found in \"xmls.pickle\", no query is needed.\n",
      "sample: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Acs.MA%26id_list%3D%26start%3D0%26max_results%3D3\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=cat:cs.MA&amp;id_list=&amp;start=0&amp;max_results=3</title>\\n  <id>http://arxiv.org/api/L0oxE8y1b/7nYmp5IOazKhJLIms</id>\\n  <updated>2018-10-24T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">2104</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">3</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0306119v1</id>\\n    <updated>2003-06-20T15:12:22Z</updated>\\n    <published>2003-06-20T15:12:22Z</published>\\n    <title>A Method for Solving Distributed Service Allocation Proble' ...\n"
     ]
    }
   ],
   "source": [
    "print ('making queries thru arvix api ...')\n",
    "\n",
    "data = qrylist_and_store(qrylist, pickle_fpath='xmls.pickle', nof_thread=100)\n",
    "xmllist = list(data.values())\n",
    "\n",
    "print ('sample:', np.random.choice(xmllist)[:1000], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Retrieving pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "required_field = ['id', 'title', 'authors', 'link', 'updated'] # these fields will be keys of the papers[i]{}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing xmls (queried results) ...\n",
      "sample: {'links': [{'href': 'http://arxiv.org/api/query?search_query%3Dcat%3Aphysics.bio-ph%26id_list%3D%26start%3D0%26max_results%3D3', 'rel': 'self', 'type': 'application/atom+xml'}], 'title': 'ArXiv Query: search_query=cat:physics.bio-ph&id_list=&start=0&max_results=3', 'title_detail': {'type': 'text/html', 'language': None, 'base': '', 'value': 'ArXiv Query: search_query=cat:physics.bio-ph&id_list=&start=0&max_results=3'}, 'id': 'http://arxiv.org/api/MLmwnW++wM0PRedEVrmAQOLJTZY', 'guidislink': True, 'link': 'http://arxiv.org/api/MLmwnW++wM0PRedEVrmAQOLJTZY', 'updated': '2018-10-24T00:00:00-04:00', 'updated_parsed': time.struct_time(tm_year=2018, tm_mon=10, tm_mday=24, tm_hour=4, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=297, tm_isdst=0), 'opensearch_totalresults': '9268', 'opensearch_startindex': '0', 'opensearch_itemsperpage': '3'} ...\n"
     ]
    }
   ],
   "source": [
    "print ('parsing xmls (queried results) ...')\n",
    "\n",
    "force = False\n",
    "feedlist = load_pickle('feeds.pickle')\n",
    "if not(feedlist) or force:\n",
    "    feedlist = run_multi_thread(feedparser.parse, xmllist, nof_thread=100)\n",
    "    dump_pickle(feedlist, 'feeds.pickle')\n",
    "\n",
    "print ('sample:', np.random.choice(feedlist).feed, '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting feeds into a list of dict ...\n",
      "8260 papers read.\n",
      "sample: {'id': 'http://arxiv.org/abs/math/9201273v1', 'link': 'http://arxiv.org/abs/math/9201273v1', 'updated': '1990-05-12T00:00:00Z', 'title': 'Remarks on iterated cubic maps', 'authors': [{'name': 'John W. Milnor'}]}\n"
     ]
    }
   ],
   "source": [
    "print ('converting feeds into a list of dict ...')\n",
    "\n",
    "paperlist = []\n",
    "for feed in feedlist:\n",
    "    for entry in feed.entries:\n",
    "        paper = {}\n",
    "        for i, k in enumerate(entry):\n",
    "            if k in required_field:\n",
    "                paper[k] = entry[k]\n",
    "        paperlist.append(paper)\n",
    "\n",
    "print (len(paperlist), 'papers read.')\n",
    "print ('sample:', np.random.choice(paperlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, local_fpath, force=False):\n",
    "    if not os.path.isfile(local_fpath) or force:\n",
    "        print ('downloading ' + local_fpath + '...', end='')\n",
    "        urllib.request.urlretrieve (url, local_fpath)\n",
    "        print ('done')\n",
    "    else:\n",
    "        print ('found ' + local_fpath)\n",
    "\n",
    "def download_arxivpdfs(url_list, force=False):\n",
    "    for url in url_list:\n",
    "        local_fpath = 'pdf/' + url.split('/')[-1]\n",
    "        download_file(url, local_fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 0111152v1.pdf..."
     ]
    }
   ],
   "source": [
    "directory = '' #r'./pdf/'\n",
    "for paper in reversed(paperlist):\n",
    "    url = paper['link'].replace('abs', 'pdf') + '.pdf'\n",
    "    name = paper['id'].split('/')[-1] + '.pdf'\n",
    "    fpath = directory + name\n",
    "    download_file(url, fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':'asdf', 'z':'zxcv', 's':'sdfg'}\n",
    "b = {'a':'asdf', 'z':'zxcv', 'q':'qwer'}\n",
    "c = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q'}\n",
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "print (set(b)-set(a))\n",
    "if a: print ('a')\n",
    "if b: print ('b')\n",
    "if c: print ('c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
