<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">

<pdf2xml producer="poppler" version="0.68.0">
<page number="1" position="absolute" top="0" left="0" height="1188" width="918">
	<fontspec id="0" size="8" family="Times" color="#000000"/>
	<fontspec id="1" size="33" family="Times" color="#000000"/>
	<fontspec id="2" size="14" family="Times" color="#000000"/>
	<fontspec id="3" size="11" family="Times" color="#000000"/>
	<fontspec id="4" size="12" family="Times" color="#000000"/>
	<fontspec id="5" size="9" family="Times" color="#000000"/>
	<fontspec id="6" size="28" family="Times" color="#7f7f7f"/>
<text top="40" left="839" width="5" height="9" font="0">1</text>
<text top="91" left="74" width="770" height="32" font="1">Assessing Visual Quality of Omnidirectional Videos</text>
<text top="139" left="84" width="749" height="15" font="2">Mai Xu, Senior Member, IEEE, Chen Li, Zhenzhong Chen Senior Member, IEEE, Zulin Wang, Member, IEEE</text>
<text top="159" left="344" width="229" height="15" font="2">and Zhenyu Guan, Member, IEEE</text>
<text top="231" left="88" width="362" height="12" font="3">Abstract—In contrast with traditional video, omnidirectional</text>
<text top="246" left="73" width="377" height="12" font="3">video enables spherical viewing direction with support for head-</text>
<text top="261" left="73" width="377" height="12" font="3">mounted displays, providing an interactive and immersive experi-</text>
<text top="276" left="73" width="377" height="12" font="3">ence. Unfortunately, to the best of our knowledge, there are few</text>
<text top="291" left="73" width="377" height="12" font="3">visual quality assessment (VQA) methods, either subjective or</text>
<text top="306" left="73" width="377" height="12" font="3">objective, for omnidirectional video coding. This paper proposes</text>
<text top="321" left="73" width="377" height="12" font="3">both subjective and objective methods for assessing quality loss</text>
<text top="335" left="73" width="377" height="12" font="3">in encoding omnidirectional video. Specifically, we first present</text>
<text top="350" left="73" width="377" height="12" font="3">a new database, which includes the viewing direction data</text>
<text top="365" left="73" width="377" height="12" font="3">from several subjects watching omnidirectional video sequences.</text>
<text top="380" left="73" width="377" height="12" font="3">Then, from our database, we find a high consistency in viewing</text>
<text top="395" left="73" width="377" height="12" font="3">directions across different subjects. The viewing directions are</text>
<text top="410" left="73" width="377" height="12" font="3">normally distributed in the center of the front regions, but they</text>
<text top="425" left="73" width="377" height="12" font="3">sometimes fall into other regions, related to video content. Given</text>
<text top="440" left="73" width="377" height="12" font="3">this finding, we present a subjective VQA method for measuring</text>
<text top="455" left="73" width="377" height="12" font="3">difference mean opinion score (DMOS) of the whole and regional</text>
<text top="470" left="73" width="377" height="12" font="3">omnidirectional video, in terms of overall DMOS (O-DMOS) and</text>
<text top="485" left="73" width="377" height="12" font="3">vectorized DMOS (V-DMOS), respectively. Moreover, we propose</text>
<text top="500" left="73" width="377" height="12" font="3">two objective VQA methods for encoded omnidirectional video,</text>
<text top="515" left="73" width="377" height="12" font="3">in light of human perception characteristics of omnidirectional</text>
<text top="530" left="73" width="377" height="12" font="3">video. One method weighs the distortion of pixels with regard</text>
<text top="545" left="73" width="377" height="12" font="3">to their distances to the center of front regions, which considers</text>
<text top="560" left="73" width="377" height="12" font="3">human preference in a panorama. The other method predicts</text>
<text top="575" left="73" width="377" height="12" font="3">viewing directions according to video content, and then the</text>
<text top="589" left="73" width="377" height="12" font="3">predicted viewing directions are leveraged to allocate weights</text>
<text top="604" left="73" width="377" height="12" font="3">to the distortion of each pixel in our objective VQA method.</text>
<text top="619" left="73" width="377" height="12" font="3">Finally, our experimental results verify that both the subjective</text>
<text top="634" left="73" width="377" height="12" font="3">and objective methods proposed in this paper advance state-of-</text>
<text top="649" left="73" width="231" height="12" font="3">the-art VQA for omnidirectional video.</text>
<text top="673" left="88" width="362" height="12" font="3">Index Terms—Omnidirectional video coding, visual quality</text>
<text top="688" left="73" width="215" height="12" font="3">assessment (VQA), viewing direction</text>
<text top="733" left="203" width="23" height="13" font="4">I. I</text>
<text top="735" left="227" width="93" height="11" font="5">NTRODUCTION</text>
<text top="759" left="88" width="362" height="13" font="4">Recent years have witnessed the rapid development of</text>
<text top="777" left="73" width="377" height="13" font="4">virtual reality (VR). According to a report by <a href="xml/1709.06342v2.html#13">[1], </a>90% VR</text>
<text top="795" left="73" width="377" height="13" font="4">content is in the form of omnidirectional video, which involves</text>
<text top="812" left="73" width="35" height="13" font="4">a 360</text>
<text top="809" left="109" width="6" height="10" font="0">◦</text>
<text top="811" left="120" width="38" height="14" font="4">× 180</text>
<text top="809" left="158" width="6" height="10" font="0">◦</text>
<text top="812" left="171" width="279" height="13" font="4">viewing direction. With the support of head-</text>
<text top="830" left="73" width="377" height="13" font="4">mounted displays (HMD), omnidirectional video offers an</text>
<text top="848" left="73" width="377" height="13" font="4">immersive and even interactive experience <a href="xml/1709.06342v2.html#13">[2]. </a>On the other</text>
<text top="866" left="73" width="377" height="13" font="4">hand, it is likely that the quality of experience (QoE) <a href="xml/1709.06342v2.html#13">[3] </a>of</text>
<text top="884" left="73" width="377" height="13" font="4">omnidirectional video dramatically degrades when presented</text>
<text top="902" left="73" width="377" height="13" font="4">at low resolutions or with compression artifacts. Such QoE</text>
<text top="920" left="73" width="377" height="13" font="4">degradation always makes humans feel uncomfortable, as</text>
<text top="938" left="73" width="377" height="13" font="4">reported in the MPEG survey <a href="xml/1709.06342v2.html#13">[4]. </a>Therefore, it is necessary</text>
<text top="956" left="73" width="377" height="13" font="4">to study visual quality assessment (VQA) for omnidirectional</text>
<text top="974" left="73" width="83" height="13" font="4">video coding.</text>
<text top="992" left="88" width="362" height="13" font="4">Both subjective and objective methods are needed for Full-</text>
<text top="1010" left="73" width="377" height="13" font="4">Reference (FR) VQA in omnidirectional video coding <a href="xml/1709.06342v2.html#13">[5].</a></text>
<text top="1043" left="85" width="365" height="11" font="5">M. Xu, C. Li, Z. Wang and Z. Guan are with the School of Electronic</text>
<text top="1057" left="73" width="377" height="11" font="5">and Information Engineering, Beihang University, Beijing, 100191 China (e-</text>
<text top="1070" left="73" width="377" height="11" font="5">mail: Maixu@buaa.edu.cn; jnlichen123@buaa.edu.cn; wzulin@buaa.edu.cn;</text>
<text top="1084" left="73" width="377" height="11" font="5">guanzhenyu@buaa.edu.cn). Z. Chen is with Wuhan University, Wuhan, China</text>
<text top="1097" left="73" width="377" height="11" font="5">(e-mail: zzchen@whu.edu.cn). This work was supported by NSFC under grant</text>
<text top="1111" left="73" width="92" height="11" font="5">number 61573037.</text>
<text top="230" left="468" width="377" height="13" font="4">Subjective VQA refers to measuring the quality of omnidi-</text>
<text top="248" left="468" width="377" height="13" font="4">rectional video as rated by humans. Since omnidirectional</text>
<text top="266" left="468" width="377" height="13" font="4">video ultimately outputs to human eyes, subjective VQA is</text>
<text top="284" left="468" width="377" height="13" font="4">more rational than objective VQA in assessing visual quality.</text>
<text top="302" left="468" width="377" height="13" font="4">In addition, subjective VQA can be also used to verify the</text>
<text top="320" left="468" width="377" height="13" font="4">effectiveness of objective VQA. There are several subjective</text>
<text top="338" left="468" width="377" height="13" font="4">VQA methods <a href="xml/1709.06342v2.html#13">[6]–[10] </a>for measuring the quality reduction</text>
<text top="356" left="468" width="377" height="13" font="4">of encoded omnidirectional videos, and most of them simply</text>
<text top="373" left="468" width="377" height="13" font="4">apply score processing metrics for traditional 2D videos. In</text>
<text top="391" left="468" width="377" height="13" font="4">contrast, there are a great number of subjective VQA methods</text>
<text top="409" left="468" width="377" height="13" font="4">for traditional 2D videos, such as <a href="xml/1709.06342v2.html#13">[11]–[13]. </a>In these methods,</text>
<text top="427" left="468" width="377" height="13" font="4">the mean opinion score (MOS) <a href="xml/1709.06342v2.html#13">[14] </a>and difference MOS</text>
<text top="445" left="468" width="377" height="13" font="4">(DMOS) <a href="xml/1709.06342v2.html#13">[15] </a>are widely used metrics for subjective VQA. In</text>
<text top="463" left="468" width="377" height="13" font="4">this paper, we thus propose a subjective VQA method to assess</text>
<text top="481" left="468" width="377" height="13" font="4">quality loss in encoding omnidirectional video in the form of</text>
<text top="499" left="468" width="377" height="13" font="4">DMOS. Different from the latest subjective VQA methods of</text>
<text top="517" left="468" width="377" height="13" font="4">omnidirectional video <a href="xml/1709.06342v2.html#13">[6]–[10], </a>our method proposes the least</text>
<text top="535" left="468" width="377" height="13" font="4">subject number and a new test procedure according to our</text>
<text top="553" left="468" width="377" height="13" font="4">findings on human behavior of viewing omnidirectional video.</text>
<text top="571" left="468" width="377" height="13" font="4">In addition, two metrics of DMOS, i.e., the overall DMOS</text>
<text top="589" left="468" width="377" height="13" font="4">(O-DMOS) and vectorized DMOS (V-DMOS), are proposed</text>
<text top="607" left="468" width="193" height="13" font="4">in our subjective VQA method.</text>
<text top="625" left="483" width="362" height="13" font="4">For objective VQA, the spherical characteristic of omnidi-</text>
<text top="642" left="468" width="377" height="13" font="4">rectional video has been taken into account in the latest work</text>
<text top="660" left="468" width="377" height="13" font="4">of <a href="xml/1709.06342v2.html#13">[7], [16]. </a>For example, Yu et al. <a href="xml/1709.06342v2.html#13">[16] </a>proposed a sphere-</text>
<text top="678" left="468" width="377" height="13" font="4">based peak signal-to-noise ratio (S-PSNR), which calculates</text>
<text top="696" left="468" width="377" height="13" font="4">peak signal-to-noise ratio (PSNR) based on a set of uniformly</text>
<text top="714" left="468" width="377" height="13" font="4">sampled points on a sphere instead of rectangularly mapped</text>
<text top="732" left="468" width="377" height="13" font="4">pixels. By applying interpolation algorithms, S-PSNR is able</text>
<text top="750" left="468" width="377" height="13" font="4">to cope with objective quality assessment for omnidirectional</text>
<text top="768" left="468" width="377" height="13" font="4">video coding under different projections. The main difference</text>
<text top="786" left="468" width="377" height="13" font="4">between 2D and omnidirectional videos is that only content</text>
<text top="804" left="468" width="377" height="13" font="4">inside the field of view (FoV) is accessible in omnidirectional</text>
<text top="822" left="468" width="377" height="13" font="4">video. However, none of the existing VQA methods takes</text>
<text top="840" left="468" width="377" height="13" font="4">into consideration such perceptual characteristics of omnidi-</text>
<text top="858" left="468" width="377" height="13" font="4">rectional video. In this paper, we further propose to objectively</text>
<text top="876" left="468" width="377" height="13" font="4">assess the perceptual quality of encoded omnidirectional video</text>
<text top="893" left="468" width="377" height="13" font="4">by considering the accessible FoV of possible viewing direc-</text>
<text top="911" left="468" width="377" height="13" font="4">tions. To the best of our knowledge, our work is the first</text>
<text top="929" left="468" width="377" height="13" font="4">attempt to predict possible FoV in weighting distortion of</text>
<text top="947" left="468" width="377" height="13" font="4">omnidirectional video, despite the weight mechanism being</text>
<text top="965" left="468" width="194" height="13" font="4">widely used in VQA <a href="xml/1709.06342v2.html#13">[16]–[18].</a></text>
<text top="983" left="483" width="362" height="13" font="4">To be more specific, this paper first presents a new database</text>
<text top="1001" left="468" width="377" height="13" font="4">containing the viewing directions of 40 subjects watching</text>
<text top="1019" left="468" width="377" height="13" font="4">48 omnidirectional video sequences. Then, by mining our</text>
<text top="1037" left="468" width="377" height="13" font="4">database, we discover that the viewing directions across differ-</text>
<text top="1055" left="468" width="377" height="13" font="4">ent subjects are highly consistent. In light of such a finding, we</text>
<text top="1073" left="468" width="377" height="13" font="4">develop two subjective VQA metrics, namely O-DMOS and</text>
<text top="1091" left="468" width="377" height="13" font="4">V-DMOS, for rating the overall and regional visual quality</text>
<text top="1109" left="468" width="377" height="13" font="4">reduction of encoded omnidirectional video, respectively. We</text>
<text top="835" left="48" width="0" height="27" font="6">arXiv:1709.06342v2  [eess.IV]  22 Sep 2018</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="1188" width="918">
<text top="40" left="839" width="5" height="9" font="0">2</text>
<text top="87" left="73" width="377" height="13" font="4">further find from our database that the consistent viewing</text>
<text top="104" left="73" width="377" height="13" font="4">directions of humans are related to both spherical location and</text>
<text top="122" left="73" width="377" height="13" font="4">video content. Accordingly, we propose two objective VQA</text>
<text top="140" left="73" width="377" height="13" font="4">methods that allocate weights to the distortion of each pixel</text>
<text top="158" left="73" width="377" height="13" font="4">when calculating the PSNR. The weight allocation in the first</text>
<text top="176" left="73" width="377" height="13" font="4">method only leverages humans’ preferences for the location of</text>
<text top="194" left="73" width="377" height="13" font="4">pixels in omnidirectional video, while the second method also</text>
<text top="212" left="73" width="377" height="13" font="4">depends on video content. There are some works on projection</text>
<text top="230" left="73" width="377" height="13" font="4"><a href="xml/1709.06342v2.html#13">[19] </a>and coding optimization <a href="xml/1709.06342v2.html#13">[20]–[22] </a>for omnidirectional</text>
<text top="248" left="73" width="377" height="13" font="4">video, which emphasize region of interest (RoI). Similarly,</text>
<text top="266" left="73" width="377" height="13" font="4">our methods also consider RoI in VQA. V-DMOS is able</text>
<text top="284" left="73" width="377" height="13" font="4">to represent the quality of different regions and guide bit</text>
<text top="302" left="73" width="377" height="13" font="4">allocation in coding optimization. For objective VQA methods,</text>
<text top="320" left="73" width="377" height="13" font="4">different regions are unequally weighted according to the</text>
<text top="338" left="73" width="377" height="13" font="4">distribution of viewing directions in calculating NCP-PSNR.</text>
<text top="356" left="73" width="377" height="13" font="4">In the CP-PSNR method, the viewing directions are further</text>
<text top="373" left="73" width="377" height="13" font="4">predicted, such that the quality of omnidirectional video is</text>
<text top="391" left="73" width="173" height="13" font="4">calculated according to RoI.</text>
<text top="409" left="88" width="362" height="13" font="4">This paper is an extended version of our conference paper</text>
<text top="427" left="73" width="377" height="13" font="4"><a href="xml/1709.06342v2.html#13">[23]. </a>Beyond the subjective VQA method in <a href="xml/1709.06342v2.html#13">[23], </a>this paper</text>
<text top="445" left="73" width="377" height="13" font="4">further proposes two objective VQA methods for measuring</text>
<text top="463" left="73" width="377" height="13" font="4">the quality of omnidirectional video coding. For the objective</text>
<text top="481" left="73" width="377" height="13" font="4">VQA methods, this paper also investigates some new findings</text>
<text top="499" left="73" width="377" height="13" font="4">about the distribution of human viewing direction in omnidi-</text>
<text top="517" left="73" width="377" height="13" font="4">rectional video. Our contributions in this paper are three-fold:</text>
<text top="541" left="88" width="6" height="10" font="0">•</text>
<text top="538" left="103" width="347" height="13" font="4">We present a viewing direction database for omnidirec-</text>
<text top="556" left="103" width="347" height="13" font="4">tional video, with a consistency analysis on the viewing</text>
<text top="574" left="103" width="347" height="13" font="4">directions of different subjects. We also present a VQA</text>
<text top="592" left="103" width="347" height="13" font="4">database consisting of raw and compressed omnidirec-</text>
<text top="610" left="103" width="347" height="13" font="4">tional sequences, in which both the subjective scores and</text>
<text top="628" left="103" width="326" height="13" font="4">viewing directions of different subjects are collected.</text>
<text top="648" left="88" width="6" height="10" font="0">•</text>
<text top="646" left="103" width="347" height="13" font="4">We develop a new method for the subjective VQA of</text>
<text top="664" left="103" width="347" height="13" font="4">omnidirectional video, taking advantage of the analysis</text>
<text top="682" left="103" width="347" height="13" font="4">of viewing directions over our database. Our subjective</text>
<text top="700" left="103" width="347" height="13" font="4">VQA method has been adopted <a href="xml/1709.06342v2.html#13">[24] </a>by the international</text>
<text top="718" left="103" width="196" height="13" font="4">standard IEEE 1857.9/AVS-VR.</text>
<text top="738" left="88" width="6" height="10" font="0">•</text>
<text top="736" left="103" width="347" height="13" font="4">We propose two methods for the objective VQA of</text>
<text top="754" left="103" width="347" height="13" font="4">omnidirectional video coding, taking into account human</text>
<text top="771" left="103" width="347" height="13" font="4">perception related to spherical location and video content,</text>
<text top="789" left="103" width="347" height="13" font="4">respectively. Our objective VQA methods are a pioneer-</text>
<text top="807" left="103" width="347" height="13" font="4">ing attempt that embeds human perception in assessing</text>
<text top="825" left="103" width="288" height="13" font="4">visual quality of omnidirectional video coding.</text>
<text top="865" left="199" width="33" height="13" font="4">II. R</text>
<text top="867" left="233" width="92" height="11" font="5">ELATED WORK</text>
<text top="888" left="73" width="221" height="13" font="4">A. Related work on subjective VQA</text>
<text top="911" left="88" width="362" height="13" font="4">The past two decades have witnessed a number of subjective</text>
<text top="929" left="73" width="377" height="13" font="4">VQA methods for 2D video. In particular, the international</text>
<text top="947" left="73" width="377" height="13" font="4">telecommunication union (ITU) has proposed several subjec-</text>
<text top="965" left="73" width="377" height="13" font="4">tive methodologies <a href="xml/1709.06342v2.html#13">[11]–[13] </a>for assessing 2D video. Among</text>
<text top="983" left="73" width="377" height="13" font="4">these proposals, the double stimulus continuous quality scale</text>
<text top="1001" left="73" width="377" height="13" font="4">(DSCQS) <a href="xml/1709.06342v2.html#13">[25], </a>single stimulus continuous quality scale (SS-</text>
<text top="1019" left="73" width="377" height="13" font="4">CQS) <a href="xml/1709.06342v2.html#13">[15] </a>and single stimulus continuous quality evaluation</text>
<text top="1037" left="73" width="377" height="13" font="4">(SSCQE) <a href="xml/1709.06342v2.html#13">[26] </a>were adopted to determine the display orders</text>
<text top="1055" left="73" width="377" height="13" font="4">of sequences when viewing and rating video sequences. Ad-</text>
<text top="1073" left="73" width="377" height="13" font="4">ditionally, two metrics have been widely used in rating the</text>
<text top="1091" left="73" width="377" height="13" font="4">subjective VQA of 2D video: one metric is MOS <a href="xml/1709.06342v2.html#13">[14] </a>for no-</text>
<text top="1109" left="73" width="377" height="13" font="4">reference (NR), reduced-reference (RR) and FR assessments;</text>
<text top="87" left="468" width="377" height="13" font="4">the other metric is DMOS <a href="xml/1709.06342v2.html#13">[15], [27], </a>which is for FR as-</text>
<text top="104" left="468" width="377" height="13" font="4">sessment only. Recently, several subjective VQA methods for</text>
<text top="122" left="468" width="377" height="13" font="4">other types of videos have emerged. For example, Pourashraf</text>
<text top="140" left="468" width="31" height="13" font="4">et al.</text>
<text top="140" left="504" width="341" height="13" font="4"><a href="xml/1709.06342v2.html#13">[28] </a>proposed measuring the subjective quality of video</text>
<text top="158" left="468" width="377" height="13" font="4">conferencing by adopting DMOS for the conventional sub-</text>
<text top="176" left="468" width="377" height="13" font="4">jective VQA method. ITU extended their DMOS-based VQA</text>
<text top="194" left="468" width="377" height="13" font="4">method for stereoscopic video <a href="xml/1709.06342v2.html#13">[29], </a>which incorporates the</text>
<text top="212" left="468" width="224" height="13" font="4">characteristics of stereoscopic video.</text>
<text top="231" left="483" width="362" height="13" font="4">Although omnidirectional video is flooding into our daily</text>
<text top="249" left="468" width="377" height="13" font="4">life, there are some works in the literature <a href="xml/1709.06342v2.html#13">[6]–[10] </a>on the</text>
<text top="266" left="468" width="377" height="13" font="4">subjective VQA of omnidirectional video. Upenik et al. <a href="xml/1709.06342v2.html#13">[6]</a></text>
<text top="284" left="468" width="377" height="13" font="4">proposed a testbed for subjective VQA on omnidirectional</text>
<text top="302" left="468" width="377" height="13" font="4">video and image. In their testbed, an HMD is suggested as</text>
<text top="320" left="468" width="377" height="13" font="4">the displaying device, and a custom software application is</text>
<text top="338" left="468" width="377" height="13" font="4">provided. Unfortunately, <a href="xml/1709.06342v2.html#13">[6] </a>does not deal with how to mea-</text>
<text top="356" left="468" width="377" height="13" font="4">sure the subjective quality of omnidirectional video coding.</text>
<text top="374" left="468" width="377" height="13" font="4">Zakharchenko et al. conducted the subjective VQA experiment</text>
<text top="392" left="468" width="377" height="13" font="4">to validate their objective VQA methods for omnidirectional</text>
<text top="410" left="468" width="377" height="13" font="4">video <a href="xml/1709.06342v2.html#13">[7]. </a>In <a href="xml/1709.06342v2.html#13">[7], </a>subjects were forced to view one region of</text>
<text top="428" left="468" width="377" height="13" font="4">omnidirectional video, and then the conventional subjective</text>
<text top="446" left="468" width="377" height="13" font="4">VQA method for 2D video is simply applied. However,</text>
<text top="464" left="468" width="377" height="13" font="4">this is not in accordance with the interactive experience</text>
<text top="482" left="468" width="377" height="13" font="4">on omnidirectional video. More importantly, an immersive</text>
<text top="500" left="468" width="377" height="13" font="4">experience cannot be achieved in the subjective VQA of</text>
<text top="517" left="468" width="377" height="13" font="4"><a href="xml/1709.06342v2.html#13">[7], </a>such that the DMOS does not meet practical QoE for</text>
<text top="535" left="468" width="377" height="13" font="4">humans. Schatz et al. developed an approach towards QoE</text>
<text top="553" left="468" width="377" height="13" font="4">of omnidirectional video streaming <a href="xml/1709.06342v2.html#13">[8], </a>which mainly focuses</text>
<text top="571" left="468" width="377" height="13" font="4">on the impact of stalling on omnidirectional streaming. Singla</text>
<text top="589" left="468" width="31" height="13" font="4">et al.</text>
<text top="589" left="505" width="340" height="13" font="4">proposed a modified absolute category rating (M-ACR)</text>
<text top="607" left="468" width="377" height="13" font="4">method, analyzing subjective quality and simulator sickness</text>
<text top="625" left="468" width="377" height="13" font="4">of omnidirectional videos at varying resolutions and bit-rates</text>
<text top="643" left="468" width="377" height="13" font="4">across different devices <a href="xml/1709.06342v2.html#13">[9], [10]. </a>However, none of the above</text>
<text top="661" left="468" width="377" height="13" font="4">methods considers human behavior and interactive experience</text>
<text top="679" left="468" width="377" height="13" font="4">in subjective VQA, which can be mined from the viewing</text>
<text top="697" left="468" width="377" height="13" font="4">direction data of many subjects. In this paper, we propose a</text>
<text top="715" left="468" width="377" height="13" font="4">subjective VQA method that considers the interactive behavior</text>
<text top="733" left="468" width="377" height="13" font="4">of humans in viewing omnidirectional video, such that the QoE</text>
<text top="751" left="468" width="322" height="13" font="4">of subjects can be reflected in our subjective metric.</text>
<text top="797" left="468" width="215" height="13" font="4">B. Related work on objective VQA</text>
<text top="822" left="483" width="362" height="13" font="4">For objective VQA of 2D video, a commonly used FR</text>
<text top="840" left="468" width="377" height="13" font="4">metric is PSNR. PSNR is based on the mean squared error</text>
<text top="858" left="468" width="377" height="13" font="4">(MSE) between the reference and processed videos and has</text>
<text top="876" left="468" width="377" height="13" font="4">been well studied from a mathematical perspective. However,</text>
<text top="893" left="468" width="377" height="13" font="4">PSNR cannot successfully reflect the subjective visual quality</text>
<text top="911" left="468" width="377" height="13" font="4">perceived by the human visual system (HVS), as it does not</text>
<text top="929" left="468" width="377" height="13" font="4">consider human perception at all. For example, the subjective</text>
<text top="947" left="468" width="377" height="13" font="4">quality is more likely to be influenced by the PSNR in</text>
<text top="965" left="468" width="377" height="13" font="4">RoI. In order to better correlate assessments with subjective</text>
<text top="983" left="468" width="377" height="13" font="4">quality, many advanced PNSR-based methods <a href="xml/1709.06342v2.html#13">[17], </a><a href="xml/1709.06342v2.html#14">[30]–[35]</a></text>
<text top="1001" left="468" width="377" height="13" font="4">have been proposed to improve the existing PSNR metric</text>
<text top="1019" left="468" width="377" height="13" font="4">for the VQA of 2D video by accounting for the importance</text>
<text top="1037" left="468" width="377" height="13" font="4">of each pixel. For example, based on the foveation response</text>
<text top="1055" left="468" width="377" height="13" font="4">of HVS, foveal PSNR (FPSNR) <a href="xml/1709.06342v2.html#13">[17] </a>was proposed, using a</text>
<text top="1073" left="468" width="377" height="13" font="4">non-uniform resolution weighting metric, in which the dis-</text>
<text top="1091" left="468" width="377" height="13" font="4">tortion weights decrease with eccentricity. The peak signal-to-</text>
<text top="1109" left="468" width="377" height="13" font="4">perceptible noise ratio (PSPNR) <a href="xml/1709.06342v2.html#14">[30] </a>and foveated PSPNR <a href="xml/1709.06342v2.html#14">[33]</a></text>
</page>
<page number="3" position="absolute" top="0" left="0" height="1188" width="918">
<text top="40" left="839" width="5" height="9" font="0">3</text>
<text top="87" left="73" width="377" height="13" font="4">were presented to consider distortion, only when the errors</text>
<text top="104" left="73" width="377" height="13" font="4">are larger than the just-noticeable-distortion (JND) thresholds.</text>
<text top="122" left="73" width="377" height="13" font="4">Similarly, semantic PSNR (SPSNR) <a href="xml/1709.06342v2.html#14">[31] </a>and eye-tracking-</text>
<text top="140" left="73" width="377" height="13" font="4">weighted PSNR (EWPSNR) <a href="xml/1709.06342v2.html#14">[32] </a>were developed based on the</text>
<text top="158" left="73" width="377" height="13" font="4">form of PSNR as well. EWPSNR has a better performance in</text>
<text top="176" left="73" width="377" height="13" font="4">evaluating visual quality according to the real-time detected</text>
<text top="194" left="73" width="377" height="13" font="4">eye fixation points. In <a href="xml/1709.06342v2.html#14">[34], </a>a weight-based PSNR metric was</text>
<text top="212" left="73" width="377" height="13" font="4">proposed to measure the quality of video conferencing. Their</text>
<text top="230" left="73" width="377" height="13" font="4">method imposes a greater penalty weight on regions with</text>
<text top="248" left="73" width="377" height="13" font="4">faces and facial features when calculating the PSNR. Free</text>
<text top="266" left="73" width="377" height="13" font="4">energy adjusted PSNR (FEA-PSNR) was proposed in <a href="xml/1709.06342v2.html#14">[35].</a></text>
<text top="284" left="73" width="377" height="13" font="4">This method considers image perceptual complexity when</text>
<text top="302" left="73" width="377" height="13" font="4">assessing image quality. In <a href="xml/1709.06342v2.html#14">[36], </a>a no-reference PSNR method</text>
<text top="320" left="73" width="377" height="13" font="4">was proposed to assess both quantization error and the blocky</text>
<text top="338" left="73" width="377" height="13" font="4">effect in measuring the non-reference quality of H.264/AVC</text>
<text top="356" left="73" width="43" height="13" font="4">videos.</text>
<text top="373" left="88" width="362" height="13" font="4">In addition to these PSNR-based methods, there are many</text>
<text top="391" left="73" width="377" height="13" font="4">other objective VQA methods for 2D video. For example, the</text>
<text top="409" left="73" width="377" height="13" font="4">universal image quality index (UQI) <a href="xml/1709.06342v2.html#14">[37] </a>and its improved</text>
<text top="427" left="73" width="377" height="13" font="4">form, the single-scale structural similarity index (SSIM) <a href="xml/1709.06342v2.html#14">[38],</a></text>
<text top="445" left="73" width="377" height="13" font="4">were proposed to model the image distortion as the combina-</text>
<text top="463" left="73" width="377" height="13" font="4">tion of losses on luminance, contrast, and structure. The infor-</text>
<text top="481" left="73" width="377" height="13" font="4">mation fidelity criterion (IFC) <a href="xml/1709.06342v2.html#14">[39] </a>is based on information-</text>
<text top="498" left="73" width="377" height="13" font="4">theory, in which the visual quality is qualified as a mutual</text>
<text top="516" left="73" width="377" height="13" font="4">information between the impaired and reference images. The</text>
<text top="534" left="73" width="377" height="13" font="4">visual signal to noise ratio (VSNR) <a href="xml/1709.06342v2.html#14">[40] </a>is a wavelet based</text>
<text top="552" left="73" width="309" height="13" font="4">method, which detects the visibility of distortions.</text>
<text top="570" left="88" width="362" height="13" font="4">For omnidirectional video, there are several objective VQA</text>
<text top="588" left="73" width="377" height="13" font="4">works <a href="xml/1709.06342v2.html#13">[7], [16], </a><a href="xml/1709.06342v2.html#14">[41], </a>also based on PSNR. In evaluating the</text>
<text top="605" left="73" width="377" height="13" font="4">quality degradation of omnidirectional video encoding, the</text>
<text top="623" left="73" width="377" height="13" font="4">work of <a href="xml/1709.06342v2.html#13">[7], [16] </a>takes into account the spherical character-</text>
<text top="641" left="73" width="377" height="13" font="4">istic of omnidirectional video. For example, Yu et al. <a href="xml/1709.06342v2.html#13">[16]</a></text>
<text top="659" left="73" width="377" height="13" font="4">proposed S-PSNR, which calculates PSNR based on a set of</text>
<text top="677" left="73" width="377" height="13" font="4">uniformly sampled points on a sphere instead of rectangu-</text>
<text top="695" left="73" width="377" height="13" font="4">larly mapped pixels. By applying interpolation algorithms, S-</text>
<text top="713" left="73" width="377" height="13" font="4">PSNR is able to generate objective quality assessments for</text>
<text top="731" left="73" width="377" height="13" font="4">encoded omnidirectional videos under different projections.</text>
<text top="749" left="73" width="377" height="13" font="4">Additionally, Zakharchenkoa et al. <a href="xml/1709.06342v2.html#13">[7] </a>proposed a weighted</text>
<text top="767" left="73" width="377" height="13" font="4">PSNR (W-PSNR) using gamma-corrected pixel values for the</text>
<text top="785" left="73" width="377" height="13" font="4">PSNR calculation process. Craster parabolic projection PSNR</text>
<text top="803" left="73" width="377" height="13" font="4">(CPP-PSNR) was also proposed to convert the projection of</text>
<text top="821" left="73" width="377" height="13" font="4">omnidirectional video to Craster parabolic projection (CPP)</text>
<text top="839" left="73" width="377" height="13" font="4">for calculating PSNR. The latest work of <a href="xml/1709.06342v2.html#14">[41] </a>conducted an ex-</text>
<text top="857" left="73" width="377" height="13" font="4">periment to evaluate the performance of several objective VQA</text>
<text top="874" left="73" width="377" height="13" font="4">methods on encoded omnidirectional images, via measuring</text>
<text top="892" left="73" width="377" height="13" font="4">the correlation between objective and subjective quality. The</text>
<text top="910" left="73" width="377" height="13" font="4">experimental results reveal that the VQA methods designed for</text>
<text top="928" left="73" width="377" height="13" font="4">omnidirectional content slightly outperform traditional VQA</text>
<text top="946" left="73" width="377" height="13" font="4">methods for 2D content. This finding is probably because none</text>
<text top="964" left="73" width="377" height="13" font="4">of the existing VQA methods explores the human perception</text>
<text top="982" left="73" width="377" height="13" font="4">model for omnidirectional video, in which only content inside</text>
<text top="1000" left="73" width="377" height="13" font="4">FoV is accessible. Therefore, this paper further proposes</text>
<text top="1018" left="73" width="377" height="13" font="4">to objectively assess visual quality of omnidirectional video</text>
<text top="1036" left="73" width="377" height="13" font="4">coding, by considering the FoV of possible viewing directions.</text>
<text top="1069" left="73" width="348" height="13" font="4">C. Related work on database of omnidirectional content</text>
<text top="1091" left="88" width="362" height="13" font="4">Since our VQA methods rely on the analysis of human</text>
<text top="1109" left="73" width="377" height="13" font="4">behavior in viewing omnidirectional video, it is necessary to</text>
<text top="87" left="468" width="377" height="13" font="4">build a database consisting of human behavior data in om-</text>
<text top="104" left="468" width="377" height="13" font="4">nidirectional content. For omnidirectional images, there exist</text>
<text top="122" left="468" width="377" height="13" font="4">several works <a href="xml/1709.06342v2.html#14">[42], [43] </a>on collecting viewing direction and</text>
<text top="140" left="468" width="377" height="13" font="4">eye gaze data. For omnidirectional video, some databases were</text>
<text top="158" left="468" width="377" height="13" font="4">also built including the viewing direction data. Specifically,</text>
<text top="176" left="468" width="377" height="13" font="4">Corbillon et al. introduced a database <a href="xml/1709.06342v2.html#14">[44] </a>with the viewing</text>
<text top="194" left="468" width="377" height="13" font="4">direction data in omnidirectional video sequences. However,</text>
<text top="212" left="468" width="377" height="13" font="4">the number of omnidirectional sequences is only 7, too small</text>
<text top="230" left="468" width="377" height="13" font="4">for a thorough analysis of human behavior. Wu et al. <a href="xml/1709.06342v2.html#14">[45]</a></text>
<text top="248" left="468" width="377" height="13" font="4">presented a larger database, which includes 18 omnidirectional</text>
<text top="266" left="468" width="377" height="13" font="4">video sequences. The sequences in <a href="xml/1709.06342v2.html#14">[45] </a>are still insufficient</text>
<text top="284" left="468" width="377" height="13" font="4">with non-diverse content for thoroughly analyzing the viewing</text>
<text top="302" left="468" width="377" height="13" font="4">directions of humans in omnidirectional video. Therefore, we</text>
<text top="320" left="468" width="377" height="13" font="4">establish a new database that is composed of 48 omnidi-</text>
<text top="338" left="468" width="377" height="13" font="4">rectional video sequences with diverse content, in order to</text>
<text top="356" left="468" width="377" height="13" font="4">analyze viewing directions of different subjects. Based on the</text>
<text top="373" left="468" width="377" height="13" font="4">newly established database, we report some findings through</text>
<text top="391" left="468" width="377" height="13" font="4">the analysis of viewing directions across subjects. Our analysis</text>
<text top="409" left="468" width="377" height="13" font="4">mainly refers to human consistency in viewing omnidirectional</text>
<text top="427" left="468" width="377" height="13" font="4">video, which has not been discovered in existing works <a href="xml/1709.06342v2.html#13">[9],</a></text>
<text top="445" left="468" width="377" height="13" font="4"><a href="xml/1709.06342v2.html#13">[10] </a>on user behavior analysis of omnidirectional video. We</text>
<text top="463" left="468" width="377" height="13" font="4">also established a VQA database for omnidirectional video that</text>
<text top="481" left="468" width="377" height="13" font="4">consists of 12 raw sequences and 36 compressed sequences.</text>
<text top="499" left="468" width="377" height="13" font="4">With this database, fair performance evaluation among differ-</text>
<text top="517" left="468" width="244" height="13" font="4">ent VQA methods can be implemented.</text>
<text top="555" left="512" width="40" height="13" font="4">III. A</text>
<text top="557" left="553" width="247" height="11" font="5">NALYSIS OF CONSISTENCY IN VIEWING</text>
<text top="575" left="573" width="167" height="11" font="5">OMNIDIRECTIONAL VIDEO</text>
<text top="596" left="483" width="362" height="13" font="4">Due to its omnidirectionality, people cannot see the whole</text>
<text top="614" left="468" width="377" height="13" font="4">content of omnidirectional video from a single viewing po-</text>
<text top="632" left="468" width="377" height="13" font="4">sition for instance. Instead, they normally look around and</text>
<text top="650" left="468" width="377" height="13" font="4">focus on what attracts them. It is intuitive that there may exist</text>
<text top="668" left="468" width="377" height="13" font="4">consistency across different subjects in their viewing directions</text>
<text top="686" left="468" width="377" height="13" font="4">on watching omnidirectional video. Thus, this section mainly</text>
<text top="704" left="468" width="377" height="13" font="4">discusses the analysis of consistency in viewing omnidirec-</text>
<text top="721" left="468" width="76" height="13" font="4">tional video.</text>
<text top="763" left="468" width="78" height="13" font="4">A. Database</text>
<text top="786" left="483" width="362" height="13" font="4">We present a new database that contains viewing direction</text>
<text top="804" left="468" width="377" height="13" font="4">data from 40 subjects when watching omnidirectional video</text>
<text top="822" left="468" width="377" height="13" font="4">sequences. In all, there are 48 sequences of omnidirectional</text>
<text top="840" left="468" width="377" height="13" font="4">video in our database. To ensure good QoE, the resolution of</text>
<text top="858" left="468" width="377" height="13" font="4">the sequences is chosen beyond 3K (2880 × 1440) and up to</text>
<text top="876" left="468" width="377" height="13" font="4">8K (7680 × 3840). These sequences are diverse in terms of</text>
<text top="893" left="468" width="377" height="13" font="4">their content, and they can be categorized according to video</text>
<text top="911" left="468" width="377" height="13" font="4">content, as shown in Table <a href="xml/1709.06342v2.html#4">I. </a>All of these 48 sequences were</text>
<text top="929" left="468" width="377" height="13" font="4">downloaded from YouTube or VRCun. Then, the sequences</text>
<text top="947" left="468" width="377" height="13" font="4">were cut into short clips with durations ranging from 20 to 60</text>
<text top="965" left="468" width="377" height="13" font="4">seconds. Following <a href="xml/1709.06342v2.html#13">[15], [27], </a>the audio tracks were discarded</text>
<text top="983" left="468" width="276" height="13" font="4">to avoid the impacts of acoustic information.</text>
<text top="1001" left="483" width="362" height="13" font="4">We used the HTC Vive as the HMD and a software Virtual</text>
<text top="1019" left="468" width="377" height="13" font="4">Desktop (VD) as the omnidirectional video player. In total, 40</text>
<text top="1037" left="468" width="377" height="13" font="4">subjects (29 male and 11 female) participated in the experi-</text>
<text top="1055" left="468" width="377" height="13" font="4">ment. For each subject, all of the 48 sequences were displayed</text>
<text top="1073" left="468" width="377" height="13" font="4">in a random order. During the experiment, the subjects were</text>
<text top="1091" left="468" width="377" height="13" font="4">seated in a swivel chair and were allowed to turn around</text>
<text top="1109" left="468" width="377" height="13" font="4">freely, such that all regions of omnidirectional video were</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="1188" width="918">
	<fontspec id="7" size="7" family="Times" color="#000000"/>
	<fontspec id="8" size="2" family="Times" color="#4b4847"/>
	<fontspec id="9" size="3" family="Times" color="#4b4847"/>
	<fontspec id="10" size="3" family="Times" color="#4b4847"/>
	<fontspec id="11" size="3" family="Times" color="#000000"/>
<text top="40" left="839" width="5" height="9" font="0">4</text>
<text top="89" left="436" width="46" height="11" font="5">TABLE I</text>
<text top="102" left="313" width="9" height="11" font="5">O</text>
<text top="104" left="322" width="279" height="9" font="7">MNIDIRECTIONAL VIDEO TEST SEQUENCE CATEGORIES</text>
<text top="102" left="602" width="3" height="11" font="5">.</text>
<text top="114" left="134" width="45" height="10" font="5">Category</text>
<text top="114" left="244" width="127" height="10" font="5">Computer Animation (CA)</text>
<text top="114" left="388" width="36" height="10" font="5">Driving</text>
<text top="114" left="441" width="65" height="10" font="5">Action Sports</text>
<text top="114" left="523" width="30" height="10" font="5">Movie</text>
<text top="114" left="570" width="59" height="10" font="5">Video Game</text>
<text top="114" left="646" width="37" height="10" font="5">Scenery</text>
<text top="114" left="700" width="26" height="10" font="5">Show</text>
<text top="114" left="744" width="31" height="10" font="5">Others</text>
<text top="114" left="792" width="40" height="10" font="5">In Total</text>
<text top="128" left="86" width="140" height="10" font="5">Number of Video sequences</text>
<text top="128" left="304" width="6" height="10" font="5">6</text>
<text top="128" left="403" width="6" height="10" font="5">6</text>
<text top="128" left="470" width="6" height="10" font="5">6</text>
<text top="128" left="535" width="6" height="10" font="5">6</text>
<text top="128" left="596" width="6" height="10" font="5">6</text>
<text top="128" left="662" width="6" height="10" font="5">6</text>
<text top="128" left="711" width="6" height="10" font="5">6</text>
<text top="128" left="756" width="6" height="10" font="5">6</text>
<text top="128" left="806" width="12" height="10" font="5">48</text>
<text top="150" left="434" width="50" height="11" font="5">TABLE II</text>
<text top="164" left="158" width="17" height="11" font="5">CC</text>
<text top="165" left="178" width="236" height="9" font="7">OF VIEWING DIRECTION HEAT MAPS BETWEEN</text>
<text top="164" left="418" width="9" height="11" font="5">G</text>
<text top="165" left="427" width="33" height="9" font="7">ROUPS</text>
<text top="163" left="463" width="10" height="11" font="5">A</text>
<text top="165" left="476" width="22" height="9" font="7">AND</text>
<text top="163" left="501" width="10" height="11" font="5">B</text>
<text top="165" left="514" width="245" height="9" font="7">FOR EACH OMNIDIRECTIONAL VIDEO SEQUENCE</text>
<text top="179" left="85" width="22" height="9" font="0">Cate-</text>
<text top="190" left="87" width="19" height="9" font="0">gory</text>
<text top="185" left="140" width="25" height="9" font="0">Name</text>
<text top="185" left="219" width="14" height="9" font="0">CC</text>
<text top="179" left="271" width="22" height="9" font="0">Cate-</text>
<text top="190" left="272" width="19" height="9" font="0">gory</text>
<text top="185" left="330" width="25" height="9" font="0">Name</text>
<text top="185" left="411" width="14" height="9" font="0">CC</text>
<text top="179" left="461" width="22" height="9" font="0">Cate-</text>
<text top="190" left="463" width="19" height="9" font="0">gory</text>
<text top="185" left="529" width="25" height="9" font="0">Name</text>
<text top="185" left="619" width="14" height="9" font="0">CC</text>
<text top="179" left="669" width="22" height="9" font="0">Cate-</text>
<text top="190" left="671" width="19" height="9" font="0">gory</text>
<text top="185" left="724" width="25" height="9" font="0">Name</text>
<text top="185" left="802" width="14" height="9" font="0">CC</text>
<text top="234" left="99" width="0" height="9" font="0">CA</text>
<text top="202" left="124" width="55" height="9" font="0">AcerPredator</text>
<text top="202" left="199" width="55" height="9" font="0">0.839±0.087</text>
<text top="243" left="284" width="0" height="9" font="0">Dri</text>
<text top="230" left="284" width="0" height="9" font="0">ving</text>
<text top="202" left="323" width="37" height="9" font="0">AirShow</text>
<text top="202" left="391" width="55" height="9" font="0">0.783±0.078</text>
<text top="241" left="476" width="0" height="9" font="0">Others</text>
<text top="202" left="530" width="23" height="9" font="0">A380</text>
<text top="202" left="599" width="55" height="9" font="0">0.839±0.106</text>
<text top="254" left="684" width="0" height="9" font="0">V</text>
<text top="247" left="684" width="0" height="9" font="0">ideo</text>
<text top="225" left="684" width="0" height="9" font="0">Game</text>
<text top="202" left="730" width="13" height="9" font="0">CS</text>
<text top="202" left="782" width="55" height="9" font="0">0.819±0.084</text>
<text top="214" left="142" width="20" height="9" font="0">BFG</text>
<text top="214" left="199" width="55" height="9" font="0">0.644±0.146</text>
<text top="214" left="312" width="60" height="9" font="0">DrivingInAlps</text>
<text top="214" left="391" width="55" height="9" font="0">0.857±0.071</text>
<text top="214" left="510" width="62" height="9" font="0">CandyCarnival</text>
<text top="214" left="599" width="55" height="9" font="0">0.723±0.094</text>
<text top="214" left="724" width="25" height="9" font="0">Dota2</text>
<text top="214" left="782" width="55" height="9" font="0">0.714±0.103</text>
<text top="225" left="124" width="55" height="9" font="0">CMLauncher</text>
<text top="225" left="199" width="55" height="9" font="0">0.828±0.119</text>
<text top="225" left="322" width="41" height="9" font="0">F5Fighter</text>
<text top="225" left="391" width="55" height="9" font="0">0.592±0.126</text>
<text top="225" left="510" width="61" height="9" font="0">MercedesBenz</text>
<text top="225" left="599" width="55" height="9" font="0">0.592±0.133</text>
<text top="225" left="707" width="59" height="9" font="0">GalaxyOnFire</text>
<text top="225" left="782" width="55" height="9" font="0">0.762±0.084</text>
<text top="237" left="128" width="48" height="9" font="0">Cryogenian</text>
<text top="237" left="199" width="55" height="9" font="0">0.526±0.174</text>
<text top="237" left="323" width="38" height="9" font="0">HondaF1</text>
<text top="237" left="391" width="55" height="9" font="0">0.872±0.053</text>
<text top="237" left="522" width="39" height="9" font="0">RingMan</text>
<text top="237" left="599" width="55" height="9" font="0">0.897±0.054</text>
<text top="237" left="727" width="20" height="9" font="0">LOL</text>
<text top="237" left="782" width="55" height="9" font="0">0.724±0.097</text>
<text top="249" left="123" width="59" height="9" font="0">LoopUniverse</text>
<text top="249" left="199" width="55" height="9" font="0">0.779±0.078</text>
<text top="249" left="331" width="22" height="9" font="0">Rally</text>
<text top="249" left="391" width="55" height="9" font="0">0.867±0.047</text>
<text top="249" left="514" width="55" height="9" font="0">RioOlympics</text>
<text top="249" left="599" width="55" height="9" font="0">0.624±0.123</text>
<text top="249" left="729" width="16" height="9" font="0">MC</text>
<text top="249" left="782" width="55" height="9" font="0">0.726±0.115</text>
<text top="260" left="132" width="39" height="9" font="0">Pokemon</text>
<text top="260" left="199" width="55" height="9" font="0">0.607±0.182</text>
<text top="260" left="324" width="37" height="9" font="0">Supercar</text>
<text top="260" left="391" width="55" height="9" font="0">0.854±0.064</text>
<text top="260" left="512" width="58" height="9" font="0">VRBasketball</text>
<text top="260" left="599" width="55" height="9" font="0">0.770±0.105</text>
<text top="260" left="707" width="60" height="9" font="0">SuperMario64</text>
<text top="260" left="782" width="55" height="9" font="0">0.860±0.054</text>
<text top="311" left="99" width="0" height="9" font="0">Mo</text>
<text top="297" left="99" width="0" height="9" font="0">vie</text>
<text top="272" left="142" width="20" height="9" font="0">Help</text>
<text top="272" left="199" width="55" height="9" font="0">0.859±0.122</text>
<text top="314" left="284" width="0" height="9" font="0">Scenery</text>
<text top="272" left="323" width="38" height="9" font="0">Antarctic</text>
<text top="272" left="391" width="55" height="9" font="0">0.674±0.135</text>
<text top="309" left="476" width="0" height="9" font="0">Sho</text>
<text top="293" left="476" width="0" height="9" font="0">w</text>
<text top="272" left="523" width="36" height="9" font="0">BTSRun</text>
<text top="272" left="599" width="55" height="9" font="0">0.867±0.061</text>
<text top="326" left="683" width="0" height="9" font="0">Action</text>
<text top="295" left="683" width="0" height="9" font="0">Sports</text>
<text top="272" left="721" width="32" height="9" font="0">Gliding</text>
<text top="272" left="782" width="55" height="9" font="0">0.528±0.158</text>
<text top="284" left="138" width="29" height="9" font="0">IRobot</text>
<text top="284" left="199" width="55" height="9" font="0">0.771±0.078</text>
<text top="284" left="319" width="45" height="9" font="0">BlueWorld</text>
<text top="284" left="391" width="55" height="9" font="0">0.559±0.156</text>
<text top="284" left="526" width="30" height="9" font="0">Graffiti</text>
<text top="284" left="599" width="55" height="9" font="0">0.807±0.100</text>
<text top="284" left="712" width="49" height="9" font="0">Parachuting</text>
<text top="284" left="782" width="55" height="9" font="0">0.628±0.157</text>
<text top="296" left="134" width="35" height="9" font="0">Predator</text>
<text top="296" left="199" width="55" height="9" font="0">0.696±0.124</text>
<text top="296" left="329" width="25" height="9" font="0">Dubai</text>
<text top="296" left="391" width="55" height="9" font="0">0.646±0.133</text>
<text top="296" left="513" width="57" height="9" font="0">KasabianLive</text>
<text top="296" left="599" width="55" height="9" font="0">0.722±0.132</text>
<text top="296" left="708" width="57" height="9" font="0">RollerCoaster</text>
<text top="296" left="782" width="55" height="9" font="0">0.834±0.078</text>
<text top="307" left="128" width="48" height="9" font="0">ProjectSoul</text>
<text top="307" left="199" width="55" height="9" font="0">0.918±0.053</text>
<text top="307" left="330" width="25" height="9" font="0">Egypt</text>
<text top="307" left="391" width="55" height="9" font="0">0.665±0.131</text>
<text top="307" left="499" width="84" height="9" font="0">NotBeAloneTonight</text>
<text top="307" left="599" width="55" height="9" font="0">0.587±0.131</text>
<text top="307" left="723" width="27" height="9" font="0">Skiing</text>
<text top="307" left="782" width="55" height="9" font="0">0.766±0.104</text>
<text top="319" left="133" width="38" height="9" font="0">StarWars</text>
<text top="319" left="199" width="55" height="9" font="0">0.950±0.016</text>
<text top="319" left="318" width="47" height="9" font="0">StarryPolar</text>
<text top="319" left="391" width="55" height="9" font="0">0.495±0.152</text>
<text top="319" left="519" width="45" height="9" font="0">Symphony</text>
<text top="319" left="599" width="55" height="9" font="0">0.779±0.096</text>
<text top="319" left="721" width="30" height="9" font="0">Surfing</text>
<text top="319" left="782" width="55" height="9" font="0">0.830±0.096</text>
<text top="330" left="129" width="46" height="9" font="0">Terminator</text>
<text top="330" left="199" width="55" height="9" font="0">0.843±0.078</text>
<text top="330" left="309" width="67" height="9" font="0">WesternSichuan</text>
<text top="330" left="391" width="55" height="9" font="0">0.667±0.138</text>
<text top="330" left="512" width="58" height="9" font="0">VRBasketball</text>
<text top="330" left="599" width="55" height="9" font="0">0.770±0.105</text>
<text top="330" left="712" width="50" height="9" font="0">Waterskiing</text>
<text top="330" left="782" width="55" height="9" font="0">0.781±0.128</text>
<text top="343" left="118" width="31" height="9" font="0">Overall</text>
<text top="343" left="197" width="59" height="9" font="0">0.745± 0.114</text>
<text top="455" left="126" width="53" height="9" font="0">(a) HondaF1</text>
<text top="455" left="279" width="55" height="9" font="0">(b) RingMan</text>
<text top="455" left="423" width="72" height="9" font="0">(c) RollerCoaster</text>
<text top="455" left="586" width="53" height="9" font="0">(d) StarWars</text>
<text top="455" left="736" width="60" height="9" font="0">(e) Symphony</text>
<text top="469" left="73" width="771" height="11" font="5">Fig. 1. Heat maps of viewing directions on some selected sequences. Note that the heat maps are obtained via the Gaussian convolution of viewing direction</text>
<text top="483" left="73" width="656" height="11" font="5">data for all frames viewed by 40 subjects, and the results are shown together with one randomly selected frame from each sequence.</text>
<text top="517" left="73" width="377" height="13" font="4">accessible. Although the subjects are able to move horizontally</text>
<text top="535" left="73" width="377" height="13" font="4">with the swivel chair, the viewport is only determined by the</text>
<text top="553" left="73" width="377" height="13" font="4">viewing direction instead of the horizontal moving. Besides,</text>
<text top="571" left="73" width="377" height="13" font="4">to avoid eye fatigue and motion sickness <a href="xml/1709.06342v2.html#13">[4], </a>there was a</text>
<text top="589" left="73" width="377" height="13" font="4">5-minute interval between every 16-sequence session. With</text>
<text top="607" left="73" width="377" height="13" font="4">the support of the Vive software development kit (SDK),</text>
<text top="625" left="73" width="377" height="13" font="4">we were able to collect the posture data of subjects when</text>
<text top="643" left="73" width="377" height="13" font="4">viewing omnidirectional video. Then, the viewing direction</text>
<text top="661" left="73" width="377" height="13" font="4">data describing where subjects paid attention were obtained</text>
<text top="678" left="73" width="377" height="13" font="4">in the form of Euler angles, and only the inclination and</text>
<text top="696" left="73" width="377" height="13" font="4">azimuth angles were recorded. Based on the inclination and</text>
<text top="714" left="73" width="377" height="13" font="4">azimuth angles, viewing directions of each subject, in terms of</text>
<text top="732" left="73" width="377" height="13" font="4">longitude and latitude, were collected for the omnidirectional</text>
<text top="750" left="73" width="377" height="13" font="4">video sequences in our database. Our database is available at</text>
<text top="768" left="73" width="282" height="13" font="4"><a href="https://github.com/Archer-Tatsu/head-tracking">https://github.com/Archer-Tatsu/head-tracking.</a></text>
<text top="825" left="73" width="104" height="13" font="4">B. Data analysis</text>
<text top="854" left="88" width="362" height="13" font="4">We now analyze the viewing direction data in our database.</text>
<text top="872" left="73" width="377" height="13" font="4">First, we discard the viewing direction data of the first second</text>
<text top="890" left="73" width="377" height="13" font="4">in each sequence since the viewing directions of all subjects</text>
<text top="908" left="73" width="377" height="13" font="4">were initialized to be in the center of the front region. The</text>
<text top="926" left="73" width="377" height="13" font="4">remaining data are then used for our analysis. Our findings</text>
<text top="944" left="73" width="377" height="13" font="4">with the corresponding analysis are presented and analyzed as</text>
<text top="962" left="73" width="48" height="13" font="4">follows.</text>
<text top="982" left="88" width="362" height="13" font="4">Finding 1: When subjects are watching omnidirectional</text>
<text top="999" left="73" width="377" height="13" font="4">video, the longitude and latitude of their viewing directions</text>
<text top="1017" left="73" width="249" height="13" font="4">are almost uncorrelated with each other.</text>
<text top="1037" left="88" width="362" height="13" font="4">The viewing direction data we collected in Section <a href="xml/1709.06342v2.html#3">III-A</a></text>
<text top="1055" left="73" width="377" height="13" font="4">consist of two dimensions, i.e., the longitude and latitude</text>
<text top="1073" left="73" width="377" height="13" font="4">in a spherical coordinate system. Let ϕ and θ denote the</text>
<text top="1091" left="73" width="377" height="13" font="4">collections of the longitude and latitude of viewing directions,</text>
<text top="1109" left="73" width="377" height="13" font="4">respectively, from all omnidirectional video sequences in our</text>
<text top="618" left="502" width="6" height="7" font="8"><b>-80</b></text>
<text top="618" left="520" width="6" height="7" font="8"><b>-60</b></text>
<text top="618" left="537" width="6" height="7" font="8"><b>-40</b></text>
<text top="618" left="554" width="6" height="7" font="8"><b>-20</b></text>
<text top="618" left="573" width="2" height="7" font="8"><b>0</b></text>
<text top="618" left="589" width="5" height="7" font="8"><b>20</b></text>
<text top="618" left="607" width="5" height="7" font="8"><b>40</b></text>
<text top="618" left="624" width="5" height="7" font="8"><b>60</b></text>
<text top="618" left="641" width="5" height="7" font="8"><b>80</b></text>
<text top="624" left="551" width="50" height="8" font="9"><b>Latitude (in degree)</b></text>
<text top="613" left="493" width="2" height="7" font="8"><b>0</b></text>
<text top="600" left="485" width="11" height="7" font="8"><b>0.005</b></text>
<text top="586" left="487" width="8" height="7" font="8"><b>0.01</b></text>
<text top="573" left="485" width="11" height="7" font="8"><b>0.015</b></text>
<text top="560" left="487" width="8" height="7" font="8"><b>0.02</b></text>
<text top="546" left="485" width="11" height="7" font="8"><b>0.025</b></text>
<text top="533" left="487" width="8" height="7" font="8"><b>0.03</b></text>
<text top="602" left="482" width="0" height="8" font="10"><b>Relative frequency of viewing</b></text>
<text top="530" left="606" width="44" height="8" font="11"><b>R-Square: 0.9980</b></text>
<text top="618" left="688" width="9" height="7" font="8"><b>-150</b></text>
<text top="618" left="709" width="9" height="7" font="8"><b>-100</b></text>
<text top="618" left="732" width="6" height="7" font="8"><b>-50</b></text>
<text top="618" left="756" width="2" height="7" font="8"><b>0</b></text>
<text top="618" left="776" width="5" height="7" font="8"><b>50</b></text>
<text top="618" left="796" width="7" height="7" font="8"><b>100</b></text>
<text top="618" left="818" width="7" height="7" font="8"><b>150</b></text>
<text top="624" left="731" width="55" height="8" font="9"><b>Longitude (in degree)</b></text>
<text top="613" left="675" width="2" height="7" font="8"><b>0</b></text>
<text top="603" left="667" width="11" height="7" font="8"><b>0.002</b></text>
<text top="593" left="667" width="11" height="7" font="8"><b>0.004</b></text>
<text top="583" left="667" width="11" height="7" font="8"><b>0.006</b></text>
<text top="573" left="667" width="11" height="7" font="8"><b>0.008</b></text>
<text top="563" left="669" width="8" height="7" font="8"><b>0.01</b></text>
<text top="553" left="667" width="11" height="7" font="8"><b>0.012</b></text>
<text top="543" left="667" width="11" height="7" font="8"><b>0.014</b></text>
<text top="533" left="667" width="11" height="7" font="8"><b>0.016</b></text>
<text top="523" left="667" width="11" height="7" font="8"><b>0.018</b></text>
<text top="602" left="664" width="0" height="8" font="10"><b>Relative frequency of viewing</b></text>
<text top="530" left="789" width="44" height="8" font="11"><b>R-Square: 0.9941</b></text>
<text top="638" left="468" width="342" height="11" font="5">Fig. 2. Viewing direction frequency along the longitude and latitude.</text>
<text top="674" left="468" width="377" height="13" font="4">database. Then, the covariance between ϕ and θ can be</text>
<text top="691" left="468" width="132" height="13" font="4">calculated as follows,</text>
<text top="719" left="543" width="227" height="12" font="3">cov(ϕ, θ) = E[(ϕ − E(ϕ))(θ − E(θ))],</text>
<text top="719" left="829" width="16" height="12" font="3">(1)</text>
<text top="748" left="468" width="377" height="13" font="4">where E(·) is the expectation operator. Given the covariance</text>
<text top="766" left="468" width="377" height="13" font="4">of <a href="xml/1709.06342v2.html#4">(1), </a>the correlation between the longitude and latitude of</text>
<text top="784" left="468" width="236" height="13" font="4">viewing direction can be computed by</text>
<text top="819" left="570" width="57" height="12" font="3">ρ(ϕ, θ) =</text>
<text top="810" left="657" width="56" height="12" font="3">cov(ϕ, θ)</text>
<text top="828" left="632" width="105" height="15" font="3">pvar(ϕ)pvar(θ)</text>
<text top="819" left="740" width="3" height="12" font="3">,</text>
<text top="819" left="829" width="16" height="12" font="3">(2)</text>
<text top="857" left="468" width="377" height="13" font="4">where var(ϕ) and var(θ) are the variances of ϕ and θ,</text>
<text top="875" left="468" width="377" height="13" font="4">respectively. In our database, we have ρ(ϕ, θ) = −0.0337 for</text>
<text top="893" left="468" width="377" height="13" font="4">the viewing directions of all omnidirectional video sequences.</text>
<text top="911" left="468" width="377" height="13" font="4">Since such a value of ρ(ϕ, θ) is close to 0, the correlation be-</text>
<text top="929" left="468" width="377" height="13" font="4">tween the longitude and latitude of human viewing directions</text>
<text top="946" left="468" width="377" height="13" font="4">in omnidirectional video is rather small. This completes the</text>
<text top="964" left="468" width="134" height="13" font="4">analysis of Finding 1.</text>
<text top="983" left="483" width="362" height="13" font="4">Finding 2: When watching omnidirectional video, subjects</text>
<text top="1001" left="468" width="377" height="13" font="4">view the front region near the equator much more frequently</text>
<text top="1019" left="468" width="116" height="13" font="4">than other regions.</text>
<text top="1037" left="483" width="362" height="13" font="4">Figure <a href="xml/1709.06342v2.html#4">1 </a>shows the heat maps of viewing directions for some</text>
<text top="1055" left="468" width="377" height="13" font="4">omnidirectional video sequences, as obtained from all 40 sub-</text>
<text top="1073" left="468" width="377" height="13" font="4">jects. Note that the heat maps in Figure <a href="xml/1709.06342v2.html#4">1 </a>have been converted</text>
<text top="1091" left="468" width="377" height="13" font="4">from spherical coordinates to a plane for omnidirectional</text>
<text top="1109" left="468" width="377" height="13" font="4">video sequences <a href="xml/1709.06342v2.html#14">[46]. </a>We can see from this figure that most</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="1188" width="918">
	<fontspec id="12" size="6" family="Times" color="#000000"/>
<text top="40" left="839" width="5" height="9" font="0">5</text>
<text top="180" left="114" width="77" height="9" font="0">(a) CandyCarnival</text>
<text top="180" left="290" width="32" height="9" font="0">(b) MC</text>
<text top="180" left="441" width="35" height="9" font="0">(c) Help</text>
<text top="180" left="581" width="62" height="9" font="0">(d) StarryPolar</text>
<text top="180" left="716" width="98" height="9" font="0">(e) NotBeAloneTonight</text>
<text top="194" left="73" width="744" height="11" font="5">Fig. 3. Viewing direction heat maps for selected frames from a few omnidirectional video sequences, in which subjects are attracted by other regions.</text>
<text top="229" left="73" width="377" height="13" font="4">viewing directions fall into small regions located in the front</text>
<text top="246" left="73" width="377" height="13" font="4">region near the equator. Furthermore, we calculate the viewing</text>
<text top="264" left="73" width="377" height="13" font="4">directions belonging to different regions of the omnidirectional</text>
<text top="282" left="73" width="377" height="13" font="4">videos. Since Finding 1 illustrated that the longitude and</text>
<text top="300" left="73" width="377" height="13" font="4">latitude of viewing directions are almost uncorrelated with</text>
<text top="318" left="73" width="377" height="13" font="4">each other, it is reasonable to separately model the distribution</text>
<text top="336" left="73" width="377" height="13" font="4">of viewing directions along the longitude and latitude. To</text>
<text top="354" left="73" width="377" height="13" font="4">this end, Figure <a href="xml/1709.06342v2.html#4">2 </a>shows the scatter diagrams of viewing</text>
<text top="372" left="73" width="377" height="13" font="4">direction frequency along the longitude and latitude, averaged</text>
<text top="390" left="73" width="377" height="13" font="4">over all subjects and all omnidirectional video sequences. In</text>
<text top="408" left="73" width="377" height="13" font="4">this figure, Gaussian mixture fitting curves are also plotted.</text>
<text top="426" left="73" width="377" height="13" font="4">According to this figure, we can see that subjects tend to watch</text>
<text top="444" left="73" width="377" height="13" font="4">regions near the front and equator regions, far more often</text>
<text top="462" left="73" width="377" height="13" font="4">than the back and pole regions. This completes the analysis</text>
<text top="480" left="73" width="346" height="13" font="4">of Finding 2, which is similar to the conclusion of <a href="xml/1709.06342v2.html#13">[16].</a></text>
<text top="499" left="88" width="362" height="13" font="4">Finding 3: In general, there exists high consistency in the</text>
<text top="517" left="73" width="377" height="13" font="4">viewed regions across different subjects for omnidirectional</text>
<text top="535" left="73" width="36" height="13" font="4">video.</text>
<text top="554" left="88" width="362" height="13" font="4">We randomly and equally divide all 40 subjects into two</text>
<text top="572" left="73" width="377" height="13" font="4">non-overlapping groups, A and B. Then, we generate heat</text>
<text top="590" left="73" width="377" height="13" font="4">maps of viewing directions at one omnidirectional video frame</text>
<text top="608" left="73" width="303" height="13" font="4">for Groups A and B, which are denoted as H</text>
<text top="613" left="377" width="9" height="9" font="0">A</text>
<text top="608" left="394" width="42" height="13" font="4">and H</text>
<text top="613" left="436" width="9" height="9" font="0">B</text>
<text top="608" left="446" width="4" height="13" font="4">,</text>
<text top="626" left="73" width="282" height="13" font="4">respectively. Note that the heat maps for H</text>
<text top="631" left="355" width="9" height="9" font="0">A</text>
<text top="626" left="372" width="42" height="13" font="4">and H</text>
<text top="631" left="414" width="9" height="9" font="0">B</text>
<text top="626" left="432" width="18" height="13" font="4">are</text>
<text top="644" left="73" width="377" height="13" font="4">in plane coordinates, in which the omnidirectional video has</text>
<text top="662" left="73" width="377" height="13" font="4">been projected from sphere to plane. Here, we quantify the</text>
<text top="680" left="73" width="219" height="13" font="4">correlations of the heat maps of H</text>
<text top="685" left="292" width="9" height="9" font="0">A</text>
<text top="680" left="308" width="41" height="13" font="4">and H</text>
<text top="685" left="349" width="9" height="9" font="0">B</text>
<text top="680" left="365" width="85" height="13" font="4">using a linear</text>
<text top="698" left="73" width="377" height="13" font="4">correlation coefficient (CC) <a href="xml/1709.06342v2.html#14">[47]. </a>Specifically, CC is calculated</text>
<text top="716" left="73" width="15" height="13" font="4">by</text>
<text top="747" left="128" width="38" height="12" font="3">CC(H</text>
<text top="752" left="165" width="8" height="8" font="12">A</text>
<text top="747" left="174" width="19" height="12" font="3">, H</text>
<text top="752" left="193" width="8" height="8" font="12">B</text>
<text top="747" left="202" width="20" height="12" font="3">) =</text>
<text top="765" left="129" width="15" height="6" font="3">P</text>
<text top="774" left="144" width="13" height="8" font="12">s,t</text>
<text top="766" left="158" width="18" height="12" font="3">(H</text>
<text top="771" left="175" width="8" height="8" font="12">A</text>
<text top="766" left="185" width="67" height="12" font="3">(s, t)−µ(H</text>
<text top="771" left="251" width="8" height="8" font="12">A</text>
<text top="766" left="260" width="34" height="12" font="3">))·(H</text>
<text top="771" left="294" width="8" height="8" font="12">B</text>
<text top="766" left="304" width="67" height="12" font="3">(s, t)−µ(H</text>
<text top="771" left="370" width="8" height="8" font="12">B</text>
<text top="766" left="380" width="11" height="12" font="3">))</text>
<text top="786" left="201" width="40" height="15" font="3">pσ(H</text>
<text top="793" left="241" width="8" height="8" font="12">A</text>
<text top="789" left="250" width="5" height="12" font="3">)</text>
<text top="788" left="256" width="5" height="8" font="12">2</text>
<text top="788" left="265" width="33" height="13" font="3">· σ(H</text>
<text top="793" left="298" width="8" height="8" font="12">B</text>
<text top="789" left="307" width="5" height="12" font="3">)</text>
<text top="788" left="313" width="5" height="8" font="12">2</text>
<text top="778" left="392" width="3" height="12" font="3">,</text>
<text top="768" left="434" width="16" height="12" font="3">(3)</text>
<text top="820" left="73" width="377" height="13" font="4">where (s, t) is the pixel coordinate, and µ(·) and σ(·) are the</text>
<text top="838" left="73" width="377" height="13" font="4">mean and standard deviation of the corresponding heat maps,</text>
<text top="856" left="73" width="377" height="13" font="4">respectively. A CC (ranging in [−1, 1]) close to +1 indicates</text>
<text top="874" left="73" width="301" height="13" font="4">a high consistency between the heat maps H</text>
<text top="879" left="374" width="9" height="9" font="0">A</text>
<text top="874" left="392" width="44" height="13" font="4">and H</text>
<text top="879" left="436" width="9" height="9" font="0">B</text>
<text top="874" left="446" width="4" height="13" font="4">.</text>
<text top="892" left="73" width="377" height="13" font="4">Table <a href="xml/1709.06342v2.html#4">II </a>reports the mean values and standard deviations of</text>
<text top="910" left="73" width="377" height="13" font="4">the CC of the viewing direction heat maps for each sequence,</text>
<text top="928" left="73" width="377" height="13" font="4">between Groups A and B. We can see from this table that</text>
<text top="946" left="73" width="377" height="13" font="4">the CC values are sufficiently high across different sequences.</text>
<text top="964" left="73" width="377" height="13" font="4">We can also see from this table that the CC value averaged</text>
<text top="982" left="73" width="377" height="13" font="4">over all 48 omnidirectional video sequences is 0.745, with a</text>
<text top="1000" left="73" width="377" height="13" font="4">standard deviation of 0.114. Thus, it is clear that the subjects</text>
<text top="1017" left="73" width="377" height="13" font="4">behaved consistently when watching omnidirectional video.</text>
<text top="1035" left="73" width="255" height="13" font="4">This completes the analysis of Finding 3.</text>
<text top="1055" left="88" width="362" height="13" font="4">Finding 4: The viewing directions of different subjects</text>
<text top="1073" left="73" width="377" height="13" font="4">are consistent in different regions according to content of</text>
<text top="1091" left="73" width="377" height="13" font="4">omnidirectional video, despite being more likely to be attracted</text>
<text top="1109" left="73" width="180" height="13" font="4">by equator and front regions.</text>
<text top="330" left="555" width="12" height="10" font="0">(a)</text>
<text top="330" left="741" width="13" height="10" font="0">(b)</text>
<text top="347" left="468" width="377" height="11" font="5">Fig. 4. (a) The GUI of our quality rating software. Subjects drag the red</text>
<text top="361" left="468" width="377" height="11" font="5">cursor on the continuous-scale slider with the controller to rate the scores. (b)</text>
<text top="374" left="468" width="377" height="11" font="5">The GUI in HMD projected by VD. Note that each of the left and right halves</text>
<text top="387" left="468" width="377" height="11" font="5">of the figure shows the picture in the corresponding eye in HMD, respectively.</text>
<text top="422" left="483" width="362" height="13" font="4">The scatter diagrams of Figure <a href="xml/1709.06342v2.html#4">2 </a>also reveal that the</text>
<text top="440" left="468" width="377" height="13" font="4">regions other than the front and equator, still have potential</text>
<text top="458" left="468" width="377" height="13" font="4">in attracting human attention. Figure <a href="xml/1709.06342v2.html#5">3 </a>demonstrates that the</text>
<text top="476" left="468" width="377" height="13" font="4">selected frames of several omnidirectional video sequences</text>
<text top="494" left="468" width="377" height="13" font="4">and their corresponding heat maps of viewing directions. We</text>
<text top="512" left="468" width="377" height="13" font="4">can see from Figure <a href="xml/1709.06342v2.html#5">3 </a>that the viewing directions may focus</text>
<text top="530" left="468" width="377" height="13" font="4">on different regions of omnidirectional video rather than the</text>
<text top="548" left="468" width="377" height="13" font="4">front equator, depending on the video content. For example,</text>
<text top="566" left="468" width="377" height="13" font="4">Figure <a href="xml/1709.06342v2.html#5">3(c) </a>shows that viewing directions concentrate on the</text>
<text top="584" left="468" width="377" height="13" font="4">corridor and people at the left hand side. This completes the</text>
<text top="602" left="468" width="144" height="13" font="4">analysis of this finding.</text>
<text top="642" left="554" width="36" height="13" font="4">IV. S</text>
<text top="644" left="591" width="70" height="11" font="5">UBJECTIVE</text>
<text top="642" left="665" width="34" height="13" font="4">VQA</text>
<text top="644" left="703" width="55" height="11" font="5">METHOD</text>
<text top="665" left="483" width="362" height="13" font="4">In this section, we introduce our subjective VQA method</text>
<text top="683" left="468" width="377" height="13" font="4">for omnidirectional video coding. In Section <a href="xml/1709.06342v2.html#5">IV-A, </a>we present</text>
<text top="701" left="468" width="377" height="13" font="4">the general configuration of the subjective test for our VQA</text>
<text top="719" left="468" width="377" height="13" font="4">method. In Section <a href="xml/1709.06342v2.html#6">IV-B, </a>the procedure of the subjective</text>
<text top="737" left="468" width="377" height="13" font="4">test is discussed for rating the raw quality scores of each</text>
<text top="755" left="468" width="377" height="13" font="4">omnidirectional video sequence. In Section <a href="xml/1709.06342v2.html#6">IV-C, </a>O-DMOS</text>
<text top="773" left="468" width="377" height="13" font="4">and V-DMOS are proposed as the metrics to assess subjective</text>
<text top="791" left="468" width="377" height="13" font="4">quality of omnidirectional video coding, which are based on</text>
<text top="809" left="468" width="377" height="13" font="4">the raw scores of reference and impaired omnidirectional</text>
<text top="827" left="468" width="43" height="13" font="4">videos.</text>
<text top="870" left="468" width="129" height="13" font="4">A. Test configuration</text>
<text top="893" left="483" width="362" height="13" font="4">Omnidirectional video differs from 2D video in the display</text>
<text top="911" left="468" width="377" height="13" font="4">devices, the viewing experience of subjects, etc. Thus, we</text>
<text top="929" left="468" width="377" height="13" font="4">design the test configuration for the subjective test on assessing</text>
<text top="947" left="468" width="377" height="13" font="4">omnidirectional video, which differs from the test for 2D</text>
<text top="965" left="468" width="377" height="13" font="4">video. In the following, we present the general configuration</text>
<text top="983" left="468" width="377" height="13" font="4">of the subjective test, including display devices and the setup</text>
<text top="1001" left="468" width="75" height="13" font="4">for subjects.</text>
<text top="1019" left="483" width="362" height="14" font="4">Display devices. An HMD with a corresponding video</text>
<text top="1037" left="468" width="377" height="13" font="4">player is used to display omnidirectional video, rather than</text>
<text top="1055" left="468" width="377" height="13" font="4">flat screens for displaying 2D video. This configuration is</text>
<text top="1073" left="468" width="377" height="13" font="4">because most omnidirectional videos are viewed by wearing</text>
<text top="1091" left="468" width="377" height="13" font="4">an HMD. In this paper, we use the HTC Vive as the display</text>
<text top="1109" left="468" width="377" height="13" font="4">device of HMD and the software VD as the omnidirectional</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="1188" width="918">
	<fontspec id="13" size="7" family="Times" color="#ffffff"/>
	<fontspec id="14" size="8" family="Times" color="#000000"/>
	<fontspec id="15" size="4" family="Times" color="#000000"/>
	<fontspec id="16" size="4" family="Times" color="#000000"/>
	<fontspec id="17" size="5" family="Times" color="#000000"/>
<text top="40" left="839" width="5" height="9" font="0">6</text>
<text top="87" left="73" width="377" height="13" font="4">video player. Additionally, as shown in Figure <a href="xml/1709.06342v2.html#5">4(b), </a>VD is</text>
<text top="104" left="73" width="377" height="13" font="4">also used to project the graphical user interface (GUI) of our</text>
<text top="122" left="73" width="377" height="13" font="4">quality rating software in the HMD. This allows the subjects to</text>
<text top="140" left="73" width="377" height="13" font="4">rate omnidirectional video without taking off the HMD. Since</text>
<text top="158" left="73" width="377" height="13" font="4">omnidirectional video can be viewed from different viewing</text>
<text top="176" left="73" width="377" height="13" font="4">directions, a swivel chair is provided to subjects when viewing</text>
<text top="194" left="73" width="166" height="13" font="4">the omnidirectional videos.</text>
<text top="212" left="88" width="362" height="14" font="4">Subjects. According to Finding 3, the viewing directions of</text>
<text top="231" left="73" width="377" height="13" font="4">subjects are highly consistent. Therefore, fixing the viewing re-</text>
<text top="248" left="73" width="377" height="13" font="4">gions of omnidirectional video in <a href="xml/1709.06342v2.html#13">[7] </a>is not necessary. Instead,</text>
<text top="266" left="73" width="377" height="13" font="4">subjects are able to freely view all content of omnidirectional</text>
<text top="284" left="73" width="377" height="13" font="4">video in our subjective test. This way, our method satisfies</text>
<text top="302" left="73" width="377" height="13" font="4">daily visual experience that subjects are free to access all</text>
<text top="320" left="73" width="377" height="13" font="4">parts of omnidirectional video. In addition, the initialization of</text>
<text top="338" left="73" width="377" height="13" font="4">viewing direction is required when watching omnidirectional</text>
<text top="356" left="73" width="377" height="13" font="4">video, which is different from viewing 2D video. In our</text>
<text top="374" left="73" width="377" height="13" font="4">test, the viewing directions of all subjects are initialized to</text>
<text top="392" left="73" width="377" height="13" font="4">be the center of front region in omnidirectional video, as</text>
<text top="410" left="73" width="60" height="13" font="4">Finding 2</text>
<text top="410" left="140" width="310" height="13" font="4">finds that subjects are more likely to be attracted</text>
<text top="428" left="73" width="377" height="13" font="4">by this region. However, there still exists slight inconsistency</text>
<text top="446" left="73" width="377" height="13" font="4">of viewed regions in omnidirectional video as analyzed in</text>
<text top="464" left="73" width="67" height="13" font="4">Findings 4</text>
<text top="464" left="141" width="309" height="13" font="4">. Thus, more subjects should be involved in the</text>
<text top="482" left="73" width="377" height="13" font="4">subjective test for rating quality of omnidirectional video, than</text>
<text top="500" left="73" width="377" height="13" font="4">at least 15 subjects required in <a href="xml/1709.06342v2.html#13">[13]. </a>We recommend that</text>
<text top="517" left="73" width="377" height="13" font="4">at least 20 subjects are required for rating quality scores of</text>
<text top="535" left="73" width="313" height="13" font="4">omnidirectional video, as verified in Section <a href="xml/1709.06342v2.html#9">VI-A.</a></text>
<text top="582" left="73" width="109" height="13" font="4">B. Test procedure</text>
<text top="606" left="88" width="362" height="14" font="4">Training and test. Generally speaking, the test procedure</text>
<text top="624" left="73" width="377" height="13" font="4">of our subjective VQA method comprises two sessions, the</text>
<text top="642" left="73" width="377" height="13" font="4">training and test sessions, as shown in Figure <a href="xml/1709.06342v2.html#6">5. </a>The training</text>
<text top="660" left="73" width="377" height="13" font="4">session is introduced, as some subjects may be unfamiliar</text>
<text top="678" left="73" width="377" height="13" font="4">with viewing omnidirectional video. In the training session,</text>
<text top="696" left="73" width="377" height="13" font="4">subjects are told about the goal of our test. Then, they watch</text>
<text top="714" left="73" width="377" height="13" font="4">a group of training sequences at different quality levels in</text>
<text top="732" left="73" width="377" height="13" font="4">order to become familiar with omnidirectional video and its</text>
<text top="749" left="73" width="377" height="13" font="4">quality. Afterwards, a short break is required before entering</text>
<text top="767" left="73" width="377" height="13" font="4">the test session. In the test session, each sequence is displayed</text>
<text top="785" left="73" width="377" height="13" font="4">followed by a 3-second mid-grey screen. Compared with</text>
<text top="803" left="73" width="377" height="13" font="4">viewing 2D video, subjects are more likely to experience eye</text>
<text top="821" left="73" width="377" height="13" font="4">fatigue and motion sickness when watching omnidirectional</text>
<text top="839" left="73" width="377" height="13" font="4">video. Thus, the maximum duration of a test session is limited</text>
<text top="857" left="73" width="377" height="13" font="4">to 30 minutes, which is the lower bound of recommended</text>
<text top="875" left="73" width="377" height="13" font="4">duration for 2D video in <a href="xml/1709.06342v2.html#13">[13]. </a>If the test session lasts more</text>
<text top="893" left="73" width="377" height="13" font="4">than 30 minutes, a short break (at least 3 minutes) with the</text>
<text top="911" left="73" width="269" height="13" font="4">HMD taken off is added in the test session.</text>
<text top="929" left="88" width="362" height="14" font="4">Quality rating. In the subjective test, SSCQS is adopted as</text>
<text top="947" left="73" width="377" height="13" font="4">shown in Figure <a href="xml/1709.06342v2.html#6">5, </a>which means that omnidirectional video</text>
<text top="965" left="73" width="377" height="13" font="4">sequences are displayed in a random order and that sequences</text>
<text top="983" left="73" width="377" height="13" font="4">with the same content at different quality need to be avoided</text>
<text top="1001" left="73" width="377" height="13" font="4">for two successive sequences. The reason for choosing SSCQS</text>
<text top="1019" left="73" width="377" height="13" font="4">is that the subjects may continue to view unseen regions when</text>
<text top="1037" left="73" width="377" height="13" font="4">viewing omnidirectional video with the same content, which</text>
<text top="1055" left="73" width="377" height="13" font="4">differs from the viewing characteristics of 2D video. After</text>
<text top="1073" left="73" width="377" height="13" font="4">viewing each sequence, subjects are required to rate its quality.</text>
<text top="1091" left="73" width="377" height="13" font="4">Note that there is one quality score to rate for each video. As</text>
<text top="1109" left="73" width="377" height="13" font="4">shown in Figure <a href="xml/1709.06342v2.html#5">4(a), </a>the grading scores in the test session</text>
<text top="99" left="623" width="7" height="13" font="13"><b>C</b></text>
<text top="110" left="624" width="5" height="13" font="13"><b>1</b></text>
<text top="80" left="510" width="73" height="14" font="14"><b>Training Session</b></text>
<text top="80" left="679" width="54" height="14" font="14"><b>Test Session</b></text>
<text top="131" left="585" width="35" height="9" font="15"><b>Break Time</b></text>
<text top="148" left="657" width="36" height="9" font="16">T1 T2 Rate</text>
<text top="99" left="654" width="7" height="13" font="13"><b>C</b></text>
<text top="110" left="655" width="5" height="13" font="13"><b>2</b></text>
<text top="99" left="780" width="7" height="13" font="13"><b>C</b></text>
<text top="110" left="781" width="5" height="13" font="13"><b>3</b></text>
<text top="99" left="796" width="6" height="13" font="13"><b>B</b></text>
<text top="110" left="797" width="5" height="13" font="13"><b>1</b></text>
<text top="99" left="639" width="7" height="13" font="13"><b>A</b></text>
<text top="110" left="640" width="5" height="13" font="13"><b>1</b></text>
<text top="99" left="702" width="7" height="13" font="13"><b>A</b></text>
<text top="110" left="703" width="5" height="13" font="13"><b>2</b></text>
<text top="99" left="670" width="7" height="13" font="13"><b>A</b></text>
<text top="110" left="671" width="5" height="13" font="13"><b>3</b></text>
<text top="99" left="764" width="6" height="13" font="13"><b>B</b></text>
<text top="110" left="765" width="5" height="13" font="13"><b>2</b></text>
<text top="99" left="686" width="6" height="13" font="13"><b>B</b></text>
<text top="110" left="687" width="5" height="13" font="13"><b>3</b></text>
<text top="165" left="468" width="35" height="11" font="5">Fig. 5.</text>
<text top="165" left="516" width="329" height="11" font="5">Structure of the test procedure with two sessions. The SSCQS</text>
<text top="179" left="468" width="377" height="11" font="5">procedure is illustrated in part of the test session. Ai, Bi and Ci represent</text>
<text top="192" left="468" width="377" height="11" font="5">various original and impaired sequences from different contents A, B and C,</text>
<text top="206" left="468" width="60" height="11" font="5">respectively.</text>
<text top="237" left="468" width="377" height="13" font="4">are available using a continuous-scale slider with a cursor in</text>
<text top="255" left="468" width="377" height="13" font="4">the GUI for quality rating. The score Q has a range from 0 to</text>
<text top="273" left="468" width="377" height="13" font="4">100, in the form of 5 levels: excellent (80 ≤ Q ≤ 100), good</text>
<text top="291" left="468" width="377" height="13" font="4">(60 ≤ Q &lt; 80), fair (40 ≤ Q &lt; 60), poor (20 ≤ Q &lt; 40) and bad</text>
<text top="309" left="468" width="78" height="13" font="4">(0 ≤ Q &lt; 20).</text>
<text top="327" left="483" width="362" height="14" font="4">Data collection. There are two kinds of data to be collected</text>
<text top="345" left="468" width="377" height="13" font="4">and processed, including the raw subjective quality scores of</text>
<text top="363" left="468" width="377" height="13" font="4">the omnidirectional video sequences as mentioned above. The</text>
<text top="381" left="468" width="377" height="13" font="4">other is the viewing direction data of subjects during sequence</text>
<text top="399" left="468" width="377" height="13" font="4">playback, which relate the quality score to the regions of</text>
<text top="417" left="468" width="377" height="13" font="4">omnidirectional video that were viewed. This also enables the</text>
<text top="435" left="468" width="283" height="13" font="4">calculation of V-DMOS, to be discussed next.</text>
<text top="477" left="468" width="212" height="13" font="4">C. Processing of subjective scores</text>
<text top="499" left="483" width="362" height="14" font="4">O-DMOS. Given the raw quality scores for each sequence,</text>
<text top="517" left="468" width="377" height="13" font="4">we follow the DMOS calculation method of 2D video as</text>
<text top="535" left="468" width="377" height="13" font="4">detailed in <a href="xml/1709.06342v2.html#13">[27] </a>to compute the O-DMOS, which indicates</text>
<text top="553" left="468" width="377" height="13" font="4">the overall quality of each omnidirectional video sequence.</text>
<text top="571" left="468" width="377" height="13" font="4">Specifically, the difference in the quality scores between</text>
<text top="589" left="468" width="377" height="13" font="4">the reference and impaired sequences is calculated for each</text>
<text top="607" left="468" width="90" height="13" font="4">subject. Let S</text>
<text top="612" left="558" width="9" height="9" font="0">ij</text>
<text top="607" left="576" width="38" height="13" font="4">and S</text>
<text top="604" left="615" width="14" height="9" font="0">ref</text>
<text top="614" left="614" width="9" height="9" font="0">ij</text>
<text top="607" left="637" width="207" height="13" font="4">denote the raw subjective scores</text>
<text top="625" left="468" width="377" height="13" font="4">assigned by subject i to sequence j and the corresponding</text>
<text top="643" left="468" width="377" height="13" font="4">reference sequence, respectively. Then, the difference score</text>
<text top="660" left="468" width="8" height="13" font="4">d</text>
<text top="666" left="476" width="9" height="9" font="0">ij</text>
<text top="661" left="492" width="163" height="13" font="4">can be simply obtained by</text>
<text top="689" left="608" width="7" height="12" font="3">d</text>
<text top="693" left="615" width="9" height="8" font="12">ij</text>
<text top="689" left="628" width="23" height="12" font="3">= S</text>
<text top="685" left="652" width="13" height="8" font="12">ref</text>
<text top="694" left="651" width="9" height="8" font="12">ij</text>
<text top="688" left="669" width="22" height="13" font="3">− S</text>
<text top="693" left="692" width="9" height="8" font="12">ij</text>
<text top="689" left="702" width="3" height="12" font="3">.</text>
<text top="689" left="829" width="16" height="12" font="3">(4)</text>
<text top="716" left="468" width="210" height="13" font="4">Afterwards, the difference score d</text>
<text top="721" left="678" width="9" height="9" font="0">ij</text>
<text top="716" left="694" width="151" height="13" font="4">needs to be converted to</text>
<text top="734" left="468" width="73" height="13" font="4">a Z-score Z</text>
<text top="739" left="541" width="9" height="9" font="0">ij</text>
<text top="734" left="557" width="63" height="13" font="4"><a href="xml/1709.06342v2.html#14">[48] </a>using</text>
<text top="779" left="509" width="8" height="12" font="3">µ</text>
<text top="783" left="517" width="4" height="8" font="12">i</text>
<text top="779" left="526" width="11" height="12" font="3">=</text>
<text top="770" left="548" width="7" height="12" font="3">1</text>
<text top="788" left="542" width="13" height="12" font="3">M</text>
<text top="793" left="555" width="4" height="8" font="12">i</text>
<text top="764" left="567" width="10" height="8" font="12">M</text>
<text top="767" left="577" width="4" height="7" font="17">i</text>
<text top="775" left="564" width="20" height="6" font="3">X</text>
<text top="797" left="565" width="19" height="8" font="12">j=1</text>
<text top="779" left="586" width="7" height="12" font="3">d</text>
<text top="783" left="594" width="9" height="8" font="12">ij</text>
<text top="779" left="603" width="4" height="12" font="3">,</text>
<text top="779" left="623" width="8" height="12" font="3">σ</text>
<text top="783" left="631" width="4" height="8" font="12">i</text>
<text top="779" left="640" width="11" height="12" font="3">=</text>
<text top="759" left="654" width="15" height="6" font="3">v</text>
<text top="767" left="654" width="15" height="6" font="3">u</text>
<text top="775" left="654" width="15" height="6" font="3">u</text>
<text top="783" left="654" width="15" height="6" font="3">t</text>
<text top="770" left="688" width="7" height="12" font="3">1</text>
<text top="788" left="671" width="13" height="12" font="3">M</text>
<text top="793" left="684" width="4" height="8" font="12">i</text>
<text top="787" left="692" width="21" height="13" font="3">− 1</text>
<text top="764" left="719" width="10" height="8" font="12">M</text>
<text top="767" left="729" width="4" height="7" font="17">i</text>
<text top="775" left="717" width="20" height="6" font="3">X</text>
<text top="797" left="717" width="19" height="8" font="12">j=1</text>
<text top="779" left="736" width="13" height="12" font="3">(d</text>
<text top="783" left="749" width="9" height="8" font="12">ij</text>
<text top="778" left="762" width="22" height="13" font="3">− µ</text>
<text top="783" left="784" width="4" height="8" font="12">i</text>
<text top="779" left="789" width="5" height="12" font="3">)</text>
<text top="778" left="794" width="5" height="8" font="12">2</text>
<text top="779" left="800" width="3" height="12" font="3">,</text>
<text top="779" left="829" width="16" height="12" font="3">(5)</text>
<text top="827" left="610" width="9" height="12" font="3">Z</text>
<text top="832" left="620" width="9" height="8" font="12">ij</text>
<text top="827" left="634" width="11" height="12" font="3">=</text>
<text top="818" left="650" width="7" height="12" font="3">d</text>
<text top="823" left="657" width="9" height="8" font="12">ij</text>
<text top="818" left="670" width="22" height="13" font="3">− µ</text>
<text top="823" left="692" width="4" height="8" font="12">i</text>
<text top="836" left="667" width="8" height="12" font="3">σ</text>
<text top="841" left="675" width="4" height="8" font="12">i</text>
<text top="827" left="699" width="3" height="12" font="3">,</text>
<text top="827" left="829" width="16" height="12" font="3">(6)</text>
<text top="858" left="468" width="56" height="13" font="4">where M</text>
<text top="863" left="524" width="4" height="9" font="0">i</text>
<text top="858" left="535" width="310" height="13" font="4">is the number of test sequences viewed by subject</text>
<text top="875" left="468" width="9" height="14" font="4">i.</text>
<text top="893" left="483" width="362" height="13" font="4">Here, we need to ensure that each subject is valid by</text>
<text top="911" left="468" width="377" height="13" font="4">examining the Z-scores assigned by this subject. In other</text>
<text top="929" left="468" width="377" height="13" font="4">words, the Z-scores from invalid subjects should not be</text>
<text top="947" left="468" width="377" height="13" font="4">included when calculating the O-DMOS for measuring the</text>
<text top="965" left="468" width="377" height="13" font="4">subjective quality of omnidirectional video. We apply the</text>
<text top="983" left="468" width="377" height="13" font="4">subject rejection method <a href="xml/1709.06342v2.html#13">[13] </a>to remove the Z-scores of some</text>
<text top="1001" left="468" width="377" height="13" font="4">subjects if 5% of the Z-scores assigned by these subjects fall</text>
<text top="1019" left="468" width="377" height="13" font="4">outside the range of two standard deviations from the mean</text>
<text top="1037" left="468" width="55" height="13" font="4">Z-scores.</text>
<text top="1055" left="483" width="126" height="13" font="4">Then, the Z-score Z</text>
<text top="1060" left="608" width="9" height="9" font="0">ij</text>
<text top="1055" left="625" width="220" height="13" font="4">needs to be linearly rescaled to fall</text>
<text top="1073" left="468" width="377" height="13" font="4">within the range of [0, 100]. Assume that the Z-scores of</text>
<text top="1091" left="468" width="304" height="13" font="4">a subject follow Gaussian distribution. Then, Z</text>
<text top="1096" left="772" width="9" height="9" font="0">ij</text>
<text top="1091" left="790" width="55" height="13" font="4">of <a href="xml/1709.06342v2.html#6">(6) </a>is</text>
<text top="1109" left="468" width="377" height="13" font="4">distributed as a standard Gaussian, i.e. N (0, 1), in which the</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="1188" width="918">
<text top="40" left="839" width="5" height="9" font="0">7</text>
<text top="87" left="73" width="377" height="13" font="4">mean is 0 and the standard deviation is 1. Thus, 99% of</text>
<text top="104" left="73" width="377" height="13" font="4">the Z-scores lie in the range of [−3, 3]. To make such Z-</text>
<text top="122" left="73" width="254" height="13" font="4">scores ∈ [0, 1], we normalize them by (Z</text>
<text top="128" left="327" width="9" height="9" font="0">ij</text>
<text top="122" left="341" width="109" height="14" font="4">+ 3)/6. Then, the</text>
<text top="141" left="73" width="253" height="13" font="4">normalized Z-scores are rescaled to be Z</text>
<text top="135" left="328" width="3" height="7" font="17">0</text>
<text top="148" left="327" width="9" height="9" font="0">ij</text>
<text top="141" left="342" width="66" height="13" font="4">as follows,</text>
<text top="176" left="202" width="9" height="12" font="3">Z</text>
<text top="169" left="213" width="3" height="7" font="17">0</text>
<text top="181" left="212" width="9" height="8" font="12">ij</text>
<text top="176" left="225" width="11" height="12" font="3">=</text>
<text top="167" left="242" width="36" height="12" font="3">100(Z</text>
<text top="172" left="277" width="9" height="8" font="12">ij</text>
<text top="167" left="290" width="26" height="12" font="3">+ 3)</text>
<text top="185" left="275" width="7" height="12" font="3">6</text>
<text top="176" left="318" width="3" height="12" font="3">,</text>
<text top="176" left="434" width="16" height="12" font="3">(7)</text>
<text top="208" left="73" width="221" height="13" font="4">such that 99% of the values of Z</text>
<text top="203" left="296" width="3" height="7" font="17">0</text>
<text top="215" left="295" width="9" height="9" font="0">ij</text>
<text top="208" left="313" width="137" height="13" font="4">fall into the range of</text>
<text top="226" left="73" width="49" height="14" font="4">[0, 100].</text>
<text top="244" left="88" width="362" height="13" font="4">Finally, the O-DMOS value of sequence j is computed by</text>
<text top="262" left="73" width="74" height="13" font="4">averaging Z</text>
<text top="256" left="148" width="3" height="7" font="17">0</text>
<text top="269" left="147" width="9" height="9" font="0">ij</text>
<text top="262" left="163" width="46" height="13" font="4">from N</text>
<text top="267" left="209" width="5" height="9" font="0">j</text>
<text top="262" left="221" width="87" height="13" font="4">valid subjects:</text>
<text top="305" left="187" width="57" height="12" font="3">O-DMOS</text>
<text top="309" left="244" width="5" height="8" font="12">j</text>
<text top="305" left="254" width="11" height="12" font="3">=</text>
<text top="296" left="275" width="7" height="12" font="3">1</text>
<text top="314" left="270" width="11" height="12" font="3">N</text>
<text top="319" left="281" width="5" height="8" font="12">j</text>
<text top="289" left="294" width="9" height="8" font="12">N</text>
<text top="292" left="303" width="4" height="7" font="17">j</text>
<text top="301" left="291" width="20" height="6" font="3">X</text>
<text top="324" left="292" width="18" height="8" font="12">i=1</text>
<text top="305" left="314" width="9" height="12" font="3">Z</text>
<text top="298" left="324" width="3" height="7" font="17">0</text>
<text top="310" left="323" width="9" height="8" font="12">ij</text>
<text top="305" left="333" width="3" height="12" font="3">.</text>
<text top="305" left="434" width="16" height="12" font="3">(8)</text>
<text top="341" left="88" width="362" height="14" font="4">V-DMOS. In addition to equirectangular projection (ERP),</text>
<text top="359" left="73" width="377" height="13" font="4">there are many other projections that map a sphere onto several</text>
<text top="377" left="73" width="377" height="13" font="4">planes of a polyhedron <a href="xml/1709.06342v2.html#13">[19], </a>such as cube map projection</text>
<text top="395" left="73" width="377" height="13" font="4">(CMP) and octahedron projection. With these projection types,</text>
<text top="413" left="73" width="377" height="13" font="4">a whole omnidirectional video is split into parts, each of which</text>
<text top="431" left="73" width="377" height="13" font="4">is able to be compressed and transmitted separately. Therefore,</text>
<text top="449" left="73" width="377" height="13" font="4">in VQA of omnidirectional video, there is a need to split</text>
<text top="467" left="73" width="377" height="13" font="4">a whole omnidirectional video into several regions and then</text>
<text top="484" left="73" width="377" height="13" font="4">evaluate the quality of different regions. According to Finding</text>
<text top="503" left="73" width="7" height="13" font="4">3</text>
<text top="502" left="81" width="369" height="13" font="4">, there is still a slight inconsistency in the omnidirectional</text>
<text top="520" left="73" width="377" height="13" font="4">video viewing directions. Finding 4 further shows that all the</text>
<text top="538" left="73" width="377" height="13" font="4">regions of omnidirectional video can attract human attention.</text>
<text top="556" left="73" width="377" height="13" font="4">Thus, V-DMOS is used in our subjective VQA method to</text>
<text top="574" left="73" width="377" height="13" font="4">quantify the subjective quality of different regions of omni-</text>
<text top="592" left="73" width="377" height="13" font="4">directional video, by making use of the collected raw quality</text>
<text top="610" left="73" width="209" height="13" font="4">scores and viewing direction data.</text>
<text top="628" left="88" width="362" height="13" font="4">First, we need to compute the ratio of the frequency, with</text>
<text top="646" left="73" width="362" height="13" font="4">which subject i views region r in sequence j, denoted as f</text>
<text top="643" left="437" width="6" height="9" font="0">r</text>
<text top="653" left="436" width="9" height="9" font="0">ij</text>
<text top="646" left="446" width="4" height="13" font="4">.</text>
<text top="664" left="73" width="69" height="13" font="4">Note that f</text>
<text top="661" left="144" width="6" height="9" font="0">r</text>
<text top="670" left="143" width="9" height="9" font="0">ij</text>
<text top="664" left="158" width="204" height="13" font="4">needs to be normalized to satisfy</text>
<text top="690" left="229" width="20" height="6" font="3">X</text>
<text top="713" left="236" width="5" height="8" font="12">r</text>
<text top="694" left="251" width="7" height="12" font="3">f</text>
<text top="691" left="260" width="5" height="8" font="12">r</text>
<text top="700" left="258" width="9" height="8" font="12">ij</text>
<text top="694" left="272" width="25" height="12" font="3">= 1.</text>
<text top="694" left="434" width="16" height="12" font="3">(9)</text>
<text top="731" left="73" width="47" height="13" font="4">When f</text>
<text top="728" left="122" width="6" height="9" font="0">r</text>
<text top="737" left="120" width="9" height="9" font="0">ij</text>
<text top="730" left="135" width="23" height="13" font="4">&gt; f</text>
<text top="736" left="158" width="6" height="9" font="0">0</text>
<text top="731" left="165" width="55" height="13" font="4">, where f</text>
<text top="736" left="219" width="6" height="9" font="0">0</text>
<text top="731" left="230" width="220" height="13" font="4">is a threshold, subject i (after subject</text>
<text top="749" left="73" width="241" height="13" font="4">rejection <a href="xml/1709.06342v2.html#13">[13]) </a>is added to collection I</text>
<text top="754" left="315" width="11" height="9" font="0">jr</text>
<text top="749" left="327" width="123" height="13" font="4">. Assuming that the</text>
<text top="766" left="73" width="52" height="13" font="4">size of I</text>
<text top="772" left="125" width="11" height="9" font="0">jr</text>
<text top="766" left="142" width="27" height="13" font="4">is N</text>
<text top="772" left="169" width="5" height="9" font="0">I</text>
<text top="775" left="174" width="10" height="7" font="17">jr</text>
<text top="766" left="186" width="264" height="13" font="4">, the DMOS value for region r in sequence</text>
<text top="784" left="73" width="129" height="14" font="4">j can be obtained by</text>
<text top="815" left="184" width="42" height="12" font="3">DMOS</text>
<text top="820" left="225" width="10" height="8" font="12">jr</text>
<text top="815" left="240" width="11" height="12" font="3">=</text>
<text top="807" left="267" width="7" height="12" font="3">1</text>
<text top="825" left="257" width="11" height="12" font="3">N</text>
<text top="829" left="268" width="5" height="8" font="12">I</text>
<text top="832" left="272" width="10" height="7" font="17">jr</text>
<text top="811" left="292" width="20" height="6" font="3">X</text>
<text top="834" left="288" width="16" height="8" font="12">i∈I</text>
<text top="837" left="304" width="10" height="7" font="17">jr</text>
<text top="815" left="317" width="9" height="12" font="3">Z</text>
<text top="809" left="328" width="3" height="7" font="17">0</text>
<text top="821" left="327" width="9" height="8" font="12">ij</text>
<text top="816" left="337" width="3" height="12" font="3">.</text>
<text top="816" left="428" width="22" height="12" font="3">(10)</text>
<text top="856" left="73" width="20" height="13" font="4">If I</text>
<text top="861" left="94" width="11" height="9" font="0">jr</text>
<text top="856" left="110" width="106" height="14" font="4">= ∅, then DMOS</text>
<text top="861" left="216" width="11" height="9" font="0">jr</text>
<text top="856" left="232" width="218" height="13" font="4">is an invalid value, denoted by “—”.</text>
<text top="874" left="73" width="331" height="13" font="4">Finally, the vector of V-DMOS can be represented by</text>
<text top="902" left="73" width="4" height="13" font="4">[</text>
<text top="902" left="78" width="57" height="12" font="3">O-DMOS</text>
<text top="906" left="134" width="5" height="8" font="12">j</text>
<text top="902" left="155" width="42" height="12" font="3">DMOS</text>
<text top="906" left="197" width="11" height="8" font="12">j1</text>
<text top="901" left="223" width="16" height="13" font="3">· · ·</text>
<text top="902" left="257" width="42" height="12" font="3">DMOS</text>
<text top="906" left="298" width="10" height="8" font="12">jr</text>
<text top="901" left="325" width="16" height="13" font="3">· · ·</text>
<text top="902" left="358" width="42" height="12" font="3">DMOS</text>
<text top="906" left="400" width="13" height="8" font="12">jR</text>
<text top="902" left="414" width="4" height="13" font="4">]</text>
<text top="903" left="420" width="3" height="12" font="3">,</text>
<text top="902" left="429" width="25" height="13" font="4">(11)</text>
<text top="929" left="73" width="377" height="13" font="4">where R is the total number of regions in omnidirectional</text>
<text top="947" left="73" width="377" height="13" font="4">video. The sphere is split into 6 regions in the same way as</text>
<text top="965" left="73" width="377" height="13" font="4">the CMP <a href="xml/1709.06342v2.html#14">[49], </a>in which the front, left, right, back, top and</text>
<text top="983" left="73" width="377" height="13" font="4">bottom regions are extracted according to the longitude and</text>
<text top="1001" left="73" width="242" height="13" font="4">latitude. For more details, refer to <a href="xml/1709.06342v2.html#14">[49].</a></text>
<text top="1019" left="88" width="362" height="13" font="4">As a result, our V-DMOS is able to measure both overall</text>
<text top="1037" left="73" width="377" height="13" font="4">and regional quality degradation for impaired omnidirectional</text>
<text top="1055" left="73" width="377" height="13" font="4">video. There exist some works on region/viewport oriented</text>
<text top="1073" left="73" width="377" height="13" font="4">coding optimization <a href="xml/1709.06342v2.html#13">[20]–[22] </a>and region/viewport adaptive</text>
<text top="1091" left="73" width="377" height="13" font="4">streaming <a href="xml/1709.06342v2.html#14">[50]–[52]. </a>The optimization or adaptation schemes</text>
<text top="1109" left="73" width="377" height="13" font="4">consider inequality of different regions. With the score vector</text>
<text top="89" left="629" width="54" height="11" font="5">TABLE III</text>
<text top="102" left="560" width="9" height="11" font="5">V</text>
<text top="104" left="568" width="158" height="9" font="7">ALUES OF THE PARAMETERS IN</text>
<text top="102" left="729" width="23" height="11" font="5"><a href="xml/1709.06342v2.html#7">(12)</a>.</text>
<text top="115" left="486" width="17" height="11" font="5">k/k</text>
<text top="112" left="503" width="3" height="8" font="12">0</text>
<text top="115" left="535" width="7" height="11" font="5">a</text>
<text top="119" left="542" width="6" height="8" font="12">k</text>
<text top="115" left="588" width="5" height="11" font="5">b</text>
<text top="119" left="594" width="6" height="8" font="12">k</text>
<text top="115" left="641" width="6" height="11" font="5">c</text>
<text top="119" left="647" width="6" height="8" font="12">k</text>
<text top="115" left="689" width="7" height="11" font="5">a</text>
<text top="112" left="696" width="3" height="8" font="12">0</text>
<text top="121" left="696" width="6" height="8" font="12">k</text>
<text top="119" left="702" width="3" height="7" font="17">0</text>
<text top="115" left="743" width="5" height="11" font="5">b</text>
<text top="112" left="748" width="3" height="8" font="12">0</text>
<text top="121" left="748" width="6" height="8" font="12">k</text>
<text top="119" left="754" width="3" height="7" font="17">0</text>
<text top="115" left="799" width="6" height="11" font="5">c</text>
<text top="112" left="804" width="3" height="8" font="12">0</text>
<text top="121" left="804" width="6" height="8" font="12">k</text>
<text top="119" left="810" width="3" height="7" font="17">0</text>
<text top="130" left="494" width="6" height="11" font="5">1</text>
<text top="130" left="525" width="33" height="11" font="5">0.0034</text>
<text top="130" left="576" width="37" height="11" font="5">-0.1549</text>
<text top="130" left="631" width="33" height="11" font="5">4.6740</text>
<text top="130" left="682" width="33" height="11" font="5">0.0075</text>
<text top="130" left="733" width="37" height="11" font="5">-2.3738</text>
<text top="130" left="790" width="33" height="11" font="5">6.6437</text>
<text top="144" left="494" width="6" height="11" font="5">2</text>
<text top="144" left="525" width="33" height="11" font="5">0.0106</text>
<text top="144" left="578" width="33" height="11" font="5">1.5140</text>
<text top="144" left="634" width="27" height="11" font="5">18.51</text>
<text top="144" left="682" width="33" height="11" font="5">0.0209</text>
<text top="144" left="735" width="33" height="11" font="5">1.8260</text>
<text top="144" left="787" width="39" height="11" font="5">14.8171</text>
<text top="158" left="494" width="6" height="11" font="5">3</text>
<text top="158" left="525" width="33" height="11" font="5">0.0032</text>
<text top="158" left="578" width="33" height="11" font="5">6.3670</text>
<text top="158" left="634" width="27" height="11" font="5">110.5</text>
<text top="158" left="682" width="33" height="11" font="5">0.0057</text>
<text top="158" left="735" width="33" height="11" font="5">1.4618</text>
<text top="158" left="787" width="39" height="11" font="5">36.1311</text>
<text top="186" left="468" width="377" height="13" font="4">of V-DMOS, we can directly evaluate the performance of</text>
<text top="204" left="468" width="377" height="13" font="4">the schemes designed for specific regions of omnidirectional</text>
<text top="222" left="468" width="37" height="13" font="4">video.</text>
<text top="262" left="557" width="32" height="13" font="4">V. O</text>
<text top="264" left="590" width="60" height="11" font="5">BJECTIVE</text>
<text top="262" left="655" width="34" height="13" font="4">VQA</text>
<text top="264" left="693" width="62" height="11" font="5">METHODS</text>
<text top="286" left="483" width="362" height="13" font="4">In this section, we propose two objective VQA methods for</text>
<text top="304" left="468" width="377" height="13" font="4">omnidirectional video coding, which are on the basis of the</text>
<text top="322" left="468" width="377" height="13" font="4">traditional PSNR mechanism and our findings in Section <a href="xml/1709.06342v2.html#4">III-B.</a></text>
<text top="340" left="468" width="377" height="13" font="4">Both of these methods impose weights on the pixel-wise</text>
<text top="358" left="468" width="377" height="13" font="4">distortion in calculating the PSNR, according to the possibility</text>
<text top="375" left="468" width="377" height="13" font="4">of attracting human attention. Thus, these methods are called</text>
<text top="393" left="468" width="377" height="13" font="4">perceptual VQA (P-VQA) methods. The first method mainly</text>
<text top="411" left="468" width="377" height="13" font="4">focuses on weighting the distortion of pixels according to their</text>
<text top="429" left="468" width="377" height="13" font="4">locations in omnidirectional video rather than their contents.</text>
<text top="447" left="468" width="377" height="13" font="4">Thus, this method is called the non-content-based P-VQA</text>
<text top="465" left="468" width="377" height="13" font="4">(NCP-VQA) method, to be discussed in Section <a href="xml/1709.06342v2.html#7">V-A. </a>The</text>
<text top="483" left="468" width="377" height="13" font="4">second method allocates weights to pixel-wise distortion based</text>
<text top="501" left="468" width="377" height="13" font="4">on the viewing directions predicted with respect to the content</text>
<text top="519" left="468" width="377" height="13" font="4">of omnidirectional video and is thus called the content-based</text>
<text top="537" left="468" width="377" height="13" font="4">P-VQA (CP-VQA) method. This is to be introduced in Section</text>
<text top="555" left="468" width="28" height="13" font="4"><a href="xml/1709.06342v2.html#8">V-B.</a></text>
<text top="598" left="468" width="286" height="13" font="4">A. Non-content-based perceptual VQA method</text>
<text top="622" left="483" width="362" height="13" font="4">According to Finding 2, front regions near the equator</text>
<text top="640" left="468" width="377" height="13" font="4">are viewed more frequently than other regions in omnidirec-</text>
<text top="658" left="468" width="377" height="13" font="4">tional video. Thus, it is necessary to consider such viewing</text>
<text top="676" left="468" width="377" height="13" font="4">direction frequency when calculating the non-content-based</text>
<text top="694" left="468" width="377" height="13" font="4">perceptual PSNR (NCP-PSNR) for our NCP-VQA method</text>
<text top="711" left="468" width="303" height="13" font="4">for omnidirectional video coding. Let ϕ ∈ [−180</text>
<text top="708" left="771" width="6" height="10" font="0">◦</text>
<text top="711" left="778" width="29" height="13" font="4">, 180</text>
<text top="708" left="807" width="6" height="10" font="0">◦</text>
<text top="711" left="814" width="31" height="14" font="4">] and</text>
<text top="729" left="468" width="56" height="13" font="4">θ ∈ [−90</text>
<text top="726" left="524" width="6" height="10" font="0">◦</text>
<text top="729" left="531" width="22" height="13" font="4">, 90</text>
<text top="726" left="553" width="6" height="10" font="0">◦</text>
<text top="729" left="560" width="285" height="14" font="4">] denote the longitude and latitude of a viewing</text>
<text top="747" left="468" width="377" height="13" font="4">direction in degrees, respectively. Since Finding 1 points out</text>
<text top="765" left="468" width="377" height="13" font="4">that the longitude and latitude of the viewing directions are</text>
<text top="783" left="468" width="377" height="13" font="4">almost independent of each other, the distribution of viewing</text>
<text top="801" left="468" width="377" height="13" font="4">direction frequency u(ϕ, θ) can be modeled using the follow-</text>
<text top="819" left="468" width="230" height="13" font="4">ing Gaussian mixture model (GMM):</text>
<text top="845" left="480" width="55" height="12" font="3">u(ϕ, θ) =</text>
<text top="864" left="480" width="11" height="6" font="3">(</text>
<text top="864" left="496" width="5" height="8" font="12">3</text>
<text top="874" left="489" width="20" height="6" font="3">X</text>
<text top="897" left="489" width="20" height="8" font="12">k=1</text>
<text top="878" left="509" width="7" height="12" font="3">a</text>
<text top="882" left="516" width="6" height="8" font="12">k</text>
<text top="878" left="525" width="21" height="12" font="3">exp</text>
<text top="864" left="546" width="8" height="6" font="3">&#34;</text>
<text top="877" left="552" width="11" height="13" font="3">−</text>
<text top="868" left="563" width="37" height="13" font="3">ϕ−b</text>
<text top="874" left="600" width="6" height="8" font="12">k</text>
<text top="887" left="583" width="6" height="12" font="3">c</text>
<text top="892" left="589" width="6" height="8" font="12">k</text>
<text top="868" left="606" width="10" height="6" font="3"></text>
<text top="865" left="616" width="5" height="8" font="12">2</text>
<text top="864" left="620" width="17" height="6" font="3">#)</text>
<text top="910" left="480" width="6" height="6" font="3">|</text>
<text top="910" left="553" width="12" height="6" font="3">{z</text>
<text top="910" left="631" width="6" height="6" font="3">}</text>
<text top="917" left="523" width="71" height="8" font="12">GMM for longitude</text>
<text top="864" left="637" width="11" height="6" font="3">(</text>
<text top="864" left="655" width="5" height="8" font="12">3</text>
<text top="874" left="648" width="20" height="6" font="3">X</text>
<text top="897" left="646" width="6" height="8" font="12">k</text>
<text top="895" left="652" width="3" height="7" font="17">0</text>
<text top="897" left="656" width="14" height="8" font="12">=1</text>
<text top="878" left="670" width="7" height="12" font="3">a</text>
<text top="874" left="677" width="3" height="8" font="12">0</text>
<text top="883" left="677" width="6" height="8" font="12">k</text>
<text top="881" left="684" width="3" height="7" font="17">0</text>
<text top="878" left="691" width="21" height="12" font="3">exp</text>
<text top="864" left="712" width="8" height="6" font="3">&#34;</text>
<text top="877" left="718" width="11" height="13" font="3">−</text>
<text top="868" left="728" width="35" height="13" font="3">θ−b</text>
<text top="866" left="763" width="3" height="8" font="12">0</text>
<text top="875" left="763" width="6" height="8" font="12">k</text>
<text top="873" left="769" width="3" height="7" font="17">0</text>
<text top="887" left="747" width="6" height="12" font="3">c</text>
<text top="885" left="753" width="3" height="8" font="12">0</text>
<text top="894" left="753" width="6" height="8" font="12">k</text>
<text top="892" left="760" width="3" height="7" font="17">0</text>
<text top="868" left="773" width="10" height="6" font="3"></text>
<text top="865" left="784" width="5" height="8" font="12">2</text>
<text top="864" left="787" width="17" height="6" font="3">#)</text>
<text top="911" left="637" width="6" height="6" font="3">|</text>
<text top="911" left="715" width="12" height="6" font="3">{z</text>
<text top="911" left="798" width="6" height="6" font="3">}</text>
<text top="918" left="689" width="64" height="8" font="12">GMM for latitude</text>
<text top="878" left="807" width="3" height="12" font="3">.</text>
<text top="879" left="822" width="22" height="12" font="3">(12)</text>
<text top="938" left="474" width="61" height="13" font="4">In <a href="xml/1709.06342v2.html#7">(12), </a>a</text>
<text top="943" left="535" width="6" height="9" font="0">k</text>
<text top="938" left="543" width="16" height="13" font="4">, b</text>
<text top="943" left="559" width="6" height="9" font="0">k</text>
<text top="938" left="572" width="34" height="13" font="4">and c</text>
<text top="943" left="607" width="6" height="9" font="0">k</text>
<text top="938" left="620" width="224" height="13" font="4">are parameters of the GMM for the</text>
<text top="956" left="468" width="309" height="13" font="4">viewing direction distribution of longitude, and a</text>
<text top="953" left="777" width="3" height="10" font="0">0</text>
<text top="963" left="777" width="6" height="9" font="0">k</text>
<text top="962" left="784" width="3" height="7" font="17">0</text>
<text top="956" left="789" width="16" height="13" font="4">, b</text>
<text top="953" left="805" width="3" height="10" font="0">0</text>
<text top="963" left="805" width="6" height="9" font="0">k</text>
<text top="962" left="812" width="3" height="7" font="17">0</text>
<text top="956" left="823" width="22" height="13" font="4">and</text>
<text top="974" left="468" width="6" height="13" font="4">c</text>
<text top="971" left="474" width="3" height="10" font="0">0</text>
<text top="981" left="474" width="6" height="9" font="0">k</text>
<text top="980" left="481" width="3" height="7" font="17">0</text>
<text top="974" left="490" width="355" height="13" font="4">are GMM parameters for the viewing direction distribution</text>
<text top="992" left="468" width="377" height="13" font="4">of latitude. The values of these parameters can be obtained via</text>
<text top="1010" left="468" width="377" height="13" font="4">least squares fitting over all viewing directions of our database</text>
<text top="1028" left="468" width="367" height="13" font="4">proposed in Section <a href="xml/1709.06342v2.html#3">III-A, </a>and they are reported in Table <a href="xml/1709.06342v2.html#7">III</a></text>
<text top="1026" left="835" width="5" height="9" font="0"><a href="xml/1709.06342v2.html#7">1</a></text>
<text top="1028" left="841" width="4" height="13" font="4"><a href="xml/1709.06342v2.html#7">.</a></text>
<text top="1046" left="468" width="377" height="13" font="4">In fact, this least squares fitting has already been discussed in</text>
<text top="1064" left="468" width="377" height="13" font="4">the analysis of Finding 2, with GMM curves been plotted in</text>
<text top="1095" left="480" width="4" height="8" font="12">1</text>
<text top="1097" left="485" width="359" height="11" font="5">The GMM model with these parameters is verified highly correlated with</text>
<text top="1111" left="468" width="351" height="11" font="5">the ground truth viewing direction data of the test set of in Section <a href="xml/1709.06342v2.html#9">VI.</a></text>
</page>
<page number="8" position="absolute" top="0" left="0" height="1188" width="918">
	<fontspec id="18" size="7" family="Times" color="#000000"/>
	<fontspec id="19" size="7" family="Times" color="#6fac46"/>
	<fontspec id="20" size="7" family="Times" color="#ffc000"/>
	<fontspec id="21" size="7" family="Times" color="#5b9bd4"/>
	<fontspec id="22" size="10" family="Times" color="#7e7e7e"/>
<text top="40" left="839" width="5" height="9" font="0">8</text>
<text top="112" left="612" width="66" height="13" font="18"><b>Random Forest </b></text>
<text top="123" left="612" width="65" height="13" font="18"><b>Model Training</b></text>
<text top="190" left="313" width="81" height="13" font="18"><b>Candidate Viewing </b></text>
<text top="202" left="310" width="85" height="13" font="18"><b>Direction Extraction</b></text>
<text top="190" left="527" width="35" height="13" font="18"><b>Feature </b></text>
<text top="202" left="523" width="39" height="13" font="18"><b>Detection</b></text>
<text top="204" left="438" width="45" height="13" font="18"><b>Candidate </b></text>
<text top="216" left="422" width="75" height="13" font="18"><b>Viewing Direction</b></text>
<text top="189" left="582" width="36" height="13" font="18"><b>Features</b></text>
<text top="262" left="307" width="94" height="13" font="18"><b>Viewport Binary Map </b></text>
<text top="274" left="329" width="48" height="13" font="18"><b>Calculation</b></text>
<text top="215" left="610" width="75" height="13" font="18"><b>Viewing Direction</b></text>
<text top="227" left="617" width="62" height="13" font="18"><b> of Next Frame</b></text>
<text top="277" left="419" width="91" height="13" font="18"><b>Viewport Binary Map</b></text>
<text top="298" left="185" width="82" height="13" font="18"><b>Non-Content-Based</b></text>
<text top="309" left="198" width="54" height="13" font="18"><b> Weight Map</b></text>
<text top="277" left="553" width="60" height="13" font="18"><b>Content-based</b></text>
<text top="289" left="555" width="54" height="13" font="18"><b> Weight Map</b></text>
<text top="183" left="204" width="54" height="13" font="18"><b>Video Frame</b></text>
<text top="196" left="634" width="22" height="13" font="18"><b>MAP</b></text>
<text top="115" left="313" width="81" height="13" font="18"><b>Candidate Viewing </b></text>
<text top="127" left="310" width="85" height="13" font="18"><b>Direction Extraction</b></text>
<text top="101" left="424" width="43" height="13" font="18"><b>Candidate</b></text>
<text top="113" left="408" width="75" height="13" font="18"><b>Viewing Direction</b></text>
<text top="135" left="194" width="75" height="13" font="18"><b>Viewing Direction</b></text>
<text top="147" left="221" width="20" height="13" font="18"><b>Data</b></text>
<text top="101" left="208" width="58" height="13" font="18"><b>Video Frames</b></text>
<text top="91" left="525" width="35" height="13" font="18"><b>Feature </b></text>
<text top="102" left="522" width="39" height="13" font="18"><b>Detection</b></text>
<text top="135" left="530" width="24" height="13" font="18"><b>Label</b></text>
<text top="146" left="518" width="47" height="13" font="18"><b>Annotation</b></text>
<text top="90" left="573" width="36" height="13" font="18"><b>Features</b></text>
<text top="147" left="577" width="27" height="13" font="18"><b>Labels</b></text>
<text top="142" left="647" width="37" height="13" font="18"><b>Training</b></text>
<text top="153" left="650" width="29" height="13" font="18"><b> Model</b></text>
<text top="302" left="275" width="185" height="13" font="19"><b>CP-PSNR Calculation of the Current Frame</b></text>
<text top="86" left="275" width="37" height="13" font="20"><b>Training</b></text>
<text top="262" left="624" width="44" height="13" font="18"><b>CP-PSNR </b></text>
<text top="274" left="621" width="48" height="13" font="18"><b>Calculation</b></text>
<text top="280" left="710" width="41" height="13" font="18"><b>CP-PSNR</b></text>
<text top="292" left="697" width="66" height="13" font="18"><b> of Input Frame</b></text>
<text top="216" left="296" width="75" height="13" font="18"><b>Viewing Direction</b></text>
<text top="228" left="296" width="76" height="13" font="18"><b> of Current Frame</b></text>
<text top="172" left="274" width="196" height="13" font="21"><b>Viewing Direction Prediction of the Next Frame</b></text>
<text top="85" left="653" width="36" height="17" font="22"><b>Stage I</b></text>
<text top="173" left="651" width="41" height="17" font="22"><b>Stage II</b></text>
<text top="331" left="322" width="273" height="11" font="5">(a) Block diagram of the content-based PVQA method.</text>
<text top="436" left="216" width="80" height="14" font="14"><b>Viewing Direction</b></text>
<text top="429" left="361" width="71" height="14" font="14"><b>Viewport Image</b></text>
<text top="429" left="525" width="60" height="14" font="14"><b>Saliency Map</b></text>
<text top="430" left="698" width="46" height="14" font="14"><b>Candidate</b></text>
<text top="443" left="681" width="80" height="14" font="14"><b>Viewing Direction</b></text>
<text top="399" left="234" width="58" height="14" font="14"><b>Video Frame</b></text>
<text top="417" left="303" width="46" height="14" font="14"><b>Projection</b></text>
<text top="411" left="443" width="69" height="14" font="14"><b>PQFT Saliency </b></text>
<text top="423" left="455" width="42" height="14" font="14"><b>Detection</b></text>
<text top="411" left="605" width="63" height="14" font="14"><b>Mean Shift &amp; </b></text>
<text top="423" left="621" width="28" height="14" font="14"><b>GMM</b></text>
<text top="471" left="334" width="250" height="11" font="5">(b) Detail of candidate viewing direction extractor.</text>
<text top="485" left="73" width="342" height="11" font="5">Fig. 6. The procedure of the proposed content-based PVQA method.</text>
<text top="519" left="73" width="377" height="13" font="4">Figure 2. Note that the numbers of Gaussian components for</text>
<text top="537" left="73" width="377" height="13" font="4">both GMM fitting of <a href="xml/1709.06342v2.html#7">(12) </a>are set to 3, for making the fitting</text>
<text top="555" left="73" width="346" height="13" font="4">error convergent (with R-square values more than 0.99).</text>
<text top="575" left="88" width="362" height="13" font="4">Next, we take into account the equirectangular projection for</text>
<text top="593" left="73" width="377" height="13" font="4">our NCP-PSNR metric. Given an omnidirectional video under</text>
<text top="611" left="73" width="377" height="13" font="4">an equirectangular projection with a resolution of W × H, the</text>
<text top="629" left="73" width="377" height="13" font="4">probability of each pixel being in the viewing direction during</text>
<text top="647" left="73" width="218" height="13" font="4">one frame can be obtained by <a href="xml/1709.06342v2.html#14">[46]:</a></text>
<text top="678" left="129" width="50" height="12" font="3">v(s, t) =</text>
<text top="706" left="129" width="8" height="12" font="3">u</text>
<text top="696" left="137" width="10" height="6" font="3"></text>
<text top="706" left="145" width="31" height="13" font="3">−360</text>
<text top="696" left="179" width="10" height="6" font="3"></text>
<text top="698" left="195" width="30" height="12" font="3">s − 1</text>
<text top="715" left="191" width="39" height="12" font="3">W − 1</text>
<text top="706" left="232" width="11" height="13" font="3">−</text>
<text top="698" left="246" width="7" height="12" font="3">1</text>
<text top="715" left="246" width="7" height="12" font="3">2</text>
<text top="696" left="254" width="10" height="6" font="3"></text>
<text top="706" left="265" width="35" height="12" font="3">,−180</text>
<text top="696" left="302" width="45" height="13" font="3"> t − 1</text>
<text top="715" left="314" width="36" height="12" font="3">H − 1</text>
<text top="706" left="353" width="11" height="13" font="3">−</text>
<text top="698" left="366" width="7" height="12" font="3">1</text>
<text top="715" left="366" width="7" height="12" font="3">2</text>
<text top="696" left="375" width="10" height="6" font="3"></text>
<text top="696" left="381" width="10" height="6" font="3"></text>
<text top="707" left="391" width="3" height="12" font="3">,</text>
<text top="697" left="428" width="22" height="12" font="3">(13)</text>
<text top="746" left="73" width="377" height="13" font="4">where (s, t) is the pixel coordinate, with 1 ≤ s ≤ W and</text>
<text top="764" left="73" width="377" height="14" font="4">1 ≤ t ≤ H. Note that our VQA method can be easily extended</text>
<text top="782" left="73" width="369" height="13" font="4">to other projections by replacing the projection formulation.</text>
<text top="802" left="88" width="362" height="13" font="4">In fact, pixels within a viewport centered in one viewing</text>
<text top="820" left="73" width="377" height="13" font="4">direction are all accessible to subjects. Thus, the pixels within</text>
<text top="838" left="73" width="377" height="13" font="4">a viewport should be of equal importance in evaluating the</text>
<text top="856" left="73" width="377" height="13" font="4">quality of encoded omnidirectional video. To model possible</text>
<text top="873" left="73" width="377" height="13" font="4">viewports, we need to generate the non-content-based weight</text>
<text top="891" left="73" width="377" height="13" font="4">map in our NCP-VQA method, based on the probability of</text>
<text top="909" left="73" width="325" height="13" font="4">viewing direction, i.e., v(s, t) in <a href="xml/1709.06342v2.html#8">(13). </a>Assume that P</text>
<text top="914" left="398" width="14" height="9" font="0">s,t</text>
<text top="909" left="417" width="33" height="13" font="4">is the</text>
<text top="927" left="73" width="377" height="13" font="4">viewport, the center of which is the viewing direction (s, t).</text>
<text top="945" left="73" width="377" height="13" font="4">According to studies on near peripheral vision <a href="xml/1709.06342v2.html#14">[53], </a>the ranges</text>
<text top="963" left="73" width="31" height="13" font="4">of P</text>
<text top="968" left="105" width="14" height="9" font="0">s,t</text>
<text top="963" left="127" width="99" height="13" font="4">are set to [−30</text>
<text top="960" left="226" width="6" height="10" font="0">◦</text>
<text top="963" left="232" width="22" height="13" font="4">, 30</text>
<text top="960" left="254" width="6" height="10" font="0">◦</text>
<text top="963" left="261" width="189" height="14" font="4">] in both directions. For each</text>
<text top="981" left="73" width="377" height="13" font="4">pixel (s, t) in an omnidirectional video frame, we can find all</text>
<text top="999" left="73" width="377" height="13" font="4">viewports including this pixel, and the viewing directions of</text>
<text top="1017" left="73" width="319" height="13" font="4">these viewports constitute a collection denoted by V</text>
<text top="1022" left="393" width="14" height="9" font="0">s,t</text>
<text top="1017" left="407" width="43" height="13" font="4">. Then,</text>
<text top="1035" left="73" width="333" height="13" font="4">the non-content-based weight map can be obtained by</text>
<text top="1067" left="179" width="53" height="12" font="3">w(s, t) =</text>
<text top="1067" left="253" width="26" height="12" font="3">max</text>
<text top="1080" left="236" width="10" height="8" font="12">(s</text>
<text top="1078" left="245" width="3" height="7" font="17">0</text>
<text top="1080" left="249" width="8" height="8" font="12">,t</text>
<text top="1078" left="257" width="3" height="7" font="17">0</text>
<text top="1080" left="261" width="21" height="8" font="12">)∈V</text>
<text top="1082" left="282" width="13" height="7" font="17">s,t</text>
<text top="1067" left="298" width="19" height="12" font="3">v(s</text>
<text top="1064" left="317" width="3" height="8" font="12">0</text>
<text top="1067" left="321" width="11" height="12" font="3">, t</text>
<text top="1064" left="332" width="3" height="8" font="12">0</text>
<text top="1067" left="336" width="9" height="12" font="3">).</text>
<text top="1068" left="428" width="22" height="12" font="3">(14)</text>
<text top="1109" left="88" width="362" height="13" font="4">Afterwards, the non-content-based weight map needs to be</text>
<text top="519" left="468" width="87" height="13" font="4">normalized by</text>
<text top="542" left="592" width="7" height="12" font="3">˜</text>
<text top="542" left="589" width="53" height="12" font="3">w(s, t) =</text>
<text top="534" left="664" width="39" height="12" font="3">w(s, t)</text>
<text top="550" left="648" width="15" height="6" font="3">P</text>
<text top="559" left="663" width="13" height="8" font="12">s,t</text>
<text top="551" left="679" width="39" height="12" font="3">w(s, t)</text>
<text top="542" left="719" width="4" height="12" font="3">,</text>
<text top="543" left="822" width="22" height="12" font="3">(15)</text>
<text top="576" left="468" width="56" height="13" font="4">to satisfy</text>
<text top="590" left="613" width="20" height="6" font="3">X</text>
<text top="613" left="616" width="13" height="8" font="12">s,t</text>
<text top="594" left="638" width="7" height="12" font="3">˜</text>
<text top="594" left="635" width="67" height="12" font="3">w(s, t) = 1.</text>
<text top="595" left="822" width="22" height="12" font="3">(16)</text>
<text top="632" left="468" width="377" height="13" font="4">Note that the non-content-based weight map can be trained</text>
<text top="650" left="468" width="377" height="13" font="4">offline. Then, the sequences with the same resolution can use</text>
<text top="668" left="468" width="377" height="13" font="4">the same offline weight map, without any extra complexity.</text>
<text top="686" left="468" width="377" height="13" font="4">Finally, based on the definition of the PSNR, the NCP-PSNR</text>
<text top="704" left="468" width="340" height="13" font="4">for each omnidirectional video frame can be <a href="xml/1709.06342v2.html#8">calculated</a></text>
<text top="702" left="808" width="5" height="9" font="0"><a href="xml/1709.06342v2.html#8">2</a></text>
<text top="704" left="819" width="12" height="13" font="4">as</text>
<text top="735" left="493" width="116" height="12" font="3">NCP-PSNR = 10 log</text>
<text top="726" left="689" width="6" height="12" font="3">I</text>
<text top="724" left="696" width="5" height="8" font="12">2</text>
<text top="732" left="695" width="20" height="8" font="12">max</text>
<text top="743" left="614" width="15" height="6" font="3">P</text>
<text top="751" left="628" width="13" height="8" font="12">s,t</text>
<text top="744" left="644" width="59" height="12" font="3">(I(s, t)−I</text>
<text top="743" left="704" width="3" height="8" font="12">0</text>
<text top="744" left="708" width="34" height="12" font="3">(s, t))</text>
<text top="743" left="742" width="5" height="8" font="12">2</text>
<text top="743" left="749" width="14" height="13" font="3">· ˜</text>
<text top="744" left="754" width="39" height="12" font="3">w(s, t)</text>
<text top="735" left="794" width="3" height="12" font="3">,</text>
<text top="735" left="822" width="22" height="12" font="3">(17)</text>
<text top="770" left="468" width="120" height="13" font="4">where I(s, t) and I</text>
<text top="767" left="589" width="3" height="10" font="0">0</text>
<text top="770" left="593" width="251" height="14" font="4">(s, t) are intensities of pixel (s, t) in the</text>
<text top="788" left="468" width="377" height="13" font="4">reference and processed omnidirectional videos, respectively.</text>
<text top="806" left="468" width="93" height="13" font="4">Additionally, I</text>
<text top="811" left="561" width="22" height="9" font="0">max</text>
<text top="806" left="592" width="252" height="13" font="4">is the maximum intensity value of the</text>
<text top="824" left="468" width="207" height="13" font="4">videos (= 255 for 8-bit intensity).</text>
<text top="864" left="468" width="200" height="13" font="4">B. Content-based PVQA method</text>
<text top="887" left="483" width="59" height="13" font="4">Finding 4</text>
<text top="886" left="547" width="298" height="13" font="4">shows that the viewing directions of the subjects</text>
<text top="904" left="468" width="377" height="13" font="4">are also correlated with the contents of the omnidirectional</text>
<text top="922" left="468" width="377" height="13" font="4">video. According to this finding, we further develop a CP-</text>
<text top="940" left="468" width="377" height="13" font="4">VQA method for omnidirectional video coding, in which the</text>
<text top="958" left="468" width="350" height="13" font="4">content-based perceptual PSNR (CP-PSNR) is measured.</text>
<text top="976" left="483" width="362" height="14" font="4">Outline. Figure <a href="xml/1709.06342v2.html#8">6(a) </a>summarizes the procedure of our CP-</text>
<text top="994" left="468" width="377" height="13" font="4">VQA method. As shown in this figure, our CP-VQA method</text>
<text top="1012" left="468" width="377" height="13" font="4">consists of two stages, i.e., Stage I: model training, and</text>
<text top="1030" left="468" width="377" height="13" font="4">Stage II: CP-PSNR calculation. For Stage I of model training,</text>
<text top="1048" left="468" width="377" height="13" font="4">the input includes omnidirectional video frames and their</text>
<text top="1066" left="468" width="377" height="13" font="4">corresponding viewing directions for all subjects. Then, a</text>
<text top="1095" left="480" width="4" height="8" font="12">2</text>
<text top="1097" left="485" width="359" height="11" font="5">Because of <a href="xml/1709.06342v2.html#8">(15) </a>and <a href="xml/1709.06342v2.html#8">(16), </a>we do not need to divide NCP-PSNR in <a href="xml/1709.06342v2.html#8">(17)</a></text>
<text top="1111" left="468" width="121" height="11" font="5">by the number of pixels.</text>
</page>
<page number="9" position="absolute" top="0" left="0" height="1188" width="918">
<text top="40" left="839" width="5" height="9" font="0">9</text>
<text top="87" left="73" width="377" height="13" font="4">random forest model of classification is trained to predict</text>
<text top="104" left="73" width="377" height="13" font="4">viewing directions. For Stage II of CP-PSNR calculation,</text>
<text top="122" left="73" width="377" height="13" font="4">each omnidirectional video frame is taken as the input. In</text>
<text top="140" left="73" width="377" height="13" font="4">this stage, there are two branches to accomplish two different</text>
<text top="158" left="73" width="377" height="13" font="4">tasks. (1) Viewing direction prediction of the next frame. After</text>
<text top="176" left="73" width="377" height="13" font="4">extracting several candidates from the input frame, a viewing</text>
<text top="194" left="73" width="377" height="13" font="4">direction can be predicted using maximum a posteriori (MAP)</text>
<text top="212" left="73" width="377" height="13" font="4">estimation for the incoming frames, with regard to detected</text>
<text top="230" left="73" width="377" height="13" font="4">features and the trained model. (2) CP-PSNR calculation of</text>
<text top="248" left="73" width="377" height="13" font="4">the current frame. Given the predicted viewing direction, the</text>
<text top="266" left="73" width="377" height="13" font="4">viewport binary map is calculated. Then, the content-based</text>
<text top="284" left="73" width="377" height="13" font="4">weight map is generated by multiplying the viewport binary</text>
<text top="302" left="73" width="377" height="13" font="4">map with the non-content-based weight map. Finally, the CP-</text>
<text top="320" left="73" width="377" height="13" font="4">PSNR is obtained by imposing the content-based weight map</text>
<text top="338" left="73" width="377" height="13" font="4">in the PSNR. In the following, we present the details of our</text>
<text top="356" left="73" width="109" height="13" font="4">CP-VQA method.</text>
<text top="373" left="88" width="362" height="14" font="4">Extraction of candidate viewing directions. In our CP-</text>
<text top="391" left="73" width="377" height="13" font="4">VQA method, the first step is to extract viewing direction</text>
<text top="409" left="73" width="377" height="13" font="4">candidates for the next omnidirectional video frame. Given</text>
<text top="427" left="73" width="377" height="13" font="4">the input omnidirectional video frame and its current viewing</text>
<text top="445" left="73" width="377" height="13" font="4">direction, the procedure for extracting the viewing direction</text>
<text top="463" left="73" width="377" height="13" font="4">candidates is shown in Figure <a href="xml/1709.06342v2.html#8">6(b). </a>This procedure also</text>
<text top="481" left="73" width="377" height="13" font="4">corresponds to the orange blocks “candidate viewing direction</text>
<text top="499" left="73" width="377" height="13" font="4">extraction” in Figure <a href="xml/1709.06342v2.html#8">6(a). </a>To extract the viewing direction</text>
<text top="517" left="73" width="377" height="13" font="4">candidates, a viewport projection <a href="xml/1709.06342v2.html#13">[16] </a>is applied to obtain the</text>
<text top="535" left="73" width="377" height="13" font="4">viewport image of the input video frame given the current</text>
<text top="553" left="73" width="377" height="13" font="4">viewing direction. Note that the viewport image is regarded</text>
<text top="571" left="73" width="377" height="13" font="4">as the image that subjects can actually see through the HMD</text>
<text top="589" left="73" width="377" height="13" font="4">with little affine geometric distortion. Therefore, the feature</text>
<text top="607" left="73" width="377" height="13" font="4">extraction and saliency prediction methods for 2D image are</text>
<text top="625" left="73" width="377" height="13" font="4">applicable to the viewport image. Subsequently, a saliency map</text>
<text top="642" left="73" width="377" height="13" font="4">of the viewport image is generated using a simple yet effective</text>
<text top="660" left="73" width="377" height="13" font="4">saliency detection algorithm, phase spectrum of quaternion</text>
<text top="678" left="73" width="377" height="13" font="4">Fourier transform (PQFT) <a href="xml/1709.06342v2.html#14">[54]. </a>Considering the saliency map</text>
<text top="696" left="73" width="377" height="13" font="4">as a 2-dimensional probability distribution, ten thousand of</text>
<text top="714" left="73" width="377" height="13" font="4">random points subject to this distribution can be generated.</text>
<text top="732" left="73" width="377" height="13" font="4">Then, the mean-shift <a href="xml/1709.06342v2.html#14">[55] </a>and GMM are applied on these</text>
<text top="750" left="73" width="377" height="13" font="4">random points for clustering. The center of each cluster is</text>
<text top="768" left="73" width="377" height="13" font="4">used as a candidate of the viewing direction for the incoming</text>
<text top="786" left="73" width="377" height="13" font="4">frame (the green dots in Figure <a href="xml/1709.06342v2.html#8">6(b)). </a>The standard deviation</text>
<text top="804" left="73" width="377" height="13" font="4">parameter of each GMM represents the area of the salient</text>
<text top="822" left="73" width="377" height="13" font="4">region around the candidate (the green circles around the dots</text>
<text top="840" left="73" width="95" height="13" font="4">in Figure <a href="xml/1709.06342v2.html#8">6(b)).</a></text>
<text top="857" left="88" width="362" height="14" font="4">Stage I: Model training. Regarding the model training,</text>
<text top="876" left="73" width="377" height="13" font="4">the inputs are video frames and their corresponding view-</text>
<text top="893" left="73" width="377" height="13" font="4">ing directions for each subject, which are derived from our</text>
<text top="911" left="73" width="377" height="13" font="4">viewing direction database for omnidirectional video. Then,</text>
<text top="929" left="73" width="377" height="13" font="4">several candidates of viewing directions are obtained using</text>
<text top="947" left="73" width="377" height="13" font="4">the aforementioned extractor. For each candidate, the features,</text>
<text top="965" left="73" width="377" height="13" font="4">which are correlated with the viewing direction transition from</text>
<text top="983" left="73" width="377" height="13" font="4">the current one to the candidate, are detected to train a random</text>
<text top="1001" left="73" width="377" height="13" font="4">forest classifier <a href="xml/1709.06342v2.html#14">[56]. </a>Here, we select the features of 2D image</text>
<text top="1019" left="73" width="377" height="13" font="4">saliency prediction, since <a href="xml/1709.06342v2.html#14">[57] </a>validates that these features are</text>
<text top="1037" left="73" width="377" height="13" font="4">effective in predicting human attention. The features include</text>
<text top="1055" left="73" width="377" height="13" font="4">(1) the Euclidean distance from the viewport center to the</text>
<text top="1073" left="73" width="377" height="13" font="4">candidate; (2) the angle between the viewport center and the</text>
<text top="1091" left="73" width="377" height="13" font="4">candidate; (3) the standard deviation of the GMM used for</text>
<text top="1109" left="73" width="377" height="13" font="4">extracting the candidate; (4) the averaged saliency value of</text>
<text top="87" left="468" width="377" height="13" font="4">the region around the candidate; and (5) the local intensity</text>
<text top="104" left="468" width="377" height="13" font="4">contrast of the neighborhood around the candidate <a href="xml/1709.06342v2.html#14">[57]. </a>These</text>
<text top="122" left="468" width="377" height="13" font="4">features form a vector υ, which is the input to the random</text>
<text top="140" left="468" width="377" height="13" font="4">forest classifier. Furthermore, the viewing direction for the</text>
<text top="158" left="468" width="377" height="13" font="4">same subject in the next frame is annotated as the ground-</text>
<text top="176" left="468" width="377" height="13" font="4">truth and is the target output of the random forest classifier.</text>
<text top="194" left="468" width="377" height="13" font="4">Finally, the random forest classifier can be trained with the</text>
<text top="212" left="468" width="377" height="13" font="4">feature vectors and ground-truth viewing directions, and is</text>
<text top="230" left="468" width="377" height="13" font="4">then used to calculate the CP-PSNR of each omnidirectional</text>
<text top="248" left="468" width="377" height="13" font="4">video frame. Note that this stage is run offline only once to</text>
<text top="266" left="468" width="244" height="13" font="4">obtain the trained random forest model.</text>
<text top="283" left="483" width="362" height="13" font="4">Branch 1, Stage II: Viewing direction prediction of</text>
<text top="301" left="468" width="377" height="14" font="4">the next fame. For the CP-PSNR calculation, the first step</text>
<text top="319" left="468" width="377" height="13" font="4">is to predict the viewing direction. To predict the viewing</text>
<text top="337" left="468" width="377" height="13" font="4">direction, a few viewing direction candidates are extracted.</text>
<text top="355" left="468" width="377" height="13" font="4">Then, the MAP estimation is employed to select one viewing</text>
<text top="373" left="468" width="377" height="13" font="4">direction from the candidates given the detected features</text>
<text top="391" left="468" width="377" height="13" font="4">embedded in vector υ and the trained random forest model.</text>
<text top="409" left="468" width="377" height="13" font="4">Specifically, assuming that C is a viewing direction candidate,</text>
<text top="427" left="468" width="377" height="13" font="4">the averaged posterior probability of candidate C being the</text>
<text top="445" left="468" width="377" height="13" font="4">viewing direction (i.e., belonging to the positive class) can be</text>
<text top="463" left="468" width="72" height="13" font="4">obtained by</text>
<text top="501" left="564" width="7" height="12" font="3">g</text>
<text top="505" left="571" width="7" height="8" font="12">λ</text>
<text top="508" left="578" width="8" height="7" font="17">+</text>
<text top="501" left="587" width="36" height="12" font="3">(C) =</text>
<text top="492" left="630" width="7" height="12" font="3">1</text>
<text top="510" left="628" width="8" height="12" font="3">T</text>
<text top="487" left="649" width="7" height="8" font="12">T</text>
<text top="497" left="643" width="20" height="6" font="3">X</text>
<text top="519" left="643" width="20" height="8" font="12">τ =1</text>
<text top="501" left="665" width="24" height="12" font="3">P (λ</text>
<text top="505" left="689" width="8" height="8" font="12">+</text>
<text top="500" left="698" width="13" height="13" font="3">|υ</text>
<text top="505" left="711" width="5" height="8" font="12">τ</text>
<text top="501" left="718" width="30" height="12" font="3">(C)),</text>
<text top="501" left="822" width="22" height="12" font="3">(18)</text>
<text top="537" left="468" width="53" height="13" font="4">where λ</text>
<text top="542" left="521" width="9" height="9" font="0">+</text>
<text top="537" left="539" width="230" height="13" font="4">represents the positive class, and υ</text>
<text top="542" left="769" width="6" height="9" font="0">τ</text>
<text top="537" left="777" width="68" height="14" font="4">(C) is the</text>
<text top="555" left="468" width="377" height="13" font="4">feature vector of C input to tree τ . In addition, T is the number</text>
<text top="573" left="468" width="377" height="13" font="4">of classification trees in the trained random forest model. Note</text>
<text top="591" left="468" width="377" height="13" font="4">that each tree randomly chooses some features from the input</text>
<text top="609" left="468" width="311" height="13" font="4">feature vector υ for the classification, such that υ</text>
<text top="614" left="778" width="6" height="9" font="0">τ</text>
<text top="609" left="786" width="59" height="14" font="4">(C) ⊆ υ.</text>
<text top="627" left="468" width="377" height="13" font="4">Finally, the viewing direction is predicted for the next frame</text>
<text top="645" left="468" width="124" height="13" font="4">by MAP as follows,</text>
<text top="662" left="594" width="75" height="12" font="3">V = argmax</text>
<text top="676" left="642" width="8" height="8" font="12">C</text>
<text top="662" left="671" width="7" height="12" font="3">g</text>
<text top="667" left="678" width="7" height="8" font="12">λ</text>
<text top="670" left="684" width="8" height="7" font="17">+</text>
<text top="662" left="693" width="25" height="12" font="3">(C).</text>
<text top="662" left="822" width="22" height="12" font="3">(19)</text>
<text top="698" left="483" width="362" height="13" font="4">Branch 2, Stage II: CP-PSNR calculation of the current</text>
<text top="716" left="468" width="377" height="14" font="4">frame. Given the predicted viewing direction of the current</text>
<text top="734" left="468" width="377" height="13" font="4">frame, a viewport binary map can be generated, in which</text>
<text top="752" left="468" width="377" height="13" font="4">1 indicates that the corresponding pixel is in the viewport</text>
<text top="770" left="468" width="377" height="13" font="4">range and 0 means that the pixel is out of the viewport range.</text>
<text top="788" left="468" width="235" height="13" font="4">Then, a content-based weight map w</text>
<text top="785" left="703" width="3" height="10" font="0">0</text>
<text top="788" left="707" width="137" height="14" font="4">(s, t) is generated via</text>
<text top="806" left="468" width="377" height="13" font="4">multiplying the viewport binary map by the non-content-base</text>
<text top="824" left="468" width="89" height="13" font="4">weight map ˜</text>
<text top="824" left="547" width="298" height="14" font="4">w(s, t) (introduced in Section <a href="xml/1709.06342v2.html#7">V-A). </a>We further</text>
<text top="842" left="468" width="76" height="13" font="4">normalize w</text>
<text top="839" left="544" width="3" height="10" font="0">0</text>
<text top="842" left="548" width="51" height="14" font="4">(s, t) by</text>
<text top="881" left="586" width="14" height="6" font="3">f</text>
<text top="875" left="586" width="10" height="12" font="3">w</text>
<text top="874" left="596" width="3" height="8" font="12">0</text>
<text top="875" left="600" width="43" height="12" font="3">(s, t) =</text>
<text top="866" left="664" width="10" height="12" font="3">w</text>
<text top="863" left="674" width="3" height="8" font="12">0</text>
<text top="866" left="678" width="28" height="12" font="3">(s, t)</text>
<text top="883" left="649" width="15" height="6" font="3">P</text>
<text top="891" left="663" width="13" height="8" font="12">s,t</text>
<text top="884" left="679" width="10" height="12" font="3">w</text>
<text top="883" left="689" width="3" height="8" font="12">0</text>
<text top="884" left="693" width="28" height="12" font="3">(s, t)</text>
<text top="875" left="723" width="3" height="12" font="3">.</text>
<text top="875" left="822" width="22" height="12" font="3">(20)</text>
<text top="909" left="468" width="377" height="13" font="4">Finally, the CP-PSNR of each omnidirectional video frame</text>
<text top="926" left="468" width="124" height="13" font="4">can be calculated as</text>
<text top="957" left="491" width="107" height="12" font="3">CP-PSNR = 10 log</text>
<text top="948" left="685" width="6" height="12" font="3">I</text>
<text top="946" left="692" width="5" height="8" font="12">2</text>
<text top="954" left="691" width="20" height="8" font="12">max</text>
<text top="967" left="602" width="15" height="6" font="3">P</text>
<text top="976" left="617" width="13" height="8" font="12">s,t</text>
<text top="969" left="633" width="64" height="12" font="3">(I(s, t) − I</text>
<text top="967" left="697" width="3" height="8" font="12">0</text>
<text top="969" left="701" width="34" height="12" font="3">(s, t))</text>
<text top="968" left="735" width="5" height="8" font="12">2</text>
<text top="968" left="744" width="4" height="13" font="3">·</text>
<text top="974" left="752" width="14" height="6" font="3">f</text>
<text top="969" left="751" width="10" height="12" font="3">w</text>
<text top="967" left="762" width="3" height="8" font="12">0</text>
<text top="969" left="766" width="28" height="12" font="3">(s, t)</text>
<text top="957" left="796" width="3" height="12" font="3">.</text>
<text top="957" left="822" width="22" height="12" font="3">(21)</text>
<text top="993" left="468" width="377" height="13" font="4">As a result, CP-VQA can be obtained for measuring the</text>
<text top="1011" left="468" width="261" height="13" font="4">objective quality of omnidirectional video.</text>
<text top="1046" left="561" width="38" height="13" font="4">VI. E</text>
<text top="1048" left="600" width="91" height="11" font="5">XPERIMENTAL</text>
<text top="1046" left="695" width="10" height="13" font="4">R</text>
<text top="1048" left="705" width="46" height="11" font="5">ESULTS</text>
<text top="1068" left="468" width="276" height="13" font="4">A. Validation on our subjective VQA method</text>
<text top="1091" left="483" width="362" height="14" font="4">Test benchmark and setting. In this section, we validate</text>
<text top="1109" left="468" width="377" height="13" font="4">the effectiveness of our subjective VQA method. First, all 12</text>
</page>
<page number="10" position="absolute" top="0" left="0" height="1188" width="918">
<text top="40" left="834" width="10" height="9" font="0">10</text>
<text top="89" left="432" width="54" height="11" font="5">TABLE IV</text>
<text top="102" left="270" width="7" height="11" font="5">T</text>
<text top="104" left="277" width="131" height="9" font="7">HE FINAL OUTPUT AS THE</text>
<text top="102" left="411" width="49" height="11" font="5">V-DMOS</text>
<text top="104" left="464" width="181" height="9" font="7">OF THE IMPAIRED TEST SEQUENCES</text>
<text top="102" left="645" width="3" height="11" font="5">.</text>
<text top="117" left="85" width="14" height="10" font="0">QP</text>
<text top="117" left="124" width="26" height="10" font="0">Name</text>
<text top="117" left="203" width="42" height="10" font="0">V-DMOS</text>
<text top="117" left="298" width="26" height="10" font="0">Name</text>
<text top="117" left="378" width="42" height="10" font="0">V-DMOS</text>
<text top="117" left="483" width="26" height="10" font="0">Name</text>
<text top="117" left="572" width="42" height="10" font="0">V-DMOS</text>
<text top="117" left="677" width="26" height="10" font="0">Name</text>
<text top="117" left="766" width="42" height="10" font="0">V-DMOS</text>
<text top="130" left="87" width="11" height="10" font="0">27</text>
<text top="142" left="116" width="40" height="10" font="0">Dianying</text>
<text top="130" left="174" width="99" height="10" font="0">[43,43,45,36,43,48,33]</text>
<text top="142" left="289" width="44" height="10" font="0">Fengjing1</text>
<text top="130" left="349" width="99" height="10" font="0">[36,37,37,36,34,—,—]</text>
<text top="142" left="474" width="44" height="10" font="0">Fengjing3</text>
<text top="130" left="543" width="99" height="10" font="0">[36,35,34,43,38,72,21]</text>
<text top="142" left="669" width="42" height="10" font="0">Hangpai1</text>
<text top="130" left="737" width="99" height="10" font="0">[33,33,30,28,29,—,34]</text>
<text top="142" left="87" width="11" height="10" font="0">37</text>
<text top="142" left="174" width="99" height="10" font="0">[65,65,64,69,66,—,57]</text>
<text top="142" left="349" width="99" height="10" font="0">[64,64,65,70,66,64,—]</text>
<text top="142" left="543" width="99" height="10" font="0">[43,44,47,41,47,—,35]</text>
<text top="142" left="737" width="99" height="10" font="0">[47,47,48,41,46,—,41]</text>
<text top="154" left="87" width="11" height="10" font="0">42</text>
<text top="154" left="174" width="99" height="10" font="0">[71,71,66,64,71,—,54]</text>
<text top="154" left="349" width="99" height="10" font="0">[70,70,70,60,70,—,55]</text>
<text top="154" left="543" width="99" height="10" font="0">[54,55,54,44,54,—,38]</text>
<text top="154" left="737" width="99" height="10" font="0">[58,58,53,65,52,—,51]</text>
<text top="167" left="87" width="11" height="10" font="0">27</text>
<text top="179" left="115" width="42" height="10" font="0">Hangpai2</text>
<text top="167" left="174" width="99" height="10" font="0">[33,33,32,34,34,—,19]</text>
<text top="179" left="290" width="42" height="10" font="0">Hangpai3</text>
<text top="167" left="349" width="99" height="10" font="0">[36,36,35,33,36,—,—]</text>
<text top="179" left="483" width="25" height="10" font="0">Tiyu1</text>
<text top="167" left="543" width="99" height="10" font="0">[43,43,40,42,39,—,—]</text>
<text top="179" left="677" width="25" height="10" font="0">Tiyu2</text>
<text top="167" left="737" width="99" height="10" font="0">[35,35,31,—,30,—,—]</text>
<text top="179" left="87" width="11" height="10" font="0">37</text>
<text top="179" left="174" width="99" height="10" font="0">[40,40,40,43,40,—,32]</text>
<text top="179" left="349" width="99" height="10" font="0">[47,47,44,48,48,—,46]</text>
<text top="179" left="543" width="99" height="10" font="0">[59,59,56,—,60,—,66]</text>
<text top="179" left="737" width="99" height="10" font="0">[58,57,59,60,61,—,70]</text>
<text top="191" left="87" width="11" height="10" font="0">42</text>
<text top="191" left="174" width="99" height="10" font="0">[52,52,54,37,51,—,48]</text>
<text top="191" left="349" width="99" height="10" font="0">[58,57,55,63,62,—,—]</text>
<text top="191" left="543" width="99" height="10" font="0">[70,71,66,67,65,55,—]</text>
<text top="191" left="737" width="99" height="10" font="0">[66,66,64,—,66,—,71]</text>
<text top="204" left="87" width="11" height="10" font="0">27</text>
<text top="216" left="117" width="40" height="10" font="0">Xinwen1</text>
<text top="204" left="174" width="99" height="10" font="0">[34,33,33,34,33,—,35]</text>
<text top="216" left="291" width="40" height="10" font="0">Xinwen2</text>
<text top="204" left="349" width="99" height="10" font="0">[34,34,33,34,34,—,46]</text>
<text top="216" left="465" width="62" height="10" font="0">Yanchanghui1</text>
<text top="204" left="543" width="99" height="10" font="0">[35,34,35,42,34,—,—]</text>
<text top="216" left="659" width="62" height="10" font="0">Yanchanghui2</text>
<text top="204" left="737" width="99" height="10" font="0">[34,33,33,32,34,—,—]</text>
<text top="216" left="87" width="11" height="10" font="0">37</text>
<text top="216" left="174" width="99" height="10" font="0">[47,46,48,47,47,—,—]</text>
<text top="216" left="349" width="99" height="10" font="0">[55,55,55,51,56,—,59]</text>
<text top="216" left="543" width="99" height="10" font="0">[45,43,46,59,43,43,—]</text>
<text top="216" left="737" width="99" height="10" font="0">[52,50,52,58,55,—,—]</text>
<text top="228" left="87" width="11" height="10" font="0">42</text>
<text top="228" left="174" width="99" height="10" font="0">[58,57,61,72,57,—,50]</text>
<text top="228" left="349" width="99" height="10" font="0">[67,67,66,59,67,—,70]</text>
<text top="228" left="543" width="99" height="10" font="0">[59,58,62,65,58,—,—]</text>
<text top="228" left="737" width="99" height="10" font="0">[62,62,62,58,64,63,—]</text>
<text top="360" left="140" width="67" height="11" font="5">(a) Reference</text>
<text top="360" left="324" width="53" height="11" font="5">(b) QP=27</text>
<text top="480" left="144" width="52" height="11" font="5">(c) QP=37</text>
<text top="480" left="322" width="53" height="11" font="5">(d) QP=42</text>
<text top="492" left="73" width="377" height="11" font="5">Fig. 7. Illustration of a representative set of distorted frames (the 92th frame,</text>
<text top="505" left="73" width="68" height="10" font="5">Yanchanghui2</text>
<text top="505" left="141" width="204" height="11" font="5">) compressed under different QP settings.</text>
<text top="542" left="73" width="377" height="13" font="4">uncompressed omnidirectional video sequences from <a href="xml/1709.06342v2.html#14">[58] </a>(in</text>
<text top="560" left="73" width="377" height="13" font="4">YUV 4:2:0 format at resolution 4096 × 2048) are selected</text>
<text top="578" left="73" width="377" height="13" font="4">as the references. The duration of these sequences is 12</text>
<text top="596" left="73" width="377" height="13" font="4">seconds with a frame rate of 25 frame per second (fps). Then,</text>
<text top="614" left="73" width="377" height="13" font="4">H.265/HEVC is used to compress these 12 sequences at 3</text>
<text top="632" left="73" width="377" height="13" font="4">different bit-rates, under an equirectangular projection. For</text>
<text top="650" left="73" width="377" height="13" font="4">each sequence, the 3 bit-rates are set to be the actual bit-</text>
<text top="668" left="73" width="377" height="13" font="4">rates by quantization parameter (QP) = 27, 37 and 42. Figure</text>
<text top="686" left="73" width="377" height="13" font="4"><a href="xml/1709.06342v2.html#10">7 </a>shows a representative set of distorted frames compressed at</text>
<text top="704" left="73" width="377" height="13" font="4">these QP settings. Thus, there are 12 reference and 36 impaired</text>
<text top="721" left="73" width="377" height="13" font="4">sequences for the test in total. Note that all test sequences are</text>
<text top="739" left="73" width="377" height="13" font="4">non-overlapping with 48 sequences of our viewing direction</text>
<text top="757" left="73" width="377" height="13" font="4">database introduced in Section <a href="xml/1709.06342v2.html#3">III-A </a>for fair comparison with</text>
<text top="775" left="73" width="90" height="13" font="4">other methods.</text>
<text top="794" left="88" width="362" height="13" font="4">A total of 48 subjects participated in the subjective test for</text>
<text top="812" left="73" width="377" height="13" font="4">our VQA method (presented in Section <a href="xml/1709.06342v2.html#5">IV). </a>The subjects are</text>
<text top="830" left="73" width="377" height="13" font="4">non-overlapping with those mentioned in Section <a href="xml/1709.06342v2.html#3">III-A. </a>In the</text>
<text top="848" left="73" width="377" height="13" font="4">test, subjects were required to view and rate all sequences for</text>
<text top="866" left="73" width="377" height="13" font="4">raw subjective scores. Next, the O-DMOS and V-DMOS are</text>
<text top="884" left="73" width="377" height="13" font="4">calculated with the rated raw scores. Here, we simply set the</text>
<text top="902" left="73" width="67" height="13" font="4">threshold f</text>
<text top="907" left="140" width="6" height="9" font="0">0</text>
<text top="902" left="151" width="299" height="13" font="4">to be 1/6 in the V-DMOS calculation, as there are</text>
<text top="920" left="73" width="377" height="13" font="4">6 regions in our omnidirectional videos. It is worth mentioning</text>
<text top="938" left="73" width="377" height="13" font="4">that no subject was rejected in the calculation of the O-DMOS</text>
<text top="956" left="73" width="377" height="13" font="4">and V-DMOS values after using the subject rejection scheme</text>
<text top="974" left="73" width="377" height="13" font="4">from <a href="xml/1709.06342v2.html#13">[13]. </a>Finally, the values of the O-DMOS and V-DMOS</text>
<text top="991" left="73" width="377" height="13" font="4">obtained from the raw quality scores of the 48 subjects are</text>
<text top="1009" left="73" width="126" height="13" font="4">reported in Table <a href="xml/1709.06342v2.html#10">IV</a></text>
<text top="1007" left="199" width="5" height="9" font="0"><a href="xml/1709.06342v2.html#10">3</a></text>
<text top="1009" left="205" width="4" height="13" font="4"><a href="xml/1709.06342v2.html#10">.</a></text>
<text top="1028" left="88" width="362" height="13" font="4">Evaluation on the effectiveness of our subjective VQA</text>
<text top="1046" left="73" width="377" height="14" font="4">method. The effectiveness of our subjective VQA method</text>
<text top="1082" left="85" width="4" height="8" font="12">3</text>
<text top="1084" left="91" width="359" height="11" font="5">Note that the O-DMOS is included in the V-DMOS as the first element and</text>
<text top="1097" left="73" width="377" height="11" font="5">in bold in Table <a href="xml/1709.06342v2.html#10">IV. </a>The second to the seventh elements represent the DMOS</text>
<text top="1111" left="73" width="364" height="11" font="5">scores of the front, left, back, right, top, and bottom regions, respectively.</text>
<text top="261" left="468" width="377" height="13" font="4">is verified by evaluating the correlations between the O-</text>
<text top="278" left="468" width="377" height="13" font="4">DMOS/V-DMOS scores of different groups of subjects.</text>
<text top="296" left="468" width="377" height="13" font="4">Specifically, all 48 subjects are randomly and equally divided</text>
<text top="314" left="468" width="377" height="13" font="4">into two non-overlapping groups, Group 1 and Group 2, by 30</text>
<text top="332" left="468" width="377" height="13" font="4">trials. Then, the O-DMOS/V-DMOS values are averaged over</text>
<text top="350" left="468" width="377" height="13" font="4">30 trials, and the correlations of the averaged O-DMOS/V-</text>
<text top="368" left="468" width="377" height="13" font="4">DMOS values between two groups are evaluated as follows.</text>
<text top="386" left="468" width="377" height="13" font="4">Figure <a href="xml/1709.06342v2.html#11">8 </a>shows the curves of the ranked O-DMOS/V-DMOS</text>
<text top="404" left="468" width="38" height="13" font="4">v<a href="xml/1709.06342v2.html#10">alues</a></text>
<text top="402" left="506" width="5" height="9" font="0"><a href="xml/1709.06342v2.html#10">4</a></text>
<text top="404" left="518" width="327" height="13" font="4">for all 36 impaired sequences obtained by Group 1,</text>
<text top="422" left="468" width="377" height="13" font="4">and the figure also presents the O-DMOS/V-DMOS values by</text>
<text top="440" left="468" width="377" height="13" font="4">Group 2 for the sequences ranked by the values for Group 1.</text>
<text top="458" left="468" width="377" height="13" font="4">We can see from this figure that the correlations between the</text>
<text top="476" left="468" width="377" height="13" font="4">two groups of O-DMOS/V-DMOS values are extremely high.</text>
<text top="494" left="468" width="377" height="13" font="4">We quantify such correlations using Spearman’s rank corre-</text>
<text top="512" left="468" width="377" height="13" font="4">lation coefficient (SRCC), which is shown in Figure <a href="xml/1709.06342v2.html#11">8. </a>The</text>
<text top="530" left="468" width="377" height="13" font="4">high SRCC values again indicate the agreement between the</text>
<text top="547" left="468" width="377" height="13" font="4">two groups for O-DMOS and V-DMOS. Since two randomly</text>
<text top="565" left="468" width="377" height="13" font="4">selected groups can be seen as the results from two subjective</text>
<text top="583" left="468" width="377" height="13" font="4">tests, the achieved agreement over different subjective tests</text>
<text top="601" left="468" width="377" height="13" font="4">implies that our method is effective in assessing subjective</text>
<text top="619" left="468" width="247" height="13" font="4">quality of omnidirectional video coding.</text>
<text top="637" left="483" width="362" height="13" font="4">Performance analysis of our subjective VQA method.</text>
<text top="655" left="468" width="377" height="13" font="4">It is necessary to ascertain the minimum number of subjects</text>
<text top="673" left="468" width="377" height="13" font="4">required for our subjective VQA method. To this end, we</text>
<text top="691" left="468" width="377" height="13" font="4">measure the SRCC of the O-DMOS values between the two</text>
<text top="709" left="468" width="377" height="13" font="4">groups with different numbers of subjects. Accordingly, Fig-</text>
<text top="726" left="468" width="377" height="13" font="4">ure <a href="xml/1709.06342v2.html#11">9 </a>shows the SRCC with an increased number of subjects</text>
<text top="744" left="468" width="377" height="13" font="4">in Groups 1 and 2, which is the averaged result over 30 trials.</text>
<text top="762" left="468" width="377" height="13" font="4">We can see that SRCC converges when the number of subjects</text>
<text top="780" left="468" width="377" height="13" font="4">is more than 20. Thus, we recommend 20 as the minimum</text>
<text top="798" left="468" width="256" height="13" font="4">number of subjects for our VQA method.</text>
<text top="816" left="483" width="362" height="13" font="4">It is also interesting to investigate the relationship between</text>
<text top="834" left="468" width="377" height="13" font="4">the O-DMOS and V-DMOS values of different regions. Ta-</text>
<text top="852" left="468" width="377" height="13" font="4">ble <a href="xml/1709.06342v2.html#11">V </a>shows the SRCC results between the O-DMOS and V-</text>
<text top="870" left="468" width="377" height="13" font="4">DMOS values of different regions, which are calculated from</text>
<text top="888" left="468" width="377" height="13" font="4">all 48 subjects. It is clear that the V-DMOS values of the</text>
<text top="905" left="468" width="377" height="13" font="4">front, left and right regions have strong correlation with the</text>
<text top="923" left="468" width="377" height="13" font="4">O-DMOS values. In contrast, the V-DMOS values of the back</text>
<text top="941" left="468" width="377" height="13" font="4">and bottom regions are generally correlated with the O-DMOS</text>
<text top="959" left="468" width="377" height="13" font="4">values. However, the SRCC result for the V-DMOS values of</text>
<text top="977" left="468" width="377" height="13" font="4">the top region is rather small. It is because the V-DMOS values</text>
<text top="995" left="468" width="377" height="13" font="4">of the top region are determined by only few subjects, since</text>
<text top="1013" left="468" width="377" height="13" font="4">most subjects pay no attention to the top region. In general,</text>
<text top="1031" left="468" width="377" height="13" font="4">the correlations between O-DMOS and V-DMOS of different</text>
<text top="1049" left="468" width="377" height="13" font="4">regions agree with Finding 2, verifying the effectiveness of the</text>
<text top="1067" left="468" width="377" height="13" font="4">proposed V-DMOS metric. It can be concluded that the quality</text>
<text top="1095" left="480" width="4" height="8" font="12">4</text>
<text top="1097" left="485" width="359" height="11" font="5">Due to space limitations, we only show the values of the front, left and</text>
<text top="1111" left="468" width="133" height="11" font="5">right regions for V-DMOS.</text>
</page>
<page number="11" position="absolute" top="0" left="0" height="1188" width="918">
	<fontspec id="23" size="5" family="Times" color="#4b4847"/>
	<fontspec id="24" size="5" family="Times" color="#4b4847"/>
	<fontspec id="25" size="3" family="Times" color="#221714"/>
	<fontspec id="26" size="8" family="Times" color="#221714"/>
	<fontspec id="27" size="6" family="Times" color="#4b4847"/>
	<fontspec id="28" size="10" family="Times" color="#4b4847"/>
	<fontspec id="29" size="10" family="Times" color="#4b4847"/>
<text top="40" left="834" width="10" height="9" font="0">11</text>
<text top="196" left="91" width="3" height="7" font="9"><b>0</b></text>
<text top="196" left="113" width="3" height="7" font="9"><b>5</b></text>
<text top="196" left="133" width="5" height="7" font="9"><b>10</b></text>
<text top="196" left="155" width="5" height="7" font="9"><b>15</b></text>
<text top="196" left="176" width="5" height="7" font="9"><b>20</b></text>
<text top="196" left="197" width="5" height="7" font="9"><b>25</b></text>
<text top="196" left="219" width="5" height="7" font="9"><b>30</b></text>
<text top="196" left="240" width="5" height="7" font="9"><b>35</b></text>
<text top="200" left="112" width="124" height="10" font="23"><b>Sequence rank N, according to Group 1</b></text>
<text top="192" left="86" width="5" height="7" font="9"><b>30</b></text>
<text top="181" left="86" width="5" height="7" font="9"><b>35</b></text>
<text top="169" left="86" width="5" height="7" font="9"><b>40</b></text>
<text top="158" left="86" width="5" height="7" font="9"><b>45</b></text>
<text top="147" left="86" width="5" height="7" font="9"><b>50</b></text>
<text top="135" left="86" width="5" height="7" font="9"><b>55</b></text>
<text top="124" left="86" width="5" height="7" font="9"><b>60</b></text>
<text top="113" left="86" width="5" height="7" font="9"><b>65</b></text>
<text top="101" left="86" width="5" height="7" font="9"><b>70</b></text>
<text top="90" left="86" width="5" height="7" font="9"><b>75</b></text>
<text top="186" left="84" width="0" height="10" font="24"><b>Averaged DMOS of each group</b></text>
<text top="176" left="227" width="21" height="7" font="25"><b> Group 1</b></text>
<text top="182" left="227" width="21" height="7" font="25"><b> Group 2</b></text>
<text top="93" left="145" width="58" height="14" font="26"><b>SRCC:0.9587</b></text>
<text top="219" left="135" width="65" height="11" font="5">(a) O-DMOS</text>
<text top="196" left="284" width="3" height="7" font="9"><b>0</b></text>
<text top="196" left="306" width="3" height="7" font="9"><b>5</b></text>
<text top="196" left="326" width="5" height="7" font="9"><b>10</b></text>
<text top="196" left="347" width="5" height="7" font="9"><b>15</b></text>
<text top="196" left="369" width="5" height="7" font="9"><b>20</b></text>
<text top="196" left="390" width="5" height="7" font="9"><b>25</b></text>
<text top="196" left="412" width="5" height="7" font="9"><b>30</b></text>
<text top="196" left="433" width="5" height="7" font="9"><b>35</b></text>
<text top="200" left="305" width="124" height="10" font="23"><b>Sequence rank N, according to Group 1</b></text>
<text top="192" left="279" width="5" height="7" font="9"><b>30</b></text>
<text top="181" left="279" width="5" height="7" font="9"><b>35</b></text>
<text top="169" left="279" width="5" height="7" font="9"><b>40</b></text>
<text top="158" left="279" width="5" height="7" font="9"><b>45</b></text>
<text top="147" left="279" width="5" height="7" font="9"><b>50</b></text>
<text top="135" left="279" width="5" height="7" font="9"><b>55</b></text>
<text top="124" left="279" width="5" height="7" font="9"><b>60</b></text>
<text top="113" left="279" width="5" height="7" font="9"><b>65</b></text>
<text top="101" left="279" width="5" height="7" font="9"><b>70</b></text>
<text top="90" left="279" width="5" height="7" font="9"><b>75</b></text>
<text top="186" left="277" width="0" height="10" font="24"><b>Averaged DMOS of each group</b></text>
<text top="177" left="422" width="21" height="7" font="25"><b> Group 1</b></text>
<text top="183" left="422" width="21" height="7" font="25"><b> Group 2</b></text>
<text top="93" left="338" width="58" height="14" font="26"><b>SRCC:0.9564</b></text>
<text top="219" left="309" width="103" height="11" font="5">(b) V-DMOS (Front)</text>
<text top="196" left="477" width="3" height="7" font="9"><b>0</b></text>
<text top="196" left="498" width="3" height="7" font="9"><b>5</b></text>
<text top="196" left="519" width="5" height="7" font="9"><b>10</b></text>
<text top="196" left="540" width="5" height="7" font="9"><b>15</b></text>
<text top="196" left="562" width="5" height="7" font="9"><b>20</b></text>
<text top="196" left="583" width="5" height="7" font="9"><b>25</b></text>
<text top="196" left="604" width="5" height="7" font="9"><b>30</b></text>
<text top="196" left="626" width="5" height="7" font="9"><b>35</b></text>
<text top="200" left="498" width="124" height="10" font="23"><b>Sequence rank N, according to Group 1</b></text>
<text top="192" left="472" width="5" height="7" font="9"><b>30</b></text>
<text top="181" left="472" width="5" height="7" font="9"><b>35</b></text>
<text top="169" left="472" width="5" height="7" font="9"><b>40</b></text>
<text top="158" left="472" width="5" height="7" font="9"><b>45</b></text>
<text top="147" left="472" width="5" height="7" font="9"><b>50</b></text>
<text top="135" left="472" width="5" height="7" font="9"><b>55</b></text>
<text top="124" left="472" width="5" height="7" font="9"><b>60</b></text>
<text top="113" left="472" width="5" height="7" font="9"><b>65</b></text>
<text top="101" left="472" width="5" height="7" font="9"><b>70</b></text>
<text top="90" left="472" width="5" height="7" font="9"><b>75</b></text>
<text top="186" left="469" width="0" height="10" font="24"><b>Averaged DMOS of each group</b></text>
<text top="176" left="612" width="21" height="7" font="25"><b> Group 1</b></text>
<text top="182" left="612" width="21" height="7" font="25"><b> Group 2</b></text>
<text top="93" left="531" width="58" height="14" font="26"><b>SRCC:0.9153</b></text>
<text top="219" left="505" width="96" height="11" font="5">(c) V-DMOS (Left)</text>
<text top="196" left="670" width="3" height="7" font="9"><b>0</b></text>
<text top="196" left="691" width="3" height="7" font="9"><b>5</b></text>
<text top="196" left="711" width="5" height="7" font="9"><b>10</b></text>
<text top="196" left="733" width="5" height="7" font="9"><b>15</b></text>
<text top="196" left="754" width="5" height="7" font="9"><b>20</b></text>
<text top="196" left="776" width="5" height="7" font="9"><b>25</b></text>
<text top="196" left="797" width="5" height="7" font="9"><b>30</b></text>
<text top="196" left="819" width="5" height="7" font="9"><b>35</b></text>
<text top="200" left="691" width="124" height="10" font="23"><b>Sequence rank N, according to Group 1</b></text>
<text top="192" left="665" width="5" height="7" font="9"><b>30</b></text>
<text top="181" left="665" width="5" height="7" font="9"><b>35</b></text>
<text top="169" left="665" width="5" height="7" font="9"><b>40</b></text>
<text top="158" left="665" width="5" height="7" font="9"><b>45</b></text>
<text top="147" left="665" width="5" height="7" font="9"><b>50</b></text>
<text top="135" left="665" width="5" height="7" font="9"><b>55</b></text>
<text top="124" left="665" width="5" height="7" font="9"><b>60</b></text>
<text top="113" left="665" width="5" height="7" font="9"><b>65</b></text>
<text top="101" left="665" width="5" height="7" font="9"><b>70</b></text>
<text top="90" left="665" width="5" height="7" font="9"><b>75</b></text>
<text top="186" left="662" width="0" height="10" font="24"><b>Averaged DMOS of each group</b></text>
<text top="174" left="804" width="21" height="7" font="25"><b> Group 1</b></text>
<text top="182" left="804" width="21" height="7" font="25"><b> Group 2</b></text>
<text top="93" left="723" width="58" height="14" font="26"><b>SRCC:0.9334</b></text>
<text top="219" left="694" width="104" height="11" font="5">(d) V-DMOS (Right)</text>
<text top="232" left="73" width="771" height="11" font="5">Fig. 8. Curves of the O-DMOS/V-DMOS values of each impaired sequence for two non-overlapping groups of subjects with equal size, in which the sequences</text>
<text top="245" left="73" width="428" height="11" font="5">are ranked in increasing order according to the O-DMOS/V-DMOS values of Group 1.</text>
<text top="282" left="237" width="50" height="11" font="5">TABLE V</text>
<text top="295" left="82" width="32" height="11" font="5">SRCC</text>
<text top="297" left="118" width="73" height="9" font="7">BETWEEN THE</text>
<text top="295" left="195" width="50" height="11" font="5">O-DMOS</text>
<text top="297" left="248" width="22" height="9" font="7">AND</text>
<text top="295" left="274" width="49" height="11" font="5">V-DMOS</text>
<text top="297" left="326" width="115" height="9" font="7">SCORES OF DIFFERENT</text>
<text top="310" left="237" width="45" height="9" font="7">REGIONS</text>
<text top="308" left="283" width="3" height="11" font="5">.</text>
<text top="325" left="88" width="36" height="11" font="5">Region</text>
<text top="325" left="145" width="26" height="11" font="5">Front</text>
<text top="325" left="199" width="20" height="11" font="5">Left</text>
<text top="325" left="246" width="27" height="11" font="5">Right</text>
<text top="325" left="298" width="25" height="11" font="5">Back</text>
<text top="325" left="354" width="18" height="11" font="5">Top</text>
<text top="325" left="400" width="36" height="11" font="5">Bottom</text>
<text top="339" left="90" width="33" height="11" font="5">SRCC</text>
<text top="339" left="142" width="33" height="11" font="5">0.9972</text>
<text top="339" left="192" width="33" height="11" font="5">0.9794</text>
<text top="339" left="243" width="33" height="11" font="5">0.9750</text>
<text top="339" left="294" width="33" height="11" font="5">0.8844</text>
<text top="339" left="345" width="37" height="11" font="5">-0.0857</text>
<text top="339" left="401" width="33" height="11" font="5">0.8487</text>
<text top="537" left="116" width="4" height="11" font="27"><b>0</b></text>
<text top="537" left="177" width="4" height="11" font="27"><b>5</b></text>
<text top="537" left="236" width="8" height="11" font="27"><b>10</b></text>
<text top="537" left="298" width="8" height="11" font="27"><b>15</b></text>
<text top="537" left="359" width="8" height="11" font="27"><b>20</b></text>
<text top="537" left="420" width="8" height="11" font="27"><b>25</b></text>
<text top="543" left="184" width="175" height="17" font="28"><b>Number of subjects in each group</b></text>
<text top="530" left="105" width="10" height="11" font="27"><b>0.8</b></text>
<text top="507" left="101" width="14" height="11" font="27"><b>0.82</b></text>
<text top="484" left="101" width="14" height="11" font="27"><b>0.84</b></text>
<text top="462" left="101" width="14" height="11" font="27"><b>0.86</b></text>
<text top="439" left="101" width="14" height="11" font="27"><b>0.88</b></text>
<text top="416" left="105" width="10" height="11" font="27"><b>0.9</b></text>
<text top="394" left="101" width="14" height="11" font="27"><b>0.92</b></text>
<text top="371" left="101" width="14" height="11" font="27"><b>0.94</b></text>
<text top="348" left="101" width="14" height="11" font="27"><b>0.96</b></text>
<text top="475" left="98" width="0" height="17" font="29"><b>Averaged SRCC</b></text>
<text top="560" left="73" width="34" height="11" font="5">Fig. 9.</text>
<text top="560" left="121" width="329" height="11" font="5">SRCC of O-DMOS scores between two groups with increasing</text>
<text top="574" left="73" width="377" height="11" font="5">numbers of subjects in both groups, representing averaged result over 30</text>
<text top="587" left="73" width="27" height="11" font="5">trials.</text>
<text top="617" left="73" width="377" height="13" font="4">of different regions does not have equal contribution to the</text>
<text top="635" left="73" width="377" height="13" font="4">overall quality of the whole omnidirectional video. V-DMOS</text>
<text top="653" left="73" width="377" height="13" font="4">allows us to obtain the quality of different regions and their</text>
<text top="671" left="73" width="377" height="13" font="4">correlation with the overall quality of omnidirectional video.</text>
<text top="689" left="73" width="377" height="13" font="4">Then, we are able to adjust the optimization schemes, bit</text>
<text top="707" left="73" width="377" height="13" font="4">allocation and bandwidth allocation in coding and streaming</text>
<text top="725" left="73" width="288" height="13" font="4">for different regions of omnidirectional videos.</text>
<text top="764" left="73" width="276" height="13" font="4">B. Validation on our objective VQA methods</text>
<text top="786" left="88" width="362" height="14" font="4">Test benchmark and evaluation metrics. The performance</text>
<text top="804" left="73" width="377" height="13" font="4">of our objective VQA methods is evaluated by measuring</text>
<text top="822" left="73" width="377" height="13" font="4">the agreement between subjective and objective quality. The</text>
<text top="840" left="73" width="377" height="13" font="4">performance evaluation is conducted on 36 impaired sequences</text>
<text top="858" left="73" width="377" height="13" font="4">of 12 uncompressed omnidirectional video sequences, as men-</text>
<text top="876" left="73" width="377" height="13" font="4">tioned in Section <a href="xml/1709.06342v2.html#9">VI-A. </a>Here, the subjective quality of those</text>
<text top="893" left="73" width="377" height="13" font="4">impaired sequences is the O-DMOS values of 48 subjects</text>
<text top="911" left="73" width="377" height="13" font="4">obtained in Section <a href="xml/1709.06342v2.html#9">VI-A. </a>For calculating CP-PSNR, all 48</text>
<text top="929" left="73" width="377" height="13" font="4">omnidirectional video sequences from our viewing direction</text>
<text top="947" left="73" width="377" height="13" font="4">database presented in Section <a href="xml/1709.06342v2.html#3">III-A, </a>which does not over-</text>
<text top="965" left="73" width="377" height="13" font="4">lap with any test sequence of this section, are used as the</text>
<text top="983" left="73" width="377" height="13" font="4">training data to learn the random forest model. All PSNR-</text>
<text top="1001" left="73" width="377" height="13" font="4">related objective methods are calculated on the Y component</text>
<text top="1019" left="73" width="377" height="13" font="4">and averaged over all frames for each impaired sequence.</text>
<text top="1037" left="73" width="377" height="13" font="4">Note that no parameter needs to be re-estimated on the test</text>
<text top="1055" left="73" width="377" height="13" font="4">sequences for fair comparison. Given the O-DMOS results,</text>
<text top="1073" left="73" width="377" height="13" font="4">the performance of the objective VQA is measured with</text>
<text top="1091" left="73" width="377" height="13" font="4">SRCC, Pearson correlation coefficient (PCC), Root-Mean-</text>
<text top="1109" left="73" width="377" height="13" font="4">Square Error (RMSE) and Mean Absolute Error (MAE).</text>
<text top="279" left="468" width="377" height="13" font="4">SRCC measures the monotonicity of the objective quality with</text>
<text top="297" left="468" width="377" height="13" font="4">respect to subjective quality, while PCC quantifies the corre-</text>
<text top="315" left="468" width="377" height="13" font="4">lation coefficients between subjective and objective quality. In</text>
<text top="333" left="468" width="377" height="13" font="4">addition, RMSE and MAE measure the difference between the</text>
<text top="351" left="468" width="377" height="13" font="4">objective and subjective VQA results. Obviously, large-valued</text>
<text top="369" left="468" width="377" height="13" font="4">SRCC and PCC, or a small-valued RMSE and MAE, indicate</text>
<text top="387" left="468" width="377" height="13" font="4">a high degree of agreement between objective and subjective</text>
<text top="405" left="468" width="377" height="13" font="4">methods. We follow <a href="xml/1709.06342v2.html#13">[27] </a>to apply a logistic function for fitting</text>
<text top="423" left="468" width="377" height="13" font="4">the objective VQA scores to the subjective O-DMOS scores</text>
<text top="441" left="468" width="377" height="13" font="4">in the performance evaluation for the objective VQA methods,</text>
<text top="473" left="574" width="11" height="12" font="3">Q</text>
<text top="469" left="585" width="3" height="8" font="12">0</text>
<text top="479" left="585" width="5" height="8" font="12">j</text>
<text top="473" left="595" width="22" height="12" font="3">= β</text>
<text top="478" left="617" width="5" height="8" font="12">2</text>
<text top="473" left="626" width="11" height="12" font="3">+</text>
<text top="465" left="665" width="8" height="12" font="3">β</text>
<text top="469" left="673" width="5" height="8" font="12">1</text>
<text top="464" left="682" width="22" height="13" font="3">− β</text>
<text top="469" left="704" width="5" height="8" font="12">2</text>
<text top="495" left="642" width="30" height="12" font="3">1 + e</text>
<text top="487" left="672" width="9" height="8" font="12">−</text>
<text top="481" left="680" width="8" height="4" font="12"></text>
<text top="482" left="690" width="33" height="9" font="17">Qj −β3</text>
<text top="493" left="697" width="19" height="9" font="17">|β4|</text>
<text top="481" left="725" width="8" height="4" font="12"></text>
<text top="473" left="735" width="3" height="12" font="3">,</text>
<text top="473" left="822" width="22" height="12" font="3">(22)</text>
<text top="517" left="468" width="54" height="13" font="4">where Q</text>
<text top="522" left="522" width="5" height="9" font="0">j</text>
<text top="517" left="534" width="39" height="13" font="4">and Q</text>
<text top="514" left="573" width="3" height="10" font="0">0</text>
<text top="524" left="573" width="5" height="9" font="0">j</text>
<text top="517" left="584" width="260" height="13" font="4">are the original and fitted objective scores</text>
<text top="535" left="468" width="245" height="13" font="4">for sequence j, respectively. In <a href="xml/1709.06342v2.html#11">(22), </a>β</text>
<text top="540" left="713" width="6" height="9" font="0">1</text>
<text top="535" left="720" width="19" height="13" font="4">, β</text>
<text top="540" left="738" width="6" height="9" font="0">2</text>
<text top="535" left="745" width="19" height="13" font="4">, β</text>
<text top="540" left="764" width="6" height="9" font="0">3</text>
<text top="535" left="777" width="36" height="13" font="4">and β</text>
<text top="540" left="813" width="6" height="9" font="0">4</text>
<text top="535" left="826" width="18" height="13" font="4">are</text>
<text top="553" left="468" width="377" height="13" font="4">fitting parameters, initialized in the same way as <a href="xml/1709.06342v2.html#13">[27]. </a>The</text>
<text top="571" left="468" width="377" height="13" font="4">non-linear least squares optimization is performed to obtain</text>
<text top="589" left="468" width="176" height="13" font="4">the optimal parameters of β</text>
<text top="594" left="644" width="6" height="9" font="0">1</text>
<text top="589" left="651" width="19" height="13" font="4">, β</text>
<text top="594" left="670" width="6" height="9" font="0">2</text>
<text top="589" left="676" width="19" height="13" font="4">, β</text>
<text top="594" left="695" width="6" height="9" font="0">3</text>
<text top="589" left="708" width="37" height="13" font="4">and β</text>
<text top="594" left="745" width="6" height="9" font="0">4</text>
<text top="589" left="751" width="93" height="13" font="4">. Then, SRCC,</text>
<text top="606" left="468" width="377" height="13" font="4">PCC, RMSE and MAE are calculated between the O-DMOS</text>
<text top="624" left="468" width="377" height="13" font="4">values and the fitted objective scores. Note that the O-DMOS</text>
<text top="642" left="468" width="377" height="13" font="4">values are reversed (i.e., subtracted from 100) for curve fitting</text>
<text top="660" left="468" width="158" height="13" font="4">with the logistic function.</text>
<text top="678" left="483" width="362" height="14" font="4">Comparison of scatter plots. Now, we compare our two</text>
<text top="696" left="468" width="377" height="13" font="4">objective VQA methods, NCP-PSNR and CP-PSNR, with</text>
<text top="714" left="468" width="377" height="13" font="4">traditional PSNR and five state-of-the-art methods. The five</text>
<text top="732" left="468" width="377" height="13" font="4">methods include S-PSNR, latitude-weighted S-PSNR (lwS-</text>
<text top="750" left="468" width="377" height="13" font="4">PSNR) and sphere-weighted S-PSNR (swS-PSNR), all of</text>
<text top="768" left="468" width="377" height="13" font="4">which are from <a href="xml/1709.06342v2.html#13">[16], </a>as well as the latest W-PSNR and</text>
<text top="786" left="468" width="377" height="13" font="4">CPP-PSNR. In addition to these PSNR based methods, four</text>
<text top="804" left="468" width="377" height="13" font="4">objective VQA methods for 2D video, SSIM, VSNR, UQI and</text>
<text top="822" left="468" width="377" height="13" font="4">IFC, are also calculated for comparison. Figure <a href="xml/1709.06342v2.html#12">10 </a>shows the</text>
<text top="840" left="468" width="377" height="13" font="4">scatter plots of objective VQA results versus the O-DMOS</text>
<text top="858" left="468" width="377" height="13" font="4">results for all 36 impaired sequences along with the logistic</text>
<text top="876" left="468" width="377" height="13" font="4">fitting curves. In general, intensive scatter points close to the</text>
<text top="893" left="468" width="377" height="13" font="4">fitting curve indicate high correlation of the objective VQA</text>
<text top="911" left="468" width="377" height="13" font="4">results with the subjective results, validating the effectiveness</text>
<text top="929" left="468" width="377" height="13" font="4">of the objective VQA method. It can be clearly seen from</text>
<text top="947" left="468" width="377" height="13" font="4">Figure <a href="xml/1709.06342v2.html#12">10 </a>that the VQA results of our NCP-PSNR and CP-</text>
<text top="965" left="468" width="377" height="13" font="4">PSNR methods have a much higher correlation with the O-</text>
<text top="983" left="468" width="377" height="13" font="4">DMOS results, compared to other VQA methods. Therefore,</text>
<text top="1001" left="468" width="377" height="13" font="4">we can conclude that both the NCP-PSNR and CP-PSNR</text>
<text top="1019" left="468" width="236" height="13" font="4">perform far better than other methods.</text>
<text top="1037" left="483" width="362" height="14" font="4">Comparison on quantification results. Furthermore, Table</text>
<text top="1055" left="468" width="377" height="13" font="4"><a href="xml/1709.06342v2.html#12">VI </a>reports the SRCC, PCC, RMSE and MAE between the</text>
<text top="1073" left="468" width="377" height="13" font="4">fitted results of objective VQA and the subjective O-DMOS</text>
<text top="1091" left="468" width="377" height="13" font="4">results, over all 36 impaired omnidirectional video sequences.</text>
<text top="1109" left="468" width="377" height="13" font="4">We can see from Table <a href="xml/1709.06342v2.html#12">VI </a>that our NCP-PSNR and CP-</text>
</page>
<page number="12" position="absolute" top="0" left="0" height="1188" width="918">
	<fontspec id="30" size="5" family="Times" color="#252525"/>
	<fontspec id="31" size="6" family="Times" color="#252525"/>
	<fontspec id="32" size="6" family="Times" color="#252525"/>
<text top="40" left="834" width="10" height="9" font="0">12</text>
<text top="189" left="141" width="8" height="11" font="30"><b>25</b></text>
<text top="189" left="168" width="8" height="11" font="30"><b>30</b></text>
<text top="189" left="194" width="8" height="11" font="30"><b>35</b></text>
<text top="189" left="221" width="8" height="11" font="30"><b>40</b></text>
<text top="189" left="247" width="8" height="11" font="30"><b>45</b></text>
<text top="189" left="273" width="8" height="11" font="30"><b>50</b></text>
<text top="198" left="200" width="23" height="12" font="31"><b>PSNR</b></text>
<text top="182" left="136" width="8" height="11" font="30"><b>30</b></text>
<text top="159" left="136" width="8" height="11" font="30"><b>40</b></text>
<text top="136" left="136" width="8" height="11" font="30"><b>50</b></text>
<text top="113" left="136" width="8" height="11" font="30"><b>60</b></text>
<text top="89" left="136" width="8" height="11" font="30"><b>70</b></text>
<text top="171" left="133" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="189" left="311" width="8" height="11" font="30"><b>25</b></text>
<text top="189" left="338" width="8" height="11" font="30"><b>30</b></text>
<text top="189" left="364" width="8" height="11" font="30"><b>35</b></text>
<text top="189" left="391" width="8" height="11" font="30"><b>40</b></text>
<text top="189" left="417" width="8" height="11" font="30"><b>45</b></text>
<text top="189" left="443" width="8" height="11" font="30"><b>50</b></text>
<text top="198" left="364" width="34" height="12" font="31"><b>W-PSNR</b></text>
<text top="182" left="305" width="8" height="11" font="30"><b>30</b></text>
<text top="159" left="305" width="8" height="11" font="30"><b>40</b></text>
<text top="136" left="305" width="8" height="11" font="30"><b>50</b></text>
<text top="113" left="305" width="8" height="11" font="30"><b>60</b></text>
<text top="89" left="305" width="8" height="11" font="30"><b>70</b></text>
<text top="171" left="303" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="189" left="481" width="8" height="11" font="30"><b>25</b></text>
<text top="189" left="508" width="8" height="11" font="30"><b>30</b></text>
<text top="189" left="534" width="8" height="11" font="30"><b>35</b></text>
<text top="189" left="560" width="8" height="11" font="30"><b>40</b></text>
<text top="189" left="587" width="8" height="11" font="30"><b>45</b></text>
<text top="189" left="613" width="8" height="11" font="30"><b>50</b></text>
<text top="198" left="530" width="42" height="12" font="31"><b>CPP-PSNR</b></text>
<text top="182" left="475" width="8" height="11" font="30"><b>30</b></text>
<text top="159" left="475" width="8" height="11" font="30"><b>40</b></text>
<text top="136" left="475" width="8" height="11" font="30"><b>50</b></text>
<text top="113" left="475" width="8" height="11" font="30"><b>60</b></text>
<text top="89" left="475" width="8" height="11" font="30"><b>70</b></text>
<text top="171" left="473" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="189" left="651" width="8" height="11" font="30"><b>25</b></text>
<text top="189" left="678" width="8" height="11" font="30"><b>30</b></text>
<text top="189" left="704" width="8" height="11" font="30"><b>35</b></text>
<text top="189" left="730" width="8" height="11" font="30"><b>40</b></text>
<text top="189" left="757" width="8" height="11" font="30"><b>45</b></text>
<text top="189" left="783" width="8" height="11" font="30"><b>50</b></text>
<text top="198" left="706" width="30" height="12" font="31"><b>S-PSNR</b></text>
<text top="182" left="645" width="8" height="11" font="30"><b>30</b></text>
<text top="159" left="645" width="8" height="11" font="30"><b>40</b></text>
<text top="136" left="645" width="8" height="11" font="30"><b>50</b></text>
<text top="113" left="645" width="8" height="11" font="30"><b>60</b></text>
<text top="89" left="645" width="8" height="11" font="30"><b>70</b></text>
<text top="171" left="643" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="320" left="141" width="8" height="11" font="30"><b>25</b></text>
<text top="320" left="168" width="8" height="11" font="30"><b>30</b></text>
<text top="320" left="194" width="8" height="11" font="30"><b>35</b></text>
<text top="320" left="221" width="8" height="11" font="30"><b>40</b></text>
<text top="320" left="247" width="8" height="11" font="30"><b>45</b></text>
<text top="320" left="273" width="8" height="11" font="30"><b>50</b></text>
<text top="329" left="192" width="39" height="12" font="31"><b>lwS-PSNR</b></text>
<text top="313" left="136" width="8" height="11" font="30"><b>30</b></text>
<text top="290" left="136" width="8" height="11" font="30"><b>40</b></text>
<text top="267" left="136" width="8" height="11" font="30"><b>50</b></text>
<text top="243" left="136" width="8" height="11" font="30"><b>60</b></text>
<text top="220" left="136" width="8" height="11" font="30"><b>70</b></text>
<text top="302" left="133" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="320" left="311" width="8" height="11" font="30"><b>25</b></text>
<text top="320" left="338" width="8" height="11" font="30"><b>30</b></text>
<text top="320" left="364" width="8" height="11" font="30"><b>35</b></text>
<text top="320" left="391" width="8" height="11" font="30"><b>40</b></text>
<text top="320" left="417" width="8" height="11" font="30"><b>45</b></text>
<text top="320" left="443" width="8" height="11" font="30"><b>50</b></text>
<text top="329" left="361" width="40" height="12" font="31"><b>swS-PSNR</b></text>
<text top="313" left="305" width="8" height="11" font="30"><b>30</b></text>
<text top="290" left="305" width="8" height="11" font="30"><b>40</b></text>
<text top="267" left="305" width="8" height="11" font="30"><b>50</b></text>
<text top="243" left="305" width="8" height="11" font="30"><b>60</b></text>
<text top="220" left="305" width="8" height="11" font="30"><b>70</b></text>
<text top="302" left="303" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="320" left="481" width="8" height="11" font="30"><b>25</b></text>
<text top="320" left="508" width="8" height="11" font="30"><b>30</b></text>
<text top="320" left="534" width="8" height="11" font="30"><b>35</b></text>
<text top="320" left="560" width="8" height="11" font="30"><b>40</b></text>
<text top="320" left="587" width="8" height="11" font="30"><b>45</b></text>
<text top="320" left="613" width="8" height="11" font="30"><b>50</b></text>
<text top="329" left="530" width="43" height="12" font="31"><b>NCP-PSNR</b></text>
<text top="313" left="475" width="8" height="11" font="30"><b>30</b></text>
<text top="290" left="475" width="8" height="11" font="30"><b>40</b></text>
<text top="267" left="475" width="8" height="11" font="30"><b>50</b></text>
<text top="243" left="475" width="8" height="11" font="30"><b>60</b></text>
<text top="220" left="475" width="8" height="11" font="30"><b>70</b></text>
<text top="302" left="473" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="320" left="651" width="8" height="11" font="30"><b>25</b></text>
<text top="320" left="678" width="8" height="11" font="30"><b>30</b></text>
<text top="320" left="704" width="8" height="11" font="30"><b>35</b></text>
<text top="320" left="730" width="8" height="11" font="30"><b>40</b></text>
<text top="320" left="757" width="8" height="11" font="30"><b>45</b></text>
<text top="320" left="783" width="8" height="11" font="30"><b>50</b></text>
<text top="329" left="703" width="37" height="12" font="31"><b>CP-PSNR</b></text>
<text top="313" left="645" width="8" height="11" font="30"><b>30</b></text>
<text top="290" left="645" width="8" height="11" font="30"><b>40</b></text>
<text top="267" left="645" width="8" height="11" font="30"><b>50</b></text>
<text top="243" left="645" width="8" height="11" font="30"><b>60</b></text>
<text top="220" left="645" width="8" height="11" font="30"><b>70</b></text>
<text top="302" left="643" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="450" left="138" width="10" height="11" font="30"><b>0.2</b></text>
<text top="450" left="181" width="10" height="11" font="30"><b>0.4</b></text>
<text top="450" left="224" width="10" height="11" font="30"><b>0.6</b></text>
<text top="450" left="267" width="10" height="11" font="30"><b>0.8</b></text>
<text top="459" left="198" width="21" height="12" font="31"><b>SSIM</b></text>
<text top="444" left="133" width="8" height="11" font="30"><b>30</b></text>
<text top="420" left="133" width="8" height="11" font="30"><b>40</b></text>
<text top="397" left="133" width="8" height="11" font="30"><b>50</b></text>
<text top="374" left="133" width="8" height="11" font="30"><b>60</b></text>
<text top="351" left="133" width="8" height="11" font="30"><b>70</b></text>
<text top="432" left="130" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="450" left="309" width="8" height="11" font="30"><b>20</b></text>
<text top="450" left="353" width="8" height="11" font="30"><b>30</b></text>
<text top="450" left="397" width="8" height="11" font="30"><b>40</b></text>
<text top="450" left="441" width="8" height="11" font="30"><b>50</b></text>
<text top="459" left="367" width="23" height="12" font="31"><b>VSNR</b></text>
<text top="444" left="303" width="8" height="11" font="30"><b>30</b></text>
<text top="420" left="303" width="8" height="11" font="30"><b>40</b></text>
<text top="397" left="303" width="8" height="11" font="30"><b>50</b></text>
<text top="374" left="303" width="8" height="11" font="30"><b>60</b></text>
<text top="351" left="303" width="8" height="11" font="30"><b>70</b></text>
<text top="432" left="300" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="450" left="489" width="10" height="11" font="30"><b>0.3</b></text>
<text top="450" left="511" width="10" height="11" font="30"><b>0.4</b></text>
<text top="450" left="533" width="10" height="11" font="30"><b>0.5</b></text>
<text top="450" left="555" width="10" height="11" font="30"><b>0.6</b></text>
<text top="450" left="577" width="10" height="11" font="30"><b>0.7</b></text>
<text top="450" left="599" width="10" height="11" font="30"><b>0.8</b></text>
<text top="458" left="540" width="16" height="12" font="31"><b>UQI</b></text>
<text top="444" left="473" width="8" height="11" font="30"><b>30</b></text>
<text top="420" left="473" width="8" height="11" font="30"><b>40</b></text>
<text top="397" left="473" width="8" height="11" font="30"><b>50</b></text>
<text top="374" left="473" width="8" height="11" font="30"><b>60</b></text>
<text top="351" left="473" width="8" height="11" font="30"><b>70</b></text>
<text top="432" left="470" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="450" left="651" width="4" height="11" font="30"><b>0</b></text>
<text top="450" left="684" width="4" height="11" font="30"><b>1</b></text>
<text top="450" left="717" width="4" height="11" font="30"><b>2</b></text>
<text top="450" left="750" width="4" height="11" font="30"><b>3</b></text>
<text top="450" left="783" width="4" height="11" font="30"><b>4</b></text>
<text top="459" left="711" width="15" height="12" font="31"><b>IFC</b></text>
<text top="444" left="643" width="8" height="11" font="30"><b>30</b></text>
<text top="420" left="643" width="8" height="11" font="30"><b>40</b></text>
<text top="397" left="643" width="8" height="11" font="30"><b>50</b></text>
<text top="374" left="643" width="8" height="11" font="30"><b>60</b></text>
<text top="351" left="643" width="8" height="11" font="30"><b>70</b></text>
<text top="432" left="640" width="0" height="12" font="32"><b>O-DMOS (Reversed)</b></text>
<text top="472" left="73" width="545" height="11" font="5">Fig. 10. Scatter plots of the objective VQA results versus the O-DMOS values for all 36 impaired sequences.</text>
<text top="503" left="235" width="54" height="11" font="5">TABLE VI</text>
<text top="516" left="100" width="8" height="11" font="5">C</text>
<text top="518" left="108" width="316" height="9" font="7">OMPARISON OF THE PERFORMANCE AND THE COMPLEXITY OF</text>
<text top="531" left="190" width="56" height="9" font="7">OBJECTIVE</text>
<text top="530" left="249" width="27" height="11" font="5">VQA</text>
<text top="531" left="280" width="50" height="9" font="7">METHODS</text>
<text top="530" left="330" width="3" height="11" font="5">.</text>
<text top="545" left="134" width="34" height="9" font="7">Methods</text>
<text top="545" left="231" width="25" height="9" font="7">SRCC</text>
<text top="545" left="272" width="18" height="9" font="7">PCC</text>
<text top="545" left="307" width="26" height="9" font="7">RMSE</text>
<text top="545" left="348" width="21" height="9" font="7">MAE</text>
<text top="545" left="386" width="57" height="9" font="7">Time (second)</text>
<text top="595" left="84" width="49" height="9" font="7">PSNR based</text>
<text top="557" left="170" width="24" height="9" font="7">PSNR</text>
<text top="557" left="233" width="22" height="9" font="7">0.512</text>
<text top="557" left="270" width="22" height="9" font="7">0.541</text>
<text top="557" left="307" width="27" height="9" font="7">10.415</text>
<text top="557" left="348" width="22" height="9" font="7">8.623</text>
<text top="557" left="404" width="22" height="9" font="7">0.011</text>
<text top="567" left="157" width="50" height="9" font="7">W-PSNR <a href="xml/1709.06342v2.html#13">[7]</a></text>
<text top="567" left="233" width="22" height="9" font="7">0.556</text>
<text top="567" left="270" width="22" height="9" font="7">0.596</text>
<text top="567" left="309" width="22" height="9" font="7">9.938</text>
<text top="567" left="348" width="22" height="9" font="7">8.097</text>
<text top="567" left="404" width="22" height="9" font="7">0.025</text>
<text top="578" left="153" width="59" height="9" font="7">CPP-PSNR <a href="xml/1709.06342v2.html#13">[7]</a></text>
<text top="578" left="233" width="22" height="9" font="7">0.575</text>
<text top="578" left="270" width="22" height="9" font="7">0.632</text>
<text top="578" left="309" width="22" height="9" font="7">9.592</text>
<text top="578" left="348" width="22" height="9" font="7">7.782</text>
<text top="578" left="401" width="27" height="8" font="7">3.857*</text>
<text top="589" left="156" width="52" height="9" font="7">S-PSNR <a href="xml/1709.06342v2.html#13">[16]</a></text>
<text top="589" left="233" width="22" height="9" font="7">0.589</text>
<text top="589" left="270" width="22" height="9" font="7">0.639</text>
<text top="589" left="309" width="22" height="9" font="7">9.518</text>
<text top="589" left="348" width="22" height="9" font="7">7.692</text>
<text top="589" left="401" width="27" height="8" font="7">0.445*</text>
<text top="600" left="152" width="62" height="9" font="7">lwS-PSNR <a href="xml/1709.06342v2.html#13">[16]</a></text>
<text top="600" left="233" width="22" height="9" font="7">0.618</text>
<text top="600" left="270" width="22" height="9" font="7">0.684</text>
<text top="600" left="309" width="22" height="9" font="7">9.028</text>
<text top="600" left="348" width="22" height="9" font="7">7.213</text>
<text top="600" left="401" width="27" height="8" font="7">0.462*</text>
<text top="611" left="151" width="63" height="9" font="7">swS-PSNR <a href="xml/1709.06342v2.html#13">[16]</a></text>
<text top="611" left="233" width="22" height="9" font="7">0.637</text>
<text top="611" left="270" width="22" height="9" font="7">0.707</text>
<text top="611" left="309" width="22" height="9" font="7">8.752</text>
<text top="611" left="348" width="22" height="9" font="7">6.934</text>
<text top="611" left="401" width="27" height="8" font="7">0.445*</text>
<text top="622" left="148" width="69" height="9" font="7">NCP-PSNR (our)</text>
<text top="622" left="233" width="22" height="9" font="7">0.702</text>
<text top="622" left="270" width="22" height="9" font="7">0.725</text>
<text top="622" left="309" width="22" height="9" font="7">8.539</text>
<text top="622" left="348" width="22" height="9" font="7">6.770</text>
<text top="622" left="404" width="22" height="9" font="7">0.025</text>
<text top="633" left="151" width="62" height="9" font="7">CP-PSNR (our)</text>
<text top="632" left="233" width="22" height="9" font="7">0.751</text>
<text top="632" left="270" width="22" height="9" font="7">0.764</text>
<text top="632" left="309" width="22" height="9" font="7">7.991</text>
<text top="632" left="348" width="22" height="9" font="7">6.657</text>
<text top="633" left="404" width="22" height="9" font="7">2.405</text>
<text top="671" left="96" width="26" height="9" font="7">Others</text>
<text top="644" left="161" width="42" height="9" font="7">SSIM <a href="xml/1709.06342v2.html#14">[38]</a></text>
<text top="644" left="233" width="22" height="9" font="7">0.547</text>
<text top="644" left="270" width="22" height="9" font="7">0.562</text>
<text top="644" left="307" width="27" height="9" font="7">10.391</text>
<text top="644" left="348" width="22" height="9" font="7">8.684</text>
<text top="644" left="404" width="22" height="9" font="7">0.585</text>
<text top="655" left="160" width="45" height="9" font="7">VSNR <a href="xml/1709.06342v2.html#14">[40]</a></text>
<text top="655" left="233" width="22" height="9" font="7">0.684</text>
<text top="655" left="270" width="22" height="9" font="7">0.741</text>
<text top="655" left="309" width="22" height="9" font="7">8.741</text>
<text top="655" left="348" width="22" height="9" font="7">6.888</text>
<text top="655" left="404" width="22" height="9" font="7">1.431</text>
<text top="666" left="164" width="37" height="9" font="7">UQI <a href="xml/1709.06342v2.html#14">[37]</a></text>
<text top="666" left="233" width="22" height="9" font="7">0.586</text>
<text top="666" left="270" width="22" height="9" font="7">0.594</text>
<text top="666" left="307" width="27" height="9" font="7">10.774</text>
<text top="666" left="348" width="22" height="9" font="7">8.943</text>
<text top="666" left="404" width="22" height="9" font="7">0.428</text>
<text top="676" left="165" width="34" height="9" font="7">IFC <a href="xml/1709.06342v2.html#14">[39]</a></text>
<text top="676" left="235" width="17" height="9" font="7">0.61</text>
<text top="676" left="270" width="22" height="9" font="7">0.682</text>
<text top="676" left="309" width="22" height="9" font="7">9.054</text>
<text top="676" left="348" width="22" height="9" font="7">7.038</text>
<text top="676" left="401" width="27" height="9" font="7">10.447</text>
<text top="687" left="149" width="67" height="9" font="7">NCP-SSIM (our)</text>
<text top="687" left="233" width="22" height="9" font="7">0.802</text>
<text top="687" left="270" width="22" height="9" font="7">0.799</text>
<text top="687" left="309" width="22" height="9" font="7">7.443</text>
<text top="687" left="348" width="22" height="9" font="7">5.984</text>
<text top="687" left="404" width="22" height="9" font="7">0.599</text>
<text top="698" left="152" width="60" height="9" font="7">CP-SSIM (our)</text>
<text top="698" left="233" width="22" height="9" font="7">0.815</text>
<text top="698" left="270" width="22" height="9" font="7">0.807</text>
<text top="698" left="309" width="22" height="9" font="7">7.307</text>
<text top="698" left="348" width="22" height="9" font="7">5.977</text>
<text top="698" left="404" width="22" height="9" font="7">2.980</text>
<text top="709" left="84" width="344" height="9" font="7">*These methods are implemented in C++, while others are implemented in MATLAB.</text>
<text top="746" left="73" width="377" height="13" font="4">PSNR improve the performance. The possible reasons of the</text>
<text top="764" left="73" width="377" height="13" font="4">improvements are as follows: (1) As stated in Section VI-A,</text>
<text top="782" left="73" width="377" height="13" font="4">the distortion of pixels at different regions contribute unequally</text>
<text top="800" left="73" width="377" height="13" font="4">to the quality of omnidirectional video. The non-content-based</text>
<text top="818" left="73" width="377" height="13" font="4">weight map used in NCP-PSNR can reflect this inequality.</text>
<text top="836" left="73" width="377" height="13" font="4">Therefore, NCP-PSNR outperforms PSNR. (2) Although other</text>
<text top="854" left="73" width="377" height="13" font="4">objective methods based on weight allocation also outperform</text>
<text top="872" left="73" width="377" height="13" font="4">PSNR, NCP-PSNR performs better than them. This indicates</text>
<text top="890" left="73" width="377" height="13" font="4">the non-content-based weight map is more effective. (3) By</text>
<text top="908" left="73" width="377" height="13" font="4">predicting viewing directions with regard to video content,</text>
<text top="926" left="73" width="377" height="13" font="4">CP-PSNR can further emphasize the distortion in region of</text>
<text top="944" left="73" width="377" height="13" font="4">interest. This results in CP-PSNR further outperforming NCP-</text>
<text top="962" left="73" width="377" height="13" font="4">PSNR. In addition, though based on PSNR, both NCP-PSNR</text>
<text top="980" left="73" width="377" height="13" font="4">and CP-PSNR perfrom better than the four non-PSNR-based</text>
<text top="998" left="73" width="377" height="13" font="4">methods. This further proves that the proposed NCP-VQA and</text>
<text top="1015" left="73" width="377" height="13" font="4">CP-VQA methods are resultful in improving the performance</text>
<text top="1033" left="73" width="325" height="13" font="4">of objective VQA methods of omnidirectional video.</text>
<text top="1055" left="88" width="362" height="14" font="4">Statistical significance analysis. We follow <a href="xml/1709.06342v2.html#13">[27] </a>to imple-</text>
<text top="1073" left="73" width="377" height="13" font="4">ment F-test on the residuals between objective VQA scores</text>
<text top="1091" left="73" width="377" height="13" font="4">and DMOS values. Refer to the supporting document for the</text>
<text top="1109" left="73" width="377" height="13" font="4">results of F-test. The results show that at 90% significance</text>
<text top="507" left="468" width="377" height="13" font="4">level, both NCP-PSNR and CP-PSNR are superior to PSNR,</text>
<text top="525" left="468" width="377" height="13" font="4">indicating the significant improvement in performance of the</text>
<text top="543" left="468" width="377" height="13" font="4">proposed methods. Additionally, our CP-PSNR method is even</text>
<text top="561" left="468" width="377" height="13" font="4">superior to SSIM, UQI and W-PSNR. This further verifies the</text>
<text top="579" left="468" width="147" height="13" font="4">advantage of CP-PSNR.</text>
<text top="596" left="483" width="362" height="14" font="4">Complexity analysis. For complexity comparison, we test</text>
<text top="614" left="468" width="377" height="13" font="4">the runtime of the objective VQA methods. The experiment</text>
<text top="632" left="468" width="377" height="13" font="4">is run on a computer with Intel® Core™ i7-8700 CPU.</text>
<text top="650" left="468" width="377" height="13" font="4">Each method is run with single thread on an omnidirectional</text>
<text top="668" left="468" width="377" height="13" font="4">video with resolution of 4096 × 2048. Table <a href="xml/1709.06342v2.html#12">VI </a>reports the</text>
<text top="686" left="468" width="377" height="13" font="4">computational time per frame in seconds of each method.</text>
<text top="704" left="468" width="377" height="13" font="4">Most of these methods are implemented in MATLAB, except</text>
<text top="722" left="468" width="377" height="13" font="4">CPP-PSNR and the three variations of S-PSNR, which are</text>
<text top="740" left="468" width="377" height="13" font="4">implemented in C++. Since C++ runs at least twice faster</text>
<text top="758" left="468" width="377" height="13" font="4">than MATLAB <a href="xml/1709.06342v2.html#14">[59], </a>the results of these four methods are</text>
<text top="776" left="468" width="377" height="13" font="4">highlighted in italic in Table <a href="xml/1709.06342v2.html#12">VI. </a>In Table <a href="xml/1709.06342v2.html#12">VI, </a>the runtime</text>
<text top="794" left="468" width="377" height="13" font="4">of NCP-PSNR equals to that of W-PSNR, since the weight</text>
<text top="812" left="468" width="377" height="13" font="4">maps are generated offline and the additional time is only</text>
<text top="830" left="468" width="377" height="13" font="4">spent on applying the weight maps in calculation. In general,</text>
<text top="848" left="468" width="377" height="13" font="4">NCP-PSNR runs rather fast but brings better performance</text>
<text top="865" left="468" width="377" height="13" font="4">than most of the methods. Although CP-PSNR reaches the</text>
<text top="883" left="468" width="377" height="13" font="4">best performance among these methods at the cost of high</text>
<text top="901" left="468" width="377" height="13" font="4">complexity, it still runs faster than CPP-PSNR and IFC. In</text>
<text top="919" left="468" width="377" height="13" font="4">conclusion, our methods achieves the best performance with</text>
<text top="937" left="468" width="275" height="13" font="4">modest computational complexity increment.</text>
<text top="955" left="483" width="362" height="14" font="4">Extension to other methods. Our VQA methods can be</text>
<text top="973" left="468" width="377" height="13" font="4">easily extended to other methods, including SSIM. Here,</text>
<text top="991" left="468" width="377" height="13" font="4">we extend our NCP-VQA and CP-VQA methods to SSIM,</text>
<text top="1009" left="468" width="377" height="13" font="4">producing NCP-SSIM and CP-SSIM. NCP-SSIM and CP-</text>
<text top="1027" left="468" width="157" height="13" font="4">SSIM can be obtained by</text>
<text top="1057" left="524" width="62" height="12" font="3">NCP-SSIM</text>
<text top="1056" left="590" width="12" height="13" font="4">=</text>
<text top="1051" left="606" width="22" height="7" font="4">X</text>
<text top="1076" left="610" width="14" height="9" font="0">s,t</text>
<text top="1056" left="630" width="13" height="13" font="4">m</text>
<text top="1061" left="643" width="28" height="9" font="0">SSIM</text>
<text top="1056" left="672" width="52" height="13" font="4">(s, t) · ˜</text>
<text top="1056" left="714" width="42" height="13" font="4">w(s, t)</text>
<text top="1057" left="756" width="3" height="12" font="3">,</text>
<text top="1056" left="820" width="25" height="13" font="4">(23)</text>
<text top="1097" left="524" width="74" height="13" font="4">CP-SSIM =</text>
<text top="1092" left="602" width="22" height="7" font="4">X</text>
<text top="1117" left="606" width="14" height="9" font="0">s,t</text>
<text top="1097" left="626" width="13" height="13" font="4">m</text>
<text top="1102" left="639" width="28" height="9" font="0">SSIM</text>
<text top="1097" left="668" width="57" height="13" font="4">(s, t) · f</text>
<text top="1097" left="709" width="11" height="13" font="4">w</text>
<text top="1095" left="720" width="3" height="10" font="0">0</text>
<text top="1097" left="725" width="34" height="14" font="4">(s, t),</text>
<text top="1097" left="820" width="25" height="13" font="4">(24)</text>
</page>
<page number="13" position="absolute" top="0" left="0" height="1188" width="918">
<text top="40" left="834" width="10" height="9" font="0">13</text>
<text top="87" left="73" width="53" height="13" font="4">where m</text>
<text top="92" left="127" width="28" height="9" font="0">SSIM</text>
<text top="87" left="159" width="291" height="13" font="4">is the SSIM map with local SSIM value for pixel</text>
<text top="104" left="73" width="53" height="14" font="4">(s, t); ˜</text>
<text top="104" left="116" width="62" height="14" font="4">w and f</text>
<text top="104" left="163" width="11" height="13" font="4">w</text>
<text top="103" left="174" width="3" height="10" font="0">0</text>
<text top="105" left="185" width="265" height="13" font="4">are the normalized non-content-based and</text>
<text top="123" left="73" width="377" height="13" font="4">content-based weight maps proposed in <a href="xml/1709.06342v2.html#8">(15) </a>and <a href="xml/1709.06342v2.html#9">(20). </a>Table <a href="xml/1709.06342v2.html#12">VI</a></text>
<text top="141" left="73" width="377" height="13" font="4">shows the performance and complexity of these two methods.</text>
<text top="158" left="73" width="377" height="13" font="4">We can see that on the basis of SSIM, the performance of our</text>
<text top="176" left="73" width="377" height="13" font="4">NCP-SSIM and CP-SSIM methods is rather high in terms of</text>
<text top="194" left="73" width="377" height="13" font="4">the four metrics, better than that of NCP-PSNR and CP-PSNR.</text>
<text top="212" left="73" width="377" height="13" font="4">However, the cost is additional computational complexity. In</text>
<text top="230" left="73" width="377" height="13" font="4">conclusion, our NCP-VQA and CP-VQA methods can be</text>
<text top="248" left="73" width="299" height="13" font="4">easily extended and with improved performance.</text>
<text top="280" left="201" width="45" height="13" font="4">VII. C</text>
<text top="282" left="247" width="75" height="11" font="5">ONCLUSION</text>
<text top="302" left="88" width="362" height="13" font="4">In this paper, we have proposed both subjective and ob-</text>
<text top="320" left="73" width="377" height="13" font="4">jective VQA methods for evaluating the quality degrada-</text>
<text top="338" left="73" width="377" height="13" font="4">tion of impaired omnidirectional video. In contrast with the</text>
<text top="356" left="73" width="377" height="13" font="4">conventional VQA methods, human viewing directions were</text>
<text top="374" left="73" width="377" height="13" font="4">investigated and then taken into account in our VQA methods.</text>
<text top="392" left="73" width="377" height="13" font="4">Specifically, we conducted an experiment to present a new</text>
<text top="410" left="73" width="377" height="13" font="4">database, which contains the viewing directions from 40</text>
<text top="428" left="73" width="377" height="13" font="4">subjects on viewing 48 omnidirectional video sequences. Next,</text>
<text top="446" left="73" width="377" height="13" font="4">we found from our database that subjects consistently prefer</text>
<text top="464" left="73" width="377" height="13" font="4">looking at the center of front region of omnidirectional video,</text>
<text top="482" left="73" width="377" height="13" font="4">but there still exists dependency on video content for viewing</text>
<text top="500" left="73" width="377" height="13" font="4">directions. In light of our findings, we proposed two subjective</text>
<text top="518" left="73" width="377" height="13" font="4">VQA metrics, O-DMOS and V-DMOS, measuring the overall</text>
<text top="536" left="73" width="377" height="13" font="4">and regional quality reduction of impaired omnidirectional</text>
<text top="553" left="73" width="377" height="13" font="4">video, repsectively. In addition, we proposed two objective</text>
<text top="571" left="73" width="377" height="13" font="4">methods, NCP-PSNR and CP-PSNR, for assessing the quality</text>
<text top="589" left="73" width="377" height="13" font="4">loss of compressed omnidirectional videos. In NCP-PSNR,</text>
<text top="607" left="73" width="377" height="13" font="4">the quality loss is weighed according to statistical results on</text>
<text top="625" left="73" width="377" height="13" font="4">the preference for the center of the front region, while CP-</text>
<text top="643" left="73" width="377" height="13" font="4">PSNR imposes quality loss using weights with respect to</text>
<text top="661" left="73" width="377" height="13" font="4">possible viewing directions predicted upon the video content.</text>
<text top="679" left="73" width="377" height="13" font="4">Finally, our experimental results validate the effectiveness of</text>
<text top="697" left="73" width="377" height="13" font="4">our subjective and objective VQA methods. Between the two</text>
<text top="715" left="73" width="377" height="13" font="4">proposed methods, NCP-PSNR can be calculated at a fast</text>
<text top="733" left="73" width="377" height="13" font="4">speed, while CP-PSNR can achieve better performance with</text>
<text top="751" left="73" width="109" height="13" font="4">the extra runtime.</text>
<text top="768" left="88" width="362" height="13" font="4">There are two promising directions for future work. First,</text>
<text top="786" left="73" width="377" height="13" font="4">in addition to FoV, there may be some other characteristics of</text>
<text top="804" left="73" width="377" height="13" font="4">the HVS benefiting for VQA of omnidirectional video. For</text>
<text top="822" left="73" width="377" height="13" font="4">example, there are some high level features, such as local</text>
<text top="840" left="73" width="377" height="13" font="4">motion and objectness, can also be incorporated in predicting</text>
<text top="858" left="73" width="377" height="13" font="4">viewing direction to improve the CP-VQA method. This is an</text>
<text top="876" left="73" width="377" height="13" font="4">promising future work. Second, future work may apply our</text>
<text top="894" left="73" width="377" height="13" font="4">VQA method in optimizing the encoder of omnidirectional</text>
<text top="912" left="73" width="377" height="13" font="4">video coding. For example, NCP-PSNR or CP-PSNR can be</text>
<text top="930" left="73" width="377" height="13" font="4">maximized in the bit allocation when encoding omnidirec-</text>
<text top="948" left="73" width="76" height="13" font="4">tional video.</text>
<text top="980" left="220" width="10" height="13" font="4">R</text>
<text top="982" left="231" width="73" height="11" font="5">EFERENCES</text>
<text top="1003" left="79" width="371" height="11" font="5">[1] HUAWEI iLab, “VR data report,” HUAWEI Report, 2016, <a href="https://mp.weixin.qq.com/s/tcsm9NIECa7d1L7gZekrrQ">https://mp.</a></text>
<text top="1017" left="101" width="219" height="11" font="5"><a href="https://mp.weixin.qq.com/s/tcsm9NIECa7d1L7gZekrrQ">weixin.qq.com/s/tcsm9NIECa7d1L7gZekrrQ.</a></text>
<text top="1030" left="79" width="371" height="11" font="5">[2] W. Sarmiento and C. Quintero, “Panoramic immersive videos-3d pro-</text>
<text top="1043" left="101" width="349" height="11" font="5">duction and visualization framework,” in IEEE SIGMAP, 2009, pp. 173–</text>
<text top="1057" left="101" width="21" height="11" font="5">177.</text>
<text top="1070" left="79" width="371" height="11" font="5">[3] R. Konrad, E. A. Cooper, and G. Wetzstein, “Novel optical configura-</text>
<text top="1084" left="101" width="349" height="11" font="5">tions for virtual reality: evaluating user preference and performance with</text>
<text top="1097" left="101" width="349" height="11" font="5">focus-tunable and monovision near-eye displays,” in ACM CHI, 2016,</text>
<text top="1111" left="101" width="76" height="11" font="5">pp. 1211–1220.</text>
<text top="89" left="474" width="371" height="11" font="5">[4] the MPEG Virtual Reality Ad-hoc Group, “Summary of survey on virtual</text>
<text top="102" left="495" width="278" height="11" font="5">reality,” in ISO/IEC JTC 1/SC 29/WG 11 N16542, 2016.</text>
<text top="117" left="474" width="371" height="11" font="5">[5] C. W. Fu, L. Wan, T. T. Wong, and C. S. Leung, “The rhombic</text>
<text top="130" left="495" width="349" height="11" font="5">dodecahedron map: An efficient scheme for encoding panoramic video,”</text>
<text top="144" left="495" width="57" height="10" font="5">IEEE TMM</text>
<text top="143" left="552" width="108" height="11" font="5">, vol. 11, no. 4, 2009.</text>
<text top="158" left="474" width="111" height="11" font="5">[6] E. Upenik, M. ˇ</text>
<text top="158" left="579" width="265" height="11" font="5">Reˇr´abek, and T. Ebrahimi, “Testbed for subjective</text>
<text top="171" left="495" width="304" height="11" font="5">evaluation of omnidirectional visual content,” in IEEE PCS.</text>
<text top="171" left="816" width="29" height="11" font="5">IEEE,</text>
<text top="185" left="495" width="71" height="11" font="5">2016, pp. 1–5.</text>
<text top="199" left="474" width="371" height="11" font="5">[7] V. Zakharchenko, K. P. Choi, and J. H. Park, “Quality metric for</text>
<text top="213" left="495" width="349" height="11" font="5">spherical panoramic video,” in SPIE Optical Engineering+ Applications,</text>
<text top="226" left="495" width="139" height="11" font="5">2016, pp. 99 700C–99 700C.</text>
<text top="241" left="474" width="371" height="11" font="5">[8] R. Schatz, A. Sackl, C. Timmerer, and B. Gardlo, “Towards subjective</text>
<text top="254" left="495" width="349" height="11" font="5">quality of experience assessment for omnidirectional video streaming,”</text>
<text top="268" left="495" width="86" height="11" font="5">in IEEE QoMEX.</text>
<text top="268" left="593" width="104" height="11" font="5">IEEE, 2017, pp. 1–6.</text>
<text top="282" left="474" width="371" height="11" font="5">[9] A. Singla, S. Fremerey, W. Robitza, and A. Raake, “Measuring and</text>
<text top="296" left="495" width="349" height="11" font="5">comparing qoe and simulator sickness of omnidirectional videos in</text>
<text top="309" left="495" width="349" height="11" font="5">different head mounted displays,” in IEEE QoMEX, May 2017, pp. 1–6.</text>
<text top="324" left="468" width="377" height="11" font="5">[10] A. Singla, S. Fremerey, W. Robitza, P. Lebreton, and A. Raake,</text>
<text top="337" left="495" width="349" height="11" font="5">“Comparison of subjective quality evaluation for hevc encoded omni-</text>
<text top="351" left="495" width="349" height="11" font="5">directional videos at different bit-rates for uhd and fhd resolution,” in</text>
<text top="364" left="495" width="167" height="10" font="5">Thematic Workshops of ACM MM</text>
<text top="364" left="663" width="3" height="11" font="5">.</text>
<text top="364" left="678" width="129" height="11" font="5">ACM, 2017, pp. 511–519.</text>
<text top="379" left="468" width="377" height="11" font="5">[11] Recommendation, ITU-R, “BT. 710-4: Subjective assessment methods</text>
<text top="392" left="495" width="349" height="11" font="5">for image quality in high-definition television,” Technical Report, ITU-</text>
<text top="405" left="495" width="102" height="11" font="5">R, Tech. Rep., 1998.</text>
<text top="420" left="468" width="377" height="11" font="5">[12] Recommendation, ITUT, “P. 910: Subjective video quality assessment</text>
<text top="433" left="495" width="287" height="11" font="5">methods for multimedia applications,” ITU, Geneva, 2008.</text>
<text top="448" left="468" width="377" height="11" font="5">[13] Assembly, ITU-R, “Methodology for the subjective assessment of the</text>
<text top="461" left="495" width="178" height="11" font="5">quality of television pictures,” 2012.</text>
<text top="476" left="468" width="377" height="11" font="5">[14] T. K. Tan, R. Weerakkody, M. Mrak, N. Ramzan, V. Baroncini, J.-R.</text>
<text top="489" left="495" width="349" height="11" font="5">Ohm, and G. J. Sullivan, “Video quality evaluation methodology and</text>
<text top="503" left="495" width="349" height="11" font="5">verification testing of hevc compression performance,” IEEE TCSVT,</text>
<text top="516" left="495" width="157" height="11" font="5">vol. 26, no. 1, pp. 76–90, 2016.</text>
<text top="531" left="468" width="377" height="11" font="5">[15] K. Seshadrinathan, R. Soundararajan, A. C. Bovik, and L. K. Cormack,</text>
<text top="544" left="495" width="349" height="11" font="5">“A subjective study to evaluate video quality assessment algorithms,” in</text>
<text top="558" left="495" width="151" height="10" font="5">IS&amp;T/SPIE Electronic Imaging</text>
<text top="558" left="647" width="147" height="11" font="5">, 2010, pp. 75 270H–75 270H.</text>
<text top="572" left="468" width="377" height="11" font="5">[16] M. Yu, H. Lakshman, and B. Girod, “A framework to evaluate omni-</text>
<text top="586" left="495" width="257" height="11" font="5">directional video coding schemes,” in IEEE ISMAR.</text>
<text top="586" left="765" width="79" height="11" font="5">IEEE, 2015, pp.</text>
<text top="599" left="495" width="33" height="11" font="5">31–36.</text>
<text top="614" left="468" width="377" height="11" font="5">[17] S. Lee, M. S. Pattichis, and A. C. Bovik, “Foveated video quality</text>
<text top="627" left="495" width="291" height="11" font="5">assessment,” IEEE TMM, vol. 4, no. 1, pp. 129–132, 2002.</text>
<text top="642" left="468" width="377" height="11" font="5">[18] S. Li, M. Xu, Y. Ren, and Z. Wang, “Closed-form optimization on</text>
<text top="655" left="495" width="349" height="11" font="5">saliency-guided image compression for hevc-msp,” IEEE TMM, vol. PP,</text>
<text top="668" left="495" width="65" height="11" font="5">no. 99, 2017.</text>
<text top="683" left="468" width="377" height="11" font="5">[19] Z. Chen, Y. Li, and Y. Zhang, “Recent advances in omnidirectional video</text>
<text top="696" left="495" width="349" height="11" font="5">coding for virtual reality: Projection and evaluation,” Signal Processing,</text>
<text top="710" left="495" width="130" height="11" font="5">vol. 146, pp. 66–78, 2018.</text>
<text top="724" left="468" width="377" height="11" font="5">[20] M. Tang, Y. Zhang, J. Wen, and S. Yang, “Optimized video coding for</text>
<text top="738" left="495" width="228" height="11" font="5">omnidirectional videos,” in 2017 IEEE ICME.</text>
<text top="738" left="736" width="108" height="11" font="5">IEEE, 2017, pp. 799–</text>
<text top="751" left="495" width="21" height="11" font="5">804.</text>
<text top="766" left="468" width="377" height="11" font="5">[21] L. Li, Z. Li, M. Budagavi, and H. Li, “Projection based advanced motion</text>
<text top="779" left="495" width="309" height="11" font="5">model for cubic mapping for 360-degree video,” in IEEE ICIP.</text>
<text top="779" left="816" width="29" height="11" font="5">IEEE,</text>
<text top="793" left="495" width="107" height="11" font="5">2017, pp. 1427–1431.</text>
<text top="807" left="468" width="377" height="11" font="5">[22] Y. He, Y. Ye, P. Hanhart, and X. Xiu, “Motion compensated prediction</text>
<text top="821" left="495" width="308" height="11" font="5">with geometry padding for 360 video coding,” in IEEE VCIP.</text>
<text top="821" left="816" width="29" height="11" font="5">IEEE,</text>
<text top="834" left="495" width="71" height="11" font="5">2017, pp. 1–4.</text>
<text top="849" left="468" width="377" height="11" font="5">[23] M. Xu, C. Li, Y. Liu, X. Deng, and J. Lu, “A subjective visual quality</text>
<text top="862" left="495" width="227" height="11" font="5">method of panoramic videos,” in IEEE ICME.</text>
<text top="862" left="735" width="60" height="11" font="5">IEEE, 2017.</text>
<text top="877" left="468" width="377" height="11" font="5">[24] C. Li, M. Xu, Y. Liu, M. Bai, and W. Wei, “IEEE1857.9-N1013 A</text>
<text top="890" left="495" width="349" height="11" font="5">subjective evaluation methodology on panoramic video,” IEEE1857.9</text>
<text top="904" left="495" width="171" height="11" font="5">7th Meeting: Hainan, China, 2016.</text>
<text top="918" left="468" width="377" height="11" font="5">[25] M. H. Pinson and S. Wolf, “Comparing subjective video quality testing</text>
<text top="932" left="495" width="349" height="11" font="5">methodologies,” in Visual Communications and Image Processing 2003,</text>
<text top="945" left="495" width="95" height="11" font="5">2003, pp. 573–582.</text>
<text top="960" left="468" width="377" height="11" font="5">[26] C. Lee, H. Choi, E. Lee, S. Lee, and J. Choe, “Comparison of various</text>
<text top="973" left="495" width="349" height="11" font="5">subjective video quality assessment methods,” in Electronic Imaging</text>
<text top="987" left="495" width="24" height="10" font="5">2006</text>
<text top="986" left="519" width="142" height="11" font="5">, 2006, pp. 605 906–605 906.</text>
<text top="1001" left="468" width="377" height="11" font="5">[27] K. Seshadrinathan, R. Soundararajan, A. C. Bovik, and L. K. Cormack,</text>
<text top="1014" left="495" width="349" height="11" font="5">“Study of subjective and objective quality assessment of video,” IEEE</text>
<text top="1028" left="495" width="18" height="10" font="5">TIP</text>
<text top="1028" left="513" width="188" height="11" font="5">, vol. 19, no. 6, pp. 1427–1441, 2010.</text>
<text top="1042" left="468" width="377" height="11" font="5">[28] P. Pourashraf, F. Safaei, and D. R. Franklin, “Minimisation of video</text>
<text top="1056" left="495" width="349" height="11" font="5">downstream bit rate for large scale immersive video conferencing by</text>
<text top="1069" left="495" width="306" height="11" font="5">utilising the perceptual variations of quality,” in IEEE ICME.</text>
<text top="1069" left="816" width="29" height="11" font="5">IEEE,</text>
<text top="1083" left="495" width="71" height="11" font="5">2014, pp. 1–6.</text>
<text top="1097" left="468" width="377" height="11" font="5">[29] Recommendation, ITU-R, “BT. 2021-1: Subjective methods for the</text>
<text top="1111" left="495" width="337" height="11" font="5">assessment of stereoscopic 3DTV systems,” ITU-R, vol. 2021, 2015.</text>
</page>
<page number="14" position="absolute" top="0" left="0" height="1188" width="918">
<text top="40" left="834" width="10" height="9" font="0">14</text>
<text top="89" left="73" width="377" height="11" font="5">[30] C.-H. Chou and C.-W. Chen, “A perceptually optimized 3-D subband</text>
<text top="102" left="101" width="349" height="11" font="5">codec for video communication over wireless channels,” IEEE TCSVT,</text>
<text top="115" left="101" width="163" height="11" font="5">vol. 6, no. 2, pp. 143–156, 1996.</text>
<text top="129" left="73" width="377" height="11" font="5">[31] A. Cavallaro, O. Steiger, and T. Ebrahimi, “Semantic video analysis</text>
<text top="143" left="101" width="349" height="11" font="5">for adaptive content delivery and automatic description,” IEEE TCSVT,</text>
<text top="156" left="101" width="187" height="11" font="5">vol. 15, no. 10, pp. 1200–1209, 2005.</text>
<text top="170" left="73" width="377" height="11" font="5">[32] Z. Li, S. Qin, and L. Itti, “Visual attention guided bit allocation in video</text>
<text top="184" left="101" width="349" height="11" font="5">compression,” Image and Vision Computing, vol. 29, no. 1, pp. 1–14,</text>
<text top="197" left="101" width="27" height="11" font="5">2011.</text>
<text top="211" left="73" width="377" height="11" font="5">[33] Z. Chen and C. Guillemot, “Perceptually-friendly H. 264/AVC video</text>
<text top="225" left="101" width="349" height="11" font="5">coding based on foveated just-noticeable-distortion model,” IEEE</text>
<text top="238" left="101" width="35" height="10" font="5">TCSVT</text>
<text top="238" left="135" width="176" height="11" font="5">, vol. 20, no. 6, pp. 806–819, 2010.</text>
<text top="252" left="73" width="377" height="11" font="5">[34] M. Xu, X. Deng, S. Li, and Z. Wang, “Region-of-interest based con-</text>
<text top="265" left="101" width="349" height="11" font="5">versational HEVC coding with hierarchical perception model of face,”</text>
<text top="279" left="101" width="65" height="10" font="5">IEEE J-STSP</text>
<text top="279" left="166" width="194" height="11" font="5">, vol. 8, no. 3, pp. 475–489, Jun. 2014.</text>
<text top="293" left="73" width="377" height="11" font="5">[35] N. Liu and G. Zhai, “Free energy adjusted peak signal to noise ratio</text>
<text top="306" left="101" width="349" height="11" font="5">(FEA-PSNR) for image quality assessment,” Sensing and Imaging,</text>
<text top="320" left="101" width="133" height="11" font="5">vol. 18, no. 1, p. 11, 2017.</text>
<text top="334" left="73" width="377" height="11" font="5">[36] T. Na and M. Kim, “A novel no-reference PSNR estimation method with</text>
<text top="347" left="101" width="349" height="11" font="5">regard to deblocking filtering effect in H. 264/AVC bitstreams,” IEEE</text>
<text top="361" left="101" width="35" height="10" font="5">TCSVT</text>
<text top="361" left="135" width="176" height="11" font="5">, vol. 24, no. 2, pp. 320–330, 2014.</text>
<text top="375" left="73" width="377" height="11" font="5">[37] Z. Wang and A. C. Bovik, “A universal image quality index,” IEEE SPL,</text>
<text top="388" left="101" width="151" height="11" font="5">vol. 9, no. 3, pp. 81–84, 2002.</text>
<text top="402" left="73" width="377" height="11" font="5">[38] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image</text>
<text top="415" left="101" width="349" height="11" font="5">quality assessment: from error visibility to structural similarity,” IEEE</text>
<text top="429" left="101" width="18" height="10" font="5">TIP</text>
<text top="429" left="119" width="176" height="11" font="5">, vol. 13, no. 4, pp. 600–612, 2004.</text>
<text top="443" left="73" width="377" height="11" font="5">[39] H. R. Sheikh, A. C. Bovik, and G. De Veciana, “An information fidelity</text>
<text top="456" left="101" width="349" height="11" font="5">criterion for image quality assessment using natural scene statistics,”</text>
<text top="470" left="101" width="48" height="10" font="5">IEEE TIP</text>
<text top="470" left="149" width="194" height="11" font="5">, vol. 14, no. 12, pp. 2117–2128, 2005.</text>
<text top="484" left="73" width="377" height="11" font="5">[40] D. M. Chandler and S. S. Hemami, “Vsnr: A wavelet-based visual</text>
<text top="497" left="101" width="349" height="11" font="5">signal-to-noise ratio for natural images,” IEEE TIP, vol. 16, no. 9, pp.</text>
<text top="511" left="101" width="88" height="11" font="5">2284–2298, 2007.</text>
<text top="525" left="73" width="377" height="11" font="5">[41] E. Upenik, M. Rerabek, and T. Ebrahimi, “On the performance of</text>
<text top="538" left="101" width="349" height="11" font="5">objective metrics for omnidirectional visual content,” in IEEE QoMEX,</text>
<text top="552" left="101" width="157" height="11" font="5">no. EPFL-CONF-227464, 2017.</text>
<text top="565" left="73" width="377" height="11" font="5">[42] Y. Rai, P. Le Callet, and P. Guillotel, “Which saliency weighting for</text>
<text top="579" left="101" width="308" height="11" font="5">omni directional image quality assessment?” in IEEE QoMEX.</text>
<text top="579" left="421" width="29" height="11" font="5">IEEE,</text>
<text top="592" left="101" width="71" height="11" font="5">2017, pp. 1–6.</text>
<text top="606" left="73" width="377" height="11" font="5">[43] A. De Abreu, C. Ozcinar, and A. Smolic, “Look around you: Saliency</text>
<text top="620" left="101" width="349" height="11" font="5">maps for omnidirectional images in vr applications,” in IEEE QoMEX.</text>
<text top="633" left="101" width="104" height="11" font="5">IEEE, 2017, pp. 1–6.</text>
<text top="647" left="73" width="377" height="11" font="5">[44] X. Corbillon, F. De Simone, and G. Simon, “360-degree video head</text>
<text top="661" left="101" width="342" height="11" font="5">movement dataset,” in ACM MMSys, no. EPFL-CONF-227447, 2017.</text>
<text top="675" left="73" width="377" height="11" font="5">[45] C. Wu, Z. Tan, Z. Wang, and S. Yang, “A dataset for exploring user</text>
<text top="688" left="101" width="304" height="11" font="5">behaviors in vr spherical video streaming,” in ACM MMSys.</text>
<text top="688" left="420" width="30" height="11" font="5">ACM,</text>
<text top="702" left="101" width="95" height="11" font="5">2017, pp. 193–198.</text>
<text top="716" left="73" width="280" height="11" font="5">[46] J. P. Snyder, Map projections–A working manual.</text>
<text top="716" left="370" width="80" height="11" font="5">US Government</text>
<text top="729" left="101" width="160" height="11" font="5">Printing Office, 1987, vol. 1395.</text>
<text top="743" left="73" width="377" height="11" font="5">[47] J. Li, C. Xia, Y. Song, S. Fang, and X. Chen, “A data-driven metric for</text>
<text top="756" left="101" width="349" height="11" font="5">comprehensive evaluation of saliency models,” in IEEE ICCV, 2015, pp.</text>
<text top="770" left="101" width="45" height="11" font="5">190–198.</text>
<text top="784" left="73" width="377" height="11" font="5">[48] A. M. Van Dijk, J.-B. Martens, and A. B. Watson, “Quality asessment of</text>
<text top="797" left="101" width="349" height="11" font="5">coded images using numerical category scaling,” in Advanced Networks</text>
<text top="811" left="101" width="61" height="10" font="5">and Services</text>
<text top="811" left="162" width="288" height="11" font="5">. International Society for Optics and Photonics, 1995, pp.</text>
<text top="824" left="101" width="39" height="11" font="5">90–101.</text>
<text top="838" left="73" width="377" height="11" font="5">[49] A. M. Dimitrijevi´c, M. Lambers, and D. Ranˇci´c, “Comparison of</text>
<text top="852" left="101" width="349" height="11" font="5">spherical cube map projections used in planet-sized terrain rendering,”</text>
<text top="865" left="101" width="82" height="10" font="5">FU Math Inform</text>
<text top="865" left="183" width="176" height="11" font="5">, vol. 31, no. 2, pp. 259–297, 2016.</text>
<text top="879" left="73" width="377" height="11" font="5">[50] K. K. Sreedhar, A. Aminlou, M. M. Hannuksela, and M. Gabbouj,</text>
<text top="892" left="101" width="349" height="11" font="5">“Viewport-adaptive encoding and streaming of 360-degree video for</text>
<text top="906" left="101" width="206" height="11" font="5">virtual reality applications,” in IEEE ISM.</text>
<text top="906" left="319" width="128" height="11" font="5">IEEE, 2016, pp. 583–586.</text>
<text top="920" left="73" width="377" height="11" font="5">[51] A. TaghaviNasrabadi, A. Mahzari, J. D. Beshay, and R. Prakash,</text>
<text top="933" left="101" width="349" height="11" font="5">“Adaptive 360-degree video streaming using layered video coding,” in</text>
<text top="947" left="101" width="45" height="10" font="5">IEEE VR</text>
<text top="947" left="146" width="3" height="11" font="5">.</text>
<text top="947" left="160" width="128" height="11" font="5">IEEE, 2017, pp. 347–348.</text>
<text top="961" left="73" width="377" height="11" font="5">[52] M. Budagavi, J. Furton, G. Jin, A. Saxena, J. Wilkinson, and A. Dick-</text>
<text top="974" left="101" width="349" height="11" font="5">erson, “360 degrees video coding using region adaptive smoothing,” in</text>
<text top="988" left="101" width="53" height="10" font="5">IEEE ICIP</text>
<text top="988" left="154" width="3" height="11" font="5">.</text>
<text top="988" left="169" width="128" height="11" font="5">IEEE, 2015, pp. 750–754.</text>
<text top="1002" left="73" width="377" height="11" font="5">[53] J. Besharse and D. Bok, The retina and its disorders. Academic Press,</text>
<text top="1015" left="101" width="27" height="11" font="5">2011.</text>
<text top="1029" left="73" width="377" height="11" font="5">[54] C. Guo and L. Zhang, “A novel multiresolution spatiotemporal saliency</text>
<text top="1042" left="101" width="349" height="11" font="5">detection model and its applications in image and video compression,”</text>
<text top="1056" left="101" width="48" height="10" font="5">IEEE TIP</text>
<text top="1056" left="149" width="176" height="11" font="5">, vol. 19, no. 1, pp. 185–198, 2010.</text>
<text top="1070" left="73" width="377" height="11" font="5">[55] Y. Cheng, “Mean shift, mode seeking, and clustering,” IEEE TPAMI,</text>
<text top="1083" left="101" width="169" height="11" font="5">vol. 17, no. 8, pp. 790–799, 1995.</text>
<text top="1097" left="73" width="377" height="11" font="5">[56] A. Liaw and M. Wiener, “Classification and regression by randomforest,”</text>
<text top="1111" left="101" width="35" height="10" font="5">R news</text>
<text top="1111" left="136" width="158" height="11" font="5">, vol. 2, no. 3, pp. 18–22, 2002.</text>
<text top="89" left="468" width="377" height="11" font="5">[57] D. Rudoy, D. B. Goldman, E. Shechtman, and L. Zelnik-Manor, “Learn-</text>
<text top="102" left="495" width="349" height="11" font="5">ing video saliency from human gaze using candidate selection,” in IEEE</text>
<text top="116" left="495" width="30" height="10" font="5">CVPR</text>
<text top="115" left="525" width="114" height="11" font="5">, 2013, pp. 1147–1154.</text>
<text top="129" left="468" width="377" height="11" font="5">[58] IEEE1857.9 1st Meeting: Beijing, China, “1857.9-01-N0001 output</text>
<text top="142" left="495" width="86" height="11" font="5">document,” 2016.</text>
<text top="156" left="468" width="377" height="11" font="5">[59] T. Andrews, “Computation time comparison between matlab and c++</text>
<text top="169" left="495" width="148" height="11" font="5">using launch windows,” 2012.</text>
</page>
<outline>
<item page="1">I Introduction</item>
<item page="2">II Related work</item>
<outline>
<item page="2">II-A Related work on subjective VQA</item>
<item page="2">II-B Related work on objective VQA</item>
<item page="3">II-C Related work on database of omnidirectional content</item>
</outline>
<item page="3">III Analysis of consistency in viewing omnidirectional video</item>
<outline>
<item page="3">III-A Database</item>
<item page="4">III-B Data analysis</item>
</outline>
<item page="5">IV Subjective VQA method</item>
<outline>
<item page="5">IV-A Test configuration</item>
<item page="6">IV-B Test procedure</item>
<item page="6">IV-C Processing of subjective scores</item>
</outline>
<item page="7">V Objective VQA methods</item>
<outline>
<item page="7">V-A Non-content-based perceptual VQA method</item>
<item page="8">V-B Content-based PVQA method</item>
</outline>
<item page="9">VI Experimental Results</item>
<outline>
<item page="9">VI-A Validation on our subjective VQA method</item>
<item page="11">VI-B Validation on our objective VQA methods</item>
</outline>
<item page="13">VII Conclusion</item>
<item page="13">References</item>
</outline>
</pdf2xml>
